{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from CycleGAN_utils import *\n",
    "from CycleGAN_models import *\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 101\n",
    "dataset_name = 'CycleGAN'\n",
    "batch_size = 1\n",
    "lr=0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "n_cpu = 8\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "channels = 1\n",
    "sample_interval = 500\n",
    "checkpoint_interval = 25\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"../datasets/64dataset.pt\")\n",
    "\n",
    "dataset = [sample for sample in dataset if sample != None]\n",
    "\n",
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "def process_gan(dataset, s):\n",
    "    \n",
    "    output = []\n",
    "    dataset = [sample[0] for sample in dataset]\n",
    "    for sample in dataset:\n",
    "        sample = sample[s][0]\n",
    "        sample /= torch.max(sample)\n",
    "        output.append(torch.unsqueeze(sample, 0))\n",
    "    return output\n",
    "\n",
    "        \n",
    "NCgan1 = process_gan(NC, 0)\n",
    "NCgan2 = process_gan(NC, 1)\n",
    "NCgan3 = process_gan(NC, 2)\n",
    "\n",
    "ADgan1 = process_gan(AD, 0)\n",
    "ADgan2 = process_gan(AD, 1)\n",
    "ADgan3 = process_gan(AD, 2)\n",
    "\n",
    "gan1 = []\n",
    "for i in range(len(ADgan1)):\n",
    "    gan1.append({\"A\": NCgan1[i], \"B\": ADgan1[i]})\n",
    "\n",
    "gan2 = []\n",
    "for i in range(len(ADgan2)):\n",
    "    gan2.append({\"A\": NCgan2[i], \"B\": ADgan2[i]})\n",
    "    \n",
    "gan3 = []\n",
    "for i in range(len(ADgan3)):\n",
    "    gan3.append({\"A\": NCgan3[i], \"B\": ADgan3[i]})\n",
    "\n",
    "batch_size = 1\n",
    "dataloader1 = DataLoader(gan1, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader2 = DataLoader(gan2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader3 = DataLoader(gan3, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "        \n",
    "def sample_images(batches_done, dataloader):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=4, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=4, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=4, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=4, normalize=True)\n",
    "\n",
    "    image_grid = torch.stack((real_A, fake_B, real_B, fake_A), 0)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n",
    "    \n",
    "def train_gan(dataloader, epoch):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "                \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "           \n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "            \n",
    "                print(\n",
    "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        n_epochs,\n",
    "                        i,\n",
    "                        len(dataloader),\n",
    "                        loss_D.item(),\n",
    "                        loss_G.item(),\n",
    "                        loss_GAN.item(),\n",
    "                        loss_cycle.item(),\n",
    "                        loss_identity.item(),\n",
    "                        time_left,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, dataloader)\n",
    "                \n",
    "            \n",
    "            G_losses.append(loss_G.item())\n",
    "            D_losses.append(loss_D.item())\n",
    "            \n",
    "            \n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/101] [Batch 0/476] [D loss: 1.609690] [G loss: 12.644590, adv: 1.950412, cycle: 0.715304, identity: 0.708227] ETA: 13:52:54.623655\n",
      "[Epoch 0/101] [Batch 100/476] [D loss: 0.266397] [G loss: 3.079685, adv: 0.651126, cycle: 0.153377, identity: 0.178958] ETA: 0:58:05.111126\n",
      "[Epoch 0/101] [Batch 200/476] [D loss: 0.376657] [G loss: 1.783553, adv: 0.254658, cycle: 0.100802, identity: 0.104174] ETA: 1:14:20.101214\n",
      "[Epoch 0/101] [Batch 300/476] [D loss: 0.304581] [G loss: 1.593322, adv: 0.170830, cycle: 0.097073, identity: 0.090353] ETA: 1:08:31.206139\n",
      "[Epoch 0/101] [Batch 400/476] [D loss: 0.262373] [G loss: 1.936204, adv: 0.625720, cycle: 0.087964, identity: 0.086169] ETA: 1:05:26.153446\n",
      "[Epoch 1/101] [Batch 0/476] [D loss: 0.169925] [G loss: 2.395388, adv: 0.883057, cycle: 0.099443, identity: 0.103579] ETA: 3:49:06.040249\n",
      "[Epoch 1/101] [Batch 100/476] [D loss: 0.387334] [G loss: 1.834254, adv: 0.323501, cycle: 0.100934, identity: 0.100282] ETA: 1:04:44.649873\n",
      "[Epoch 1/101] [Batch 200/476] [D loss: 0.185397] [G loss: 1.583868, adv: 0.202168, cycle: 0.092415, identity: 0.091509] ETA: 1:13:49.635000\n",
      "[Epoch 1/101] [Batch 300/476] [D loss: 0.162594] [G loss: 1.912327, adv: 0.473890, cycle: 0.094551, identity: 0.098586] ETA: 1:04:30.548892\n",
      "[Epoch 1/101] [Batch 400/476] [D loss: 0.245764] [G loss: 1.506032, adv: 0.246948, cycle: 0.083267, identity: 0.085283] ETA: 0:59:11.435661\n",
      "[Epoch 2/101] [Batch 0/476] [D loss: 0.276611] [G loss: 1.541178, adv: 0.278703, cycle: 0.084591, identity: 0.083313] ETA: 4:16:33.488262\n",
      "[Epoch 2/101] [Batch 100/476] [D loss: 0.168136] [G loss: 1.407785, adv: 0.163322, cycle: 0.081343, identity: 0.086206] ETA: 1:52:24.046650\n",
      "[Epoch 2/101] [Batch 200/476] [D loss: 0.258543] [G loss: 1.696600, adv: 0.273084, cycle: 0.097634, identity: 0.089436] ETA: 1:50:22.449863\n",
      "[Epoch 2/101] [Batch 300/476] [D loss: 0.226861] [G loss: 1.429133, adv: 0.281200, cycle: 0.074791, identity: 0.080004] ETA: 2:00:33.605043\n",
      "[Epoch 2/101] [Batch 400/476] [D loss: 0.381226] [G loss: 1.152485, adv: 0.144697, cycle: 0.067604, identity: 0.066351] ETA: 1:53:16.300819\n",
      "[Epoch 3/101] [Batch 0/476] [D loss: 0.203947] [G loss: 1.534630, adv: 0.250909, cycle: 0.085818, identity: 0.085109] ETA: 4:34:19.055401\n",
      "[Epoch 3/101] [Batch 100/476] [D loss: 0.185194] [G loss: 1.488620, adv: 0.328246, cycle: 0.075836, identity: 0.080404] ETA: 1:01:36.369126\n",
      "[Epoch 3/101] [Batch 200/476] [D loss: 0.124809] [G loss: 1.514628, adv: 0.438865, cycle: 0.072429, identity: 0.070294] ETA: 0:56:49.760342\n",
      "[Epoch 3/101] [Batch 300/476] [D loss: 0.130766] [G loss: 2.362710, adv: 0.403219, cycle: 0.131570, identity: 0.128758] ETA: 1:08:43.601153\n",
      "[Epoch 3/101] [Batch 400/476] [D loss: 0.211226] [G loss: 1.414757, adv: 0.451334, cycle: 0.063071, identity: 0.066543] ETA: 1:04:05.891933\n",
      "[Epoch 4/101] [Batch 0/476] [D loss: 0.108938] [G loss: 1.375832, adv: 0.156820, cycle: 0.080892, identity: 0.082018] ETA: 3:00:47.806198\n",
      "[Epoch 4/101] [Batch 100/476] [D loss: 0.169475] [G loss: 1.498204, adv: 0.279496, cycle: 0.079721, identity: 0.084300] ETA: 1:07:36.524624\n",
      "[Epoch 4/101] [Batch 200/476] [D loss: 0.143422] [G loss: 1.984224, adv: 0.501365, cycle: 0.095842, identity: 0.104887] ETA: 0:59:53.787498\n",
      "[Epoch 4/101] [Batch 300/476] [D loss: 0.445947] [G loss: 1.243883, adv: 0.219278, cycle: 0.068003, identity: 0.068914] ETA: 1:02:58.358299\n",
      "[Epoch 4/101] [Batch 400/476] [D loss: 0.316542] [G loss: 2.037741, adv: 0.856604, cycle: 0.080119, identity: 0.075989] ETA: 1:16:33.172113\n",
      "[Epoch 5/101] [Batch 0/476] [D loss: 0.097734] [G loss: 1.649648, adv: 0.441262, cycle: 0.078418, identity: 0.084841] ETA: 3:50:36.103271\n",
      "[Epoch 5/101] [Batch 100/476] [D loss: 0.163899] [G loss: 1.690719, adv: 0.675595, cycle: 0.066369, identity: 0.070286] ETA: 1:46:03.855361\n",
      "[Epoch 5/101] [Batch 200/476] [D loss: 0.121281] [G loss: 1.642909, adv: 0.568087, cycle: 0.072603, identity: 0.069759] ETA: 1:52:11.021727\n",
      "[Epoch 5/101] [Batch 300/476] [D loss: 0.183945] [G loss: 1.233448, adv: 0.174503, cycle: 0.068737, identity: 0.074315] ETA: 1:12:23.781435\n",
      "[Epoch 5/101] [Batch 400/476] [D loss: 0.231546] [G loss: 1.172240, adv: 0.140947, cycle: 0.068004, identity: 0.070250] ETA: 0:58:48.814545\n",
      "[Epoch 6/101] [Batch 0/476] [D loss: 0.097321] [G loss: 1.767913, adv: 0.462080, cycle: 0.084479, identity: 0.092209] ETA: 2:38:26.411605\n",
      "[Epoch 6/101] [Batch 100/476] [D loss: 0.050053] [G loss: 1.397893, adv: 0.466552, cycle: 0.061181, identity: 0.063905] ETA: 1:04:53.593140\n",
      "[Epoch 6/101] [Batch 200/476] [D loss: 0.174615] [G loss: 1.500198, adv: 0.387918, cycle: 0.070585, identity: 0.081285] ETA: 0:54:04.532480\n",
      "[Epoch 6/101] [Batch 300/476] [D loss: 0.091886] [G loss: 1.228231, adv: 0.285150, cycle: 0.063449, identity: 0.061719] ETA: 0:52:54.577112\n",
      "[Epoch 6/101] [Batch 400/476] [D loss: 0.130803] [G loss: 1.668364, adv: 0.711554, cycle: 0.063189, identity: 0.064983] ETA: 0:56:18.791957\n",
      "[Epoch 7/101] [Batch 0/476] [D loss: 0.102878] [G loss: 1.363826, adv: 0.375034, cycle: 0.065705, identity: 0.066349] ETA: 2:55:42.891623\n",
      "[Epoch 7/101] [Batch 100/476] [D loss: 0.150499] [G loss: 1.478319, adv: 0.345103, cycle: 0.075531, identity: 0.075581] ETA: 0:53:07.322892\n",
      "[Epoch 7/101] [Batch 200/476] [D loss: 0.171937] [G loss: 1.320992, adv: 0.285779, cycle: 0.069576, identity: 0.067890] ETA: 0:53:48.717407\n",
      "[Epoch 7/101] [Batch 300/476] [D loss: 0.226920] [G loss: 1.385990, adv: 0.379909, cycle: 0.065012, identity: 0.071193] ETA: 0:55:05.667033\n",
      "[Epoch 7/101] [Batch 400/476] [D loss: 0.236759] [G loss: 1.250188, adv: 0.359699, cycle: 0.059346, identity: 0.059405] ETA: 0:52:25.330675\n",
      "[Epoch 8/101] [Batch 0/476] [D loss: 0.229800] [G loss: 1.232162, adv: 0.206473, cycle: 0.068942, identity: 0.067253] ETA: 3:16:30.223789\n",
      "[Epoch 8/101] [Batch 100/476] [D loss: 0.144081] [G loss: 1.694285, adv: 0.792921, cycle: 0.059546, identity: 0.061180] ETA: 0:57:46.683969\n",
      "[Epoch 8/101] [Batch 200/476] [D loss: 0.078486] [G loss: 1.377262, adv: 0.370581, cycle: 0.067407, identity: 0.066523] ETA: 0:52:52.109179\n",
      "[Epoch 8/101] [Batch 300/476] [D loss: 0.241737] [G loss: 1.418312, adv: 0.284230, cycle: 0.076486, identity: 0.073844] ETA: 0:55:29.186737\n",
      "[Epoch 8/101] [Batch 400/476] [D loss: 0.277995] [G loss: 1.020320, adv: 0.107953, cycle: 0.059792, identity: 0.062890] ETA: 0:55:29.061666\n",
      "[Epoch 9/101] [Batch 0/476] [D loss: 0.089992] [G loss: 1.429117, adv: 0.388240, cycle: 0.069036, identity: 0.070102] ETA: 2:46:36.819832\n",
      "[Epoch 9/101] [Batch 100/476] [D loss: 0.075349] [G loss: 1.392041, adv: 0.419197, cycle: 0.064241, identity: 0.066087] ETA: 1:22:36.859600\n",
      "[Epoch 9/101] [Batch 200/476] [D loss: 0.149108] [G loss: 1.790888, adv: 0.679438, cycle: 0.072677, identity: 0.076936] ETA: 0:57:15.370174\n",
      "[Epoch 9/101] [Batch 300/476] [D loss: 0.113786] [G loss: 2.000959, adv: 0.981872, cycle: 0.068029, identity: 0.067760] ETA: 0:54:23.208606\n",
      "[Epoch 9/101] [Batch 400/476] [D loss: 0.109710] [G loss: 1.960518, adv: 0.399486, cycle: 0.105656, identity: 0.100894] ETA: 0:55:17.623169\n",
      "[Epoch 10/101] [Batch 0/476] [D loss: 0.076781] [G loss: 1.525238, adv: 0.468171, cycle: 0.068148, identity: 0.075117] ETA: 2:38:20.511744\n",
      "[Epoch 10/101] [Batch 100/476] [D loss: 0.106142] [G loss: 1.476489, adv: 0.439789, cycle: 0.068595, identity: 0.070151] ETA: 0:56:45.542843\n",
      "[Epoch 10/101] [Batch 200/476] [D loss: 0.074139] [G loss: 1.514254, adv: 0.531129, cycle: 0.065399, identity: 0.065826] ETA: 0:53:55.110369\n",
      "[Epoch 10/101] [Batch 300/476] [D loss: 0.276024] [G loss: 1.747971, adv: 0.405150, cycle: 0.089556, identity: 0.089452] ETA: 0:50:05.661036\n",
      "[Epoch 10/101] [Batch 400/476] [D loss: 0.184115] [G loss: 1.308830, adv: 0.332329, cycle: 0.064179, identity: 0.066942] ETA: 0:52:17.050912\n",
      "[Epoch 11/101] [Batch 0/476] [D loss: 0.233981] [G loss: 1.482231, adv: 0.287279, cycle: 0.079107, identity: 0.080776] ETA: 3:06:22.288513\n",
      "[Epoch 11/101] [Batch 100/476] [D loss: 0.113227] [G loss: 1.468638, adv: 0.289032, cycle: 0.077889, identity: 0.080144] ETA: 1:58:42.684569\n",
      "[Epoch 11/101] [Batch 200/476] [D loss: 0.156026] [G loss: 1.316374, adv: 0.253945, cycle: 0.070334, identity: 0.071817] ETA: 1:51:15.665188\n",
      "[Epoch 11/101] [Batch 300/476] [D loss: 0.145793] [G loss: 1.297073, adv: 0.314643, cycle: 0.065672, identity: 0.065142] ETA: 1:33:34.021053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/101] [Batch 400/476] [D loss: 0.178973] [G loss: 1.110549, adv: 0.241015, cycle: 0.058550, identity: 0.056808] ETA: 1:33:39.887238\n",
      "[Epoch 12/101] [Batch 0/476] [D loss: 0.109616] [G loss: 1.263821, adv: 0.296841, cycle: 0.062740, identity: 0.067916] ETA: 3:40:27.195183\n",
      "[Epoch 12/101] [Batch 100/476] [D loss: 0.123288] [G loss: 1.488978, adv: 0.367447, cycle: 0.074167, identity: 0.075973] ETA: 0:55:58.101997\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "dataset_name = 'CycleGANnoSS1'\n",
    "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader1, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
