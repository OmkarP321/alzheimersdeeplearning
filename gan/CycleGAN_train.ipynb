{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from CycleGAN_utils import *\n",
    "from CycleGAN_models import *\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 101\n",
    "dataset_name = 'CycleGAN'\n",
    "batch_size = 1\n",
    "lr=0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "n_cpu = 8\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "channels = 1\n",
    "sample_interval = 500\n",
    "checkpoint_interval = 25\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"../datasets/64dataset.pt\")\n",
    "\n",
    "dataset = [sample for sample in dataset if sample != None]\n",
    "\n",
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "        \n",
    "def process_gan(dataset, s):\n",
    "    output = []\n",
    "    dataset = [sample[0] for sample in dataset]\n",
    "    for sample in dataset:\n",
    "        sample = sample[s][0]\n",
    "        sample /= torch.max(sample)\n",
    "        output.append(torch.unsqueeze(sample, 0))\n",
    "    return output\n",
    "\n",
    "        \n",
    "NCgan1 = process_gan(NC, 0)\n",
    "NCgan2 = process_gan(NC, 1)\n",
    "NCgan3 = process_gan(NC, 2)\n",
    "\n",
    "ADgan1 = process_gan(AD, 0)\n",
    "ADgan2 = process_gan(AD, 1)\n",
    "ADgan3 = process_gan(AD, 2)\n",
    "\n",
    "gan1 = []\n",
    "for i in range(len(ADgan1)):\n",
    "    gan1.append({\"A\": NCgan1[i], \"B\": ADgan1[i]})\n",
    "\n",
    "gan2 = []\n",
    "for i in range(len(ADgan2)):\n",
    "    gan2.append({\"A\": NCgan2[i], \"B\": ADgan2[i]})\n",
    "    \n",
    "gan3 = []\n",
    "for i in range(len(ADgan3)):\n",
    "    gan3.append({\"A\": NCgan3[i], \"B\": ADgan3[i]})\n",
    "\n",
    "batch_size = 1\n",
    "dataloader1 = DataLoader(gan1, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader2 = DataLoader(gan2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader3 = DataLoader(gan3, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "        \n",
    "def sample_images(batches_done, dataloader):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=4, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=4, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=4, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=4, normalize=True)\n",
    "\n",
    "    image_grid = torch.stack((real_A, fake_B, real_B, fake_A), 0)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n",
    "    \n",
    "def train_gan(dataloader, epoch):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "                \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "           \n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "            \n",
    "                print(\n",
    "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        n_epochs,\n",
    "                        i,\n",
    "                        len(dataloader),\n",
    "                        loss_D.item(),\n",
    "                        loss_G.item(),\n",
    "                        loss_GAN.item(),\n",
    "                        loss_cycle.item(),\n",
    "                        loss_identity.item(),\n",
    "                        time_left,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, dataloader)\n",
    "                \n",
    "            \n",
    "            G_losses.append(loss_G.item())\n",
    "            D_losses.append(loss_D.item())\n",
    "            \n",
    "            \n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/101] [Batch 0/476] [D loss: 1.609690] [G loss: 12.644590, adv: 1.950412, cycle: 0.715304, identity: 0.708227] ETA: 13:52:54.623655\n",
      "[Epoch 0/101] [Batch 100/476] [D loss: 0.266397] [G loss: 3.079685, adv: 0.651126, cycle: 0.153377, identity: 0.178958] ETA: 0:58:05.111126\n",
      "[Epoch 0/101] [Batch 200/476] [D loss: 0.376657] [G loss: 1.783553, adv: 0.254658, cycle: 0.100802, identity: 0.104174] ETA: 1:14:20.101214\n",
      "[Epoch 0/101] [Batch 300/476] [D loss: 0.304581] [G loss: 1.593322, adv: 0.170830, cycle: 0.097073, identity: 0.090353] ETA: 1:08:31.206139\n",
      "[Epoch 0/101] [Batch 400/476] [D loss: 0.262373] [G loss: 1.936204, adv: 0.625720, cycle: 0.087964, identity: 0.086169] ETA: 1:05:26.153446\n",
      "[Epoch 1/101] [Batch 0/476] [D loss: 0.169925] [G loss: 2.395388, adv: 0.883057, cycle: 0.099443, identity: 0.103579] ETA: 3:49:06.040249\n",
      "[Epoch 1/101] [Batch 100/476] [D loss: 0.387334] [G loss: 1.834254, adv: 0.323501, cycle: 0.100934, identity: 0.100282] ETA: 1:04:44.649873\n",
      "[Epoch 1/101] [Batch 200/476] [D loss: 0.185397] [G loss: 1.583868, adv: 0.202168, cycle: 0.092415, identity: 0.091509] ETA: 1:13:49.635000\n",
      "[Epoch 1/101] [Batch 300/476] [D loss: 0.162594] [G loss: 1.912327, adv: 0.473890, cycle: 0.094551, identity: 0.098586] ETA: 1:04:30.548892\n",
      "[Epoch 1/101] [Batch 400/476] [D loss: 0.245764] [G loss: 1.506032, adv: 0.246948, cycle: 0.083267, identity: 0.085283] ETA: 0:59:11.435661\n",
      "[Epoch 2/101] [Batch 0/476] [D loss: 0.276611] [G loss: 1.541178, adv: 0.278703, cycle: 0.084591, identity: 0.083313] ETA: 4:16:33.488262\n",
      "[Epoch 2/101] [Batch 100/476] [D loss: 0.168136] [G loss: 1.407785, adv: 0.163322, cycle: 0.081343, identity: 0.086206] ETA: 1:52:24.046650\n",
      "[Epoch 2/101] [Batch 200/476] [D loss: 0.258543] [G loss: 1.696600, adv: 0.273084, cycle: 0.097634, identity: 0.089436] ETA: 1:50:22.449863\n",
      "[Epoch 2/101] [Batch 300/476] [D loss: 0.226861] [G loss: 1.429133, adv: 0.281200, cycle: 0.074791, identity: 0.080004] ETA: 2:00:33.605043\n",
      "[Epoch 2/101] [Batch 400/476] [D loss: 0.381226] [G loss: 1.152485, adv: 0.144697, cycle: 0.067604, identity: 0.066351] ETA: 1:53:16.300819\n",
      "[Epoch 3/101] [Batch 0/476] [D loss: 0.203947] [G loss: 1.534630, adv: 0.250909, cycle: 0.085818, identity: 0.085109] ETA: 4:34:19.055401\n",
      "[Epoch 3/101] [Batch 100/476] [D loss: 0.185194] [G loss: 1.488620, adv: 0.328246, cycle: 0.075836, identity: 0.080404] ETA: 1:01:36.369126\n",
      "[Epoch 3/101] [Batch 200/476] [D loss: 0.124809] [G loss: 1.514628, adv: 0.438865, cycle: 0.072429, identity: 0.070294] ETA: 0:56:49.760342\n",
      "[Epoch 3/101] [Batch 300/476] [D loss: 0.130766] [G loss: 2.362710, adv: 0.403219, cycle: 0.131570, identity: 0.128758] ETA: 1:08:43.601153\n",
      "[Epoch 3/101] [Batch 400/476] [D loss: 0.211226] [G loss: 1.414757, adv: 0.451334, cycle: 0.063071, identity: 0.066543] ETA: 1:04:05.891933\n",
      "[Epoch 4/101] [Batch 0/476] [D loss: 0.108938] [G loss: 1.375832, adv: 0.156820, cycle: 0.080892, identity: 0.082018] ETA: 3:00:47.806198\n",
      "[Epoch 4/101] [Batch 100/476] [D loss: 0.169475] [G loss: 1.498204, adv: 0.279496, cycle: 0.079721, identity: 0.084300] ETA: 1:07:36.524624\n",
      "[Epoch 4/101] [Batch 200/476] [D loss: 0.143422] [G loss: 1.984224, adv: 0.501365, cycle: 0.095842, identity: 0.104887] ETA: 0:59:53.787498\n",
      "[Epoch 4/101] [Batch 300/476] [D loss: 0.445947] [G loss: 1.243883, adv: 0.219278, cycle: 0.068003, identity: 0.068914] ETA: 1:02:58.358299\n",
      "[Epoch 4/101] [Batch 400/476] [D loss: 0.316542] [G loss: 2.037741, adv: 0.856604, cycle: 0.080119, identity: 0.075989] ETA: 1:16:33.172113\n",
      "[Epoch 5/101] [Batch 0/476] [D loss: 0.097734] [G loss: 1.649648, adv: 0.441262, cycle: 0.078418, identity: 0.084841] ETA: 3:50:36.103271\n",
      "[Epoch 5/101] [Batch 100/476] [D loss: 0.163899] [G loss: 1.690719, adv: 0.675595, cycle: 0.066369, identity: 0.070286] ETA: 1:46:03.855361\n",
      "[Epoch 5/101] [Batch 200/476] [D loss: 0.121281] [G loss: 1.642909, adv: 0.568087, cycle: 0.072603, identity: 0.069759] ETA: 1:52:11.021727\n",
      "[Epoch 5/101] [Batch 300/476] [D loss: 0.183945] [G loss: 1.233448, adv: 0.174503, cycle: 0.068737, identity: 0.074315] ETA: 1:12:23.781435\n",
      "[Epoch 5/101] [Batch 400/476] [D loss: 0.231546] [G loss: 1.172240, adv: 0.140947, cycle: 0.068004, identity: 0.070250] ETA: 0:58:48.814545\n",
      "[Epoch 6/101] [Batch 0/476] [D loss: 0.097321] [G loss: 1.767913, adv: 0.462080, cycle: 0.084479, identity: 0.092209] ETA: 2:38:26.411605\n",
      "[Epoch 6/101] [Batch 100/476] [D loss: 0.050053] [G loss: 1.397893, adv: 0.466552, cycle: 0.061181, identity: 0.063905] ETA: 1:04:53.593140\n",
      "[Epoch 6/101] [Batch 200/476] [D loss: 0.174615] [G loss: 1.500198, adv: 0.387918, cycle: 0.070585, identity: 0.081285] ETA: 0:54:04.532480\n",
      "[Epoch 6/101] [Batch 300/476] [D loss: 0.091886] [G loss: 1.228231, adv: 0.285150, cycle: 0.063449, identity: 0.061719] ETA: 0:52:54.577112\n",
      "[Epoch 6/101] [Batch 400/476] [D loss: 0.130803] [G loss: 1.668364, adv: 0.711554, cycle: 0.063189, identity: 0.064983] ETA: 0:56:18.791957\n",
      "[Epoch 7/101] [Batch 0/476] [D loss: 0.102878] [G loss: 1.363826, adv: 0.375034, cycle: 0.065705, identity: 0.066349] ETA: 2:55:42.891623\n",
      "[Epoch 7/101] [Batch 100/476] [D loss: 0.150499] [G loss: 1.478319, adv: 0.345103, cycle: 0.075531, identity: 0.075581] ETA: 0:53:07.322892\n",
      "[Epoch 7/101] [Batch 200/476] [D loss: 0.171937] [G loss: 1.320992, adv: 0.285779, cycle: 0.069576, identity: 0.067890] ETA: 0:53:48.717407\n",
      "[Epoch 7/101] [Batch 300/476] [D loss: 0.226920] [G loss: 1.385990, adv: 0.379909, cycle: 0.065012, identity: 0.071193] ETA: 0:55:05.667033\n",
      "[Epoch 7/101] [Batch 400/476] [D loss: 0.236759] [G loss: 1.250188, adv: 0.359699, cycle: 0.059346, identity: 0.059405] ETA: 0:52:25.330675\n",
      "[Epoch 8/101] [Batch 0/476] [D loss: 0.229800] [G loss: 1.232162, adv: 0.206473, cycle: 0.068942, identity: 0.067253] ETA: 3:16:30.223789\n",
      "[Epoch 8/101] [Batch 100/476] [D loss: 0.144081] [G loss: 1.694285, adv: 0.792921, cycle: 0.059546, identity: 0.061180] ETA: 0:57:46.683969\n",
      "[Epoch 8/101] [Batch 200/476] [D loss: 0.078486] [G loss: 1.377262, adv: 0.370581, cycle: 0.067407, identity: 0.066523] ETA: 0:52:52.109179\n",
      "[Epoch 8/101] [Batch 300/476] [D loss: 0.241737] [G loss: 1.418312, adv: 0.284230, cycle: 0.076486, identity: 0.073844] ETA: 0:55:29.186737\n",
      "[Epoch 8/101] [Batch 400/476] [D loss: 0.277995] [G loss: 1.020320, adv: 0.107953, cycle: 0.059792, identity: 0.062890] ETA: 0:55:29.061666\n",
      "[Epoch 9/101] [Batch 0/476] [D loss: 0.089992] [G loss: 1.429117, adv: 0.388240, cycle: 0.069036, identity: 0.070102] ETA: 2:46:36.819832\n",
      "[Epoch 9/101] [Batch 100/476] [D loss: 0.075349] [G loss: 1.392041, adv: 0.419197, cycle: 0.064241, identity: 0.066087] ETA: 1:22:36.859600\n",
      "[Epoch 9/101] [Batch 200/476] [D loss: 0.149108] [G loss: 1.790888, adv: 0.679438, cycle: 0.072677, identity: 0.076936] ETA: 0:57:15.370174\n",
      "[Epoch 9/101] [Batch 300/476] [D loss: 0.113786] [G loss: 2.000959, adv: 0.981872, cycle: 0.068029, identity: 0.067760] ETA: 0:54:23.208606\n",
      "[Epoch 9/101] [Batch 400/476] [D loss: 0.109710] [G loss: 1.960518, adv: 0.399486, cycle: 0.105656, identity: 0.100894] ETA: 0:55:17.623169\n",
      "[Epoch 10/101] [Batch 0/476] [D loss: 0.076781] [G loss: 1.525238, adv: 0.468171, cycle: 0.068148, identity: 0.075117] ETA: 2:38:20.511744\n",
      "[Epoch 10/101] [Batch 100/476] [D loss: 0.106142] [G loss: 1.476489, adv: 0.439789, cycle: 0.068595, identity: 0.070151] ETA: 0:56:45.542843\n",
      "[Epoch 10/101] [Batch 200/476] [D loss: 0.074139] [G loss: 1.514254, adv: 0.531129, cycle: 0.065399, identity: 0.065826] ETA: 0:53:55.110369\n",
      "[Epoch 10/101] [Batch 300/476] [D loss: 0.276024] [G loss: 1.747971, adv: 0.405150, cycle: 0.089556, identity: 0.089452] ETA: 0:50:05.661036\n",
      "[Epoch 10/101] [Batch 400/476] [D loss: 0.184115] [G loss: 1.308830, adv: 0.332329, cycle: 0.064179, identity: 0.066942] ETA: 0:52:17.050912\n",
      "[Epoch 11/101] [Batch 0/476] [D loss: 0.233981] [G loss: 1.482231, adv: 0.287279, cycle: 0.079107, identity: 0.080776] ETA: 3:06:22.288513\n",
      "[Epoch 11/101] [Batch 100/476] [D loss: 0.113227] [G loss: 1.468638, adv: 0.289032, cycle: 0.077889, identity: 0.080144] ETA: 1:58:42.684569\n",
      "[Epoch 11/101] [Batch 200/476] [D loss: 0.156026] [G loss: 1.316374, adv: 0.253945, cycle: 0.070334, identity: 0.071817] ETA: 1:51:15.665188\n",
      "[Epoch 11/101] [Batch 300/476] [D loss: 0.145793] [G loss: 1.297073, adv: 0.314643, cycle: 0.065672, identity: 0.065142] ETA: 1:33:34.021053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/101] [Batch 400/476] [D loss: 0.178973] [G loss: 1.110549, adv: 0.241015, cycle: 0.058550, identity: 0.056808] ETA: 1:33:39.887238\n",
      "[Epoch 12/101] [Batch 0/476] [D loss: 0.109616] [G loss: 1.263821, adv: 0.296841, cycle: 0.062740, identity: 0.067916] ETA: 3:40:27.195183\n",
      "[Epoch 12/101] [Batch 100/476] [D loss: 0.123288] [G loss: 1.488978, adv: 0.367447, cycle: 0.074167, identity: 0.075973] ETA: 0:55:58.101997\n",
      "[Epoch 12/101] [Batch 200/476] [D loss: 0.154466] [G loss: 1.202550, adv: 0.262747, cycle: 0.062231, identity: 0.063499] ETA: 0:50:04.012496\n",
      "[Epoch 12/101] [Batch 300/476] [D loss: 0.051970] [G loss: 1.496055, adv: 0.457875, cycle: 0.068859, identity: 0.069918] ETA: 0:47:58.497459\n",
      "[Epoch 12/101] [Batch 400/476] [D loss: 0.284775] [G loss: 1.855143, adv: 0.801866, cycle: 0.070766, identity: 0.069123] ETA: 0:49:38.877877\n",
      "[Epoch 13/101] [Batch 0/476] [D loss: 0.145130] [G loss: 1.401749, adv: 0.283569, cycle: 0.076804, identity: 0.070027] ETA: 2:35:42.284424\n",
      "[Epoch 13/101] [Batch 100/476] [D loss: 0.267584] [G loss: 1.472512, adv: 0.157824, cycle: 0.084845, identity: 0.093248] ETA: 0:51:58.230877\n",
      "[Epoch 13/101] [Batch 200/476] [D loss: 0.057611] [G loss: 1.624448, adv: 0.680393, cycle: 0.060246, identity: 0.068320] ETA: 0:55:13.319681\n",
      "[Epoch 13/101] [Batch 300/476] [D loss: 0.273147] [G loss: 1.615166, adv: 0.692961, cycle: 0.061734, identity: 0.060973] ETA: 0:51:34.997763\n",
      "[Epoch 13/101] [Batch 400/476] [D loss: 0.314817] [G loss: 1.735707, adv: 0.881490, cycle: 0.057630, identity: 0.055583] ETA: 0:50:45.229946\n",
      "[Epoch 14/101] [Batch 0/476] [D loss: 0.157978] [G loss: 1.155863, adv: 0.211264, cycle: 0.062858, identity: 0.063203] ETA: 2:42:42.077597\n",
      "[Epoch 14/101] [Batch 100/476] [D loss: 0.249036] [G loss: 1.346534, adv: 0.170112, cycle: 0.077345, identity: 0.080595] ETA: 0:49:52.913307\n",
      "[Epoch 14/101] [Batch 200/476] [D loss: 0.276713] [G loss: 1.385247, adv: 0.272895, cycle: 0.073429, identity: 0.075613] ETA: 0:48:52.472274\n",
      "[Epoch 14/101] [Batch 300/476] [D loss: 0.078949] [G loss: 1.642104, adv: 0.616793, cycle: 0.066909, identity: 0.071244] ETA: 0:49:08.675331\n",
      "[Epoch 14/101] [Batch 400/476] [D loss: 0.124688] [G loss: 1.734601, adv: 0.448293, cycle: 0.086088, identity: 0.085086] ETA: 0:47:56.215175\n",
      "[Epoch 15/101] [Batch 0/476] [D loss: 0.138542] [G loss: 0.925643, adv: 0.102250, cycle: 0.053708, identity: 0.057263] ETA: 2:32:51.595524\n",
      "[Epoch 15/101] [Batch 100/476] [D loss: 0.106921] [G loss: 1.442817, adv: 0.565671, cycle: 0.058725, identity: 0.057979] ETA: 0:47:18.704390\n",
      "[Epoch 15/101] [Batch 200/476] [D loss: 0.119590] [G loss: 1.470214, adv: 0.648376, cycle: 0.053959, identity: 0.056449] ETA: 0:55:50.346573\n",
      "[Epoch 15/101] [Batch 300/476] [D loss: 0.070324] [G loss: 1.522482, adv: 0.691841, cycle: 0.055357, identity: 0.055415] ETA: 0:49:08.812670\n",
      "[Epoch 15/101] [Batch 400/476] [D loss: 0.055197] [G loss: 1.600263, adv: 0.542640, cycle: 0.070703, identity: 0.070118] ETA: 0:56:04.553410\n",
      "[Epoch 16/101] [Batch 0/476] [D loss: 0.025150] [G loss: 1.314614, adv: 0.506742, cycle: 0.056041, identity: 0.049493] ETA: 2:34:51.147866\n",
      "[Epoch 16/101] [Batch 100/476] [D loss: 0.069409] [G loss: 1.391317, adv: 0.417264, cycle: 0.066402, identity: 0.062006] ETA: 0:45:58.387775\n",
      "[Epoch 16/101] [Batch 200/476] [D loss: 0.062617] [G loss: 1.403057, adv: 0.469254, cycle: 0.063113, identity: 0.060533] ETA: 0:51:44.364309\n",
      "[Epoch 16/101] [Batch 300/476] [D loss: 0.129530] [G loss: 1.422510, adv: 0.296579, cycle: 0.075797, identity: 0.073593] ETA: 0:46:52.049484\n",
      "[Epoch 16/101] [Batch 400/476] [D loss: 0.066481] [G loss: 1.436960, adv: 0.430327, cycle: 0.067814, identity: 0.065698] ETA: 0:52:22.142067\n",
      "[Epoch 17/101] [Batch 0/476] [D loss: 0.085193] [G loss: 1.673513, adv: 0.496828, cycle: 0.079001, identity: 0.077335] ETA: 2:52:13.332207\n",
      "[Epoch 17/101] [Batch 100/476] [D loss: 0.230493] [G loss: 1.581272, adv: 0.397608, cycle: 0.079661, identity: 0.077410] ETA: 0:53:41.507380\n",
      "[Epoch 17/101] [Batch 200/476] [D loss: 0.079805] [G loss: 1.766794, adv: 0.807434, cycle: 0.064189, identity: 0.063494] ETA: 0:53:07.450102\n",
      "[Epoch 17/101] [Batch 300/476] [D loss: 0.254775] [G loss: 0.953511, adv: 0.062685, cycle: 0.059863, identity: 0.058439] ETA: 0:53:43.310738\n",
      "[Epoch 17/101] [Batch 400/476] [D loss: 0.093911] [G loss: 1.560775, adv: 0.552153, cycle: 0.066761, identity: 0.068203] ETA: 0:49:18.014755\n",
      "[Epoch 18/101] [Batch 0/476] [D loss: 0.126764] [G loss: 1.610199, adv: 0.636231, cycle: 0.065801, identity: 0.063192] ETA: 2:24:13.923235\n",
      "[Epoch 18/101] [Batch 100/476] [D loss: 0.094759] [G loss: 1.681263, adv: 0.419108, cycle: 0.083040, identity: 0.086350] ETA: 0:48:42.717281\n",
      "[Epoch 18/101] [Batch 200/476] [D loss: 0.096757] [G loss: 1.336895, adv: 0.410415, cycle: 0.058795, identity: 0.067705] ETA: 0:44:15.103248\n",
      "[Epoch 18/101] [Batch 300/476] [D loss: 0.063882] [G loss: 1.931122, adv: 0.965934, cycle: 0.063864, identity: 0.065311] ETA: 0:46:19.873808\n",
      "[Epoch 18/101] [Batch 400/476] [D loss: 0.028786] [G loss: 1.120633, adv: 0.243370, cycle: 0.060235, identity: 0.054982] ETA: 0:47:01.940257\n",
      "[Epoch 19/101] [Batch 0/476] [D loss: 0.069164] [G loss: 1.504998, adv: 0.526393, cycle: 0.066171, identity: 0.063379] ETA: 2:20:55.399092\n",
      "[Epoch 19/101] [Batch 100/476] [D loss: 0.145117] [G loss: 1.504794, adv: 0.438399, cycle: 0.068584, identity: 0.076111] ETA: 0:50:33.384959\n",
      "[Epoch 19/101] [Batch 200/476] [D loss: 0.204408] [G loss: 1.960634, adv: 0.880619, cycle: 0.073346, identity: 0.069311] ETA: 0:45:26.801308\n",
      "[Epoch 19/101] [Batch 300/476] [D loss: 0.181027] [G loss: 1.356770, adv: 0.378758, cycle: 0.064144, identity: 0.067315] ETA: 0:44:38.815336\n",
      "[Epoch 19/101] [Batch 400/476] [D loss: 0.053007] [G loss: 1.574069, adv: 0.479539, cycle: 0.073641, identity: 0.071624] ETA: 0:46:02.872015\n",
      "[Epoch 20/101] [Batch 0/476] [D loss: 0.193485] [G loss: 1.408954, adv: 0.400493, cycle: 0.065849, identity: 0.069995] ETA: 2:20:46.084399\n",
      "[Epoch 20/101] [Batch 100/476] [D loss: 0.077068] [G loss: 1.348977, adv: 0.429438, cycle: 0.063431, identity: 0.057047] ETA: 0:48:14.828270\n",
      "[Epoch 20/101] [Batch 200/476] [D loss: 0.170992] [G loss: 1.694454, adv: 0.708146, cycle: 0.067774, identity: 0.061713] ETA: 0:46:10.466885\n",
      "[Epoch 20/101] [Batch 300/476] [D loss: 0.080497] [G loss: 1.478699, adv: 0.593534, cycle: 0.060337, identity: 0.056359] ETA: 0:43:25.013763\n",
      "[Epoch 20/101] [Batch 400/476] [D loss: 0.070497] [G loss: 1.577521, adv: 0.368864, cycle: 0.078911, identity: 0.083909] ETA: 0:48:25.122274\n",
      "[Epoch 21/101] [Batch 0/476] [D loss: 0.021268] [G loss: 1.602629, adv: 0.675530, cycle: 0.061105, identity: 0.063210] ETA: 2:15:06.375656\n",
      "[Epoch 21/101] [Batch 100/476] [D loss: 0.172586] [G loss: 1.301930, adv: 0.423040, cycle: 0.059301, identity: 0.057177] ETA: 0:48:44.673629\n",
      "[Epoch 21/101] [Batch 200/476] [D loss: 0.058927] [G loss: 1.853301, adv: 0.859263, cycle: 0.066955, identity: 0.064898] ETA: 0:47:03.607531\n",
      "[Epoch 21/101] [Batch 300/476] [D loss: 0.098879] [G loss: 1.567207, adv: 0.586032, cycle: 0.064695, identity: 0.066844] ETA: 0:49:26.073508\n",
      "[Epoch 21/101] [Batch 400/476] [D loss: 0.065172] [G loss: 2.156703, adv: 1.053995, cycle: 0.074178, identity: 0.072186] ETA: 0:47:19.602985\n",
      "[Epoch 22/101] [Batch 0/476] [D loss: 0.087376] [G loss: 1.298753, adv: 0.404977, cycle: 0.058072, identity: 0.062612] ETA: 2:14:08.241702\n",
      "[Epoch 22/101] [Batch 100/476] [D loss: 0.151123] [G loss: 1.298369, adv: 0.340875, cycle: 0.066797, identity: 0.057905] ETA: 0:50:54.440948\n",
      "[Epoch 22/101] [Batch 200/476] [D loss: 0.078935] [G loss: 1.608938, adv: 0.713018, cycle: 0.061721, identity: 0.055743] ETA: 0:45:30.668644\n",
      "[Epoch 22/101] [Batch 300/476] [D loss: 0.119629] [G loss: 1.873491, adv: 0.808374, cycle: 0.069676, identity: 0.073672] ETA: 0:45:28.322111\n",
      "[Epoch 22/101] [Batch 400/476] [D loss: 0.061610] [G loss: 1.475045, adv: 0.630758, cycle: 0.058838, identity: 0.051181] ETA: 0:49:37.896044\n",
      "[Epoch 23/101] [Batch 0/476] [D loss: 0.033627] [G loss: 1.158903, adv: 0.270673, cycle: 0.058408, identity: 0.060830] ETA: 2:13:20.300491\n",
      "[Epoch 23/101] [Batch 100/476] [D loss: 0.105731] [G loss: 1.451798, adv: 0.539459, cycle: 0.062020, identity: 0.058428] ETA: 0:46:19.503026\n",
      "[Epoch 23/101] [Batch 200/476] [D loss: 0.096319] [G loss: 1.695403, adv: 0.515180, cycle: 0.078336, identity: 0.079372] ETA: 0:42:33.350021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/101] [Batch 300/476] [D loss: 0.029094] [G loss: 1.779685, adv: 0.884906, cycle: 0.059449, identity: 0.060058] ETA: 0:48:59.370855\n",
      "[Epoch 23/101] [Batch 400/476] [D loss: 0.140240] [G loss: 1.427425, adv: 0.640051, cycle: 0.052411, identity: 0.052653] ETA: 0:46:03.761194\n",
      "[Epoch 24/101] [Batch 0/476] [D loss: 0.140137] [G loss: 1.228270, adv: 0.429056, cycle: 0.054517, identity: 0.050809] ETA: 2:17:32.979499\n",
      "[Epoch 24/101] [Batch 100/476] [D loss: 0.074013] [G loss: 1.673724, adv: 0.777222, cycle: 0.059960, identity: 0.059381] ETA: 0:45:19.135746\n",
      "[Epoch 24/101] [Batch 200/476] [D loss: 0.072118] [G loss: 1.388967, adv: 0.551588, cycle: 0.054370, identity: 0.058737] ETA: 0:46:07.109415\n",
      "[Epoch 24/101] [Batch 300/476] [D loss: 0.030256] [G loss: 1.170304, adv: 0.375762, cycle: 0.054300, identity: 0.050308] ETA: 0:43:32.682129\n",
      "[Epoch 24/101] [Batch 400/476] [D loss: 0.072497] [G loss: 1.871025, adv: 0.608661, cycle: 0.082888, identity: 0.086697] ETA: 0:42:28.778601\n",
      "[Epoch 25/101] [Batch 0/476] [D loss: 0.068976] [G loss: 1.513922, adv: 0.636603, cycle: 0.059950, identity: 0.055564] ETA: 2:14:03.729336\n",
      "[Epoch 25/101] [Batch 100/476] [D loss: 0.032904] [G loss: 1.311574, adv: 0.463349, cycle: 0.058390, identity: 0.052865] ETA: 0:45:43.366325\n",
      "[Epoch 25/101] [Batch 200/476] [D loss: 0.103061] [G loss: 1.474889, adv: 0.516558, cycle: 0.065725, identity: 0.060216] ETA: 0:44:15.975437\n",
      "[Epoch 25/101] [Batch 300/476] [D loss: 0.054506] [G loss: 1.397601, adv: 0.489285, cycle: 0.063150, identity: 0.055363] ETA: 0:45:50.208445\n",
      "[Epoch 25/101] [Batch 400/476] [D loss: 0.087598] [G loss: 1.444365, adv: 0.583815, cycle: 0.057472, identity: 0.057165] ETA: 0:42:38.276260\n",
      "[Epoch 26/101] [Batch 0/476] [D loss: 0.041398] [G loss: 1.813280, adv: 0.826236, cycle: 0.067630, identity: 0.062149] ETA: 2:47:54.449873\n",
      "[Epoch 26/101] [Batch 100/476] [D loss: 0.075797] [G loss: 1.411204, adv: 0.546674, cycle: 0.059209, identity: 0.054487] ETA: 0:41:50.526276\n",
      "[Epoch 26/101] [Batch 200/476] [D loss: 0.059577] [G loss: 1.593632, adv: 0.636913, cycle: 0.064554, identity: 0.062236] ETA: 0:41:09.915032\n",
      "[Epoch 26/101] [Batch 300/476] [D loss: 0.060787] [G loss: 1.308235, adv: 0.493921, cycle: 0.056619, identity: 0.049625] ETA: 0:43:41.663618\n",
      "[Epoch 26/101] [Batch 400/476] [D loss: 0.040620] [G loss: 1.445261, adv: 0.545496, cycle: 0.061114, identity: 0.057724] ETA: 0:43:35.545464\n",
      "[Epoch 27/101] [Batch 0/476] [D loss: 0.049357] [G loss: 1.574573, adv: 0.788500, cycle: 0.054043, identity: 0.049128] ETA: 1:56:18.263882\n",
      "[Epoch 27/101] [Batch 100/476] [D loss: 0.107511] [G loss: 2.074103, adv: 1.056237, cycle: 0.068466, identity: 0.066642] ETA: 0:46:11.362438\n",
      "[Epoch 27/101] [Batch 200/476] [D loss: 0.024460] [G loss: 1.415642, adv: 0.559086, cycle: 0.058265, identity: 0.054781] ETA: 0:40:38.584278\n",
      "[Epoch 27/101] [Batch 300/476] [D loss: 0.030985] [G loss: 1.388644, adv: 0.445448, cycle: 0.061603, identity: 0.065433] ETA: 0:40:33.703301\n",
      "[Epoch 27/101] [Batch 400/476] [D loss: 0.060514] [G loss: 1.230503, adv: 0.302920, cycle: 0.063641, identity: 0.058235] ETA: 0:38:39.165098\n",
      "[Epoch 28/101] [Batch 0/476] [D loss: 0.038708] [G loss: 1.349513, adv: 0.542178, cycle: 0.055404, identity: 0.050659] ETA: 1:55:44.191833\n",
      "[Epoch 28/101] [Batch 100/476] [D loss: 0.054676] [G loss: 1.231898, adv: 0.353984, cycle: 0.058961, identity: 0.057662] ETA: 0:41:24.132759\n",
      "[Epoch 28/101] [Batch 200/476] [D loss: 0.146278] [G loss: 2.021897, adv: 0.987629, cycle: 0.070663, identity: 0.065527] ETA: 0:39:55.813339\n",
      "[Epoch 28/101] [Batch 300/476] [D loss: 0.048686] [G loss: 1.294535, adv: 0.350824, cycle: 0.064477, identity: 0.059788] ETA: 0:40:04.581940\n",
      "[Epoch 28/101] [Batch 400/476] [D loss: 0.062836] [G loss: 1.604298, adv: 0.762478, cycle: 0.056378, identity: 0.055608] ETA: 0:41:06.055150\n",
      "[Epoch 29/101] [Batch 0/476] [D loss: 0.061480] [G loss: 1.082847, adv: 0.363943, cycle: 0.048023, identity: 0.047734] ETA: 1:57:25.866417\n",
      "[Epoch 29/101] [Batch 100/476] [D loss: 0.092298] [G loss: 1.896904, adv: 0.693547, cycle: 0.081128, identity: 0.078415] ETA: 0:40:08.120370\n",
      "[Epoch 29/101] [Batch 200/476] [D loss: 0.065856] [G loss: 1.642340, adv: 0.823161, cycle: 0.055686, identity: 0.052464] ETA: 0:37:45.688765\n",
      "[Epoch 29/101] [Batch 300/476] [D loss: 0.027343] [G loss: 1.516485, adv: 0.654601, cycle: 0.057751, identity: 0.056875] ETA: 0:41:21.865940\n",
      "[Epoch 29/101] [Batch 400/476] [D loss: 0.098046] [G loss: 1.347957, adv: 0.427815, cycle: 0.063043, identity: 0.057942] ETA: 0:40:48.645351\n",
      "[Epoch 30/101] [Batch 0/476] [D loss: 0.049436] [G loss: 1.517850, adv: 0.613350, cycle: 0.061922, identity: 0.057056] ETA: 1:53:48.650017\n",
      "[Epoch 30/101] [Batch 100/476] [D loss: 0.077518] [G loss: 1.864369, adv: 0.856963, cycle: 0.068613, identity: 0.064255] ETA: 0:38:30.410797\n",
      "[Epoch 30/101] [Batch 200/476] [D loss: 0.140789] [G loss: 1.368623, adv: 0.499695, cycle: 0.059891, identity: 0.054003] ETA: 0:37:59.964978\n",
      "[Epoch 30/101] [Batch 300/476] [D loss: 0.085032] [G loss: 2.267833, adv: 1.217425, cycle: 0.072516, identity: 0.065049] ETA: 0:39:14.293060\n",
      "[Epoch 30/101] [Batch 400/476] [D loss: 0.050727] [G loss: 1.587117, adv: 0.715063, cycle: 0.058072, identity: 0.058267] ETA: 0:39:24.343457\n",
      "[Epoch 31/101] [Batch 0/476] [D loss: 0.026693] [G loss: 1.604537, adv: 0.844826, cycle: 0.052469, identity: 0.047005] ETA: 1:51:51.014814\n",
      "[Epoch 31/101] [Batch 100/476] [D loss: 0.085882] [G loss: 1.765396, adv: 0.781316, cycle: 0.067532, identity: 0.061753] ETA: 0:37:38.146811\n",
      "[Epoch 31/101] [Batch 200/476] [D loss: 0.083685] [G loss: 1.261033, adv: 0.502427, cycle: 0.052897, identity: 0.045926] ETA: 0:39:08.956947\n",
      "[Epoch 31/101] [Batch 300/476] [D loss: 0.088493] [G loss: 1.859661, adv: 0.804036, cycle: 0.072375, identity: 0.066375] ETA: 0:37:35.486722\n",
      "[Epoch 31/101] [Batch 400/476] [D loss: 0.127079] [G loss: 1.431839, adv: 0.642621, cycle: 0.053752, identity: 0.050340] ETA: 0:37:56.652508\n",
      "[Epoch 32/101] [Batch 0/476] [D loss: 0.075159] [G loss: 1.486807, adv: 0.625247, cycle: 0.059657, identity: 0.052998] ETA: 1:45:32.199389\n",
      "[Epoch 32/101] [Batch 100/476] [D loss: 0.100946] [G loss: 1.543516, adv: 0.759656, cycle: 0.054012, identity: 0.048749] ETA: 0:37:17.984650\n",
      "[Epoch 32/101] [Batch 200/476] [D loss: 0.077538] [G loss: 1.547151, adv: 0.609181, cycle: 0.064345, identity: 0.058905] ETA: 0:40:02.358885\n",
      "[Epoch 32/101] [Batch 300/476] [D loss: 0.046489] [G loss: 1.492759, adv: 0.638698, cycle: 0.058090, identity: 0.054632] ETA: 0:39:15.304092\n",
      "[Epoch 32/101] [Batch 400/476] [D loss: 0.022794] [G loss: 1.692671, adv: 0.846546, cycle: 0.059025, identity: 0.051175] ETA: 0:47:59.455434\n",
      "[Epoch 33/101] [Batch 0/476] [D loss: 0.060590] [G loss: 1.195773, adv: 0.369929, cycle: 0.056114, identity: 0.052940] ETA: 1:55:57.789875\n",
      "[Epoch 33/101] [Batch 100/476] [D loss: 0.050805] [G loss: 1.797411, adv: 1.002130, cycle: 0.054612, identity: 0.049832] ETA: 0:42:59.244950\n",
      "[Epoch 33/101] [Batch 200/476] [D loss: 0.023748] [G loss: 1.661575, adv: 0.653601, cycle: 0.068834, identity: 0.063928] ETA: 0:36:41.400251\n",
      "[Epoch 33/101] [Batch 300/476] [D loss: 0.038124] [G loss: 1.768275, adv: 0.947664, cycle: 0.056870, identity: 0.050382] ETA: 0:43:45.707873\n",
      "[Epoch 33/101] [Batch 400/476] [D loss: 0.058982] [G loss: 1.066726, adv: 0.220111, cycle: 0.056160, identity: 0.057003] ETA: 0:39:19.096367\n",
      "[Epoch 34/101] [Batch 0/476] [D loss: 0.043857] [G loss: 1.471257, adv: 0.356705, cycle: 0.076418, identity: 0.070075] ETA: 2:02:31.196696\n",
      "[Epoch 34/101] [Batch 100/476] [D loss: 0.099369] [G loss: 1.661984, adv: 0.419852, cycle: 0.079290, identity: 0.089846] ETA: 0:37:28.624496\n",
      "[Epoch 34/101] [Batch 200/476] [D loss: 0.044605] [G loss: 1.428161, adv: 0.645789, cycle: 0.053560, identity: 0.049355] ETA: 0:38:11.345358\n",
      "[Epoch 34/101] [Batch 300/476] [D loss: 0.074630] [G loss: 1.754030, adv: 0.857633, cycle: 0.060375, identity: 0.058530] ETA: 0:42:23.257895\n",
      "[Epoch 34/101] [Batch 400/476] [D loss: 0.034147] [G loss: 1.473831, adv: 0.710357, cycle: 0.052821, identity: 0.047052] ETA: 0:41:40.016273\n",
      "[Epoch 35/101] [Batch 0/476] [D loss: 0.054113] [G loss: 1.343503, adv: 0.435612, cycle: 0.060333, identity: 0.060912] ETA: 1:47:50.837448\n",
      "[Epoch 35/101] [Batch 100/476] [D loss: 0.022280] [G loss: 1.699378, adv: 0.783427, cycle: 0.061241, identity: 0.060708] ETA: 0:36:52.963864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/101] [Batch 200/476] [D loss: 0.058630] [G loss: 1.378194, adv: 0.363062, cycle: 0.070517, identity: 0.061993] ETA: 0:38:31.662308\n",
      "[Epoch 35/101] [Batch 300/476] [D loss: 0.160850] [G loss: 1.526265, adv: 0.544249, cycle: 0.067598, identity: 0.061207] ETA: 0:37:08.816857\n",
      "[Epoch 35/101] [Batch 400/476] [D loss: 0.018904] [G loss: 1.194218, adv: 0.369786, cycle: 0.057022, identity: 0.050843] ETA: 0:39:45.248882\n",
      "[Epoch 36/101] [Batch 0/476] [D loss: 0.090697] [G loss: 2.046805, adv: 1.255673, cycle: 0.055221, identity: 0.047785] ETA: 1:47:19.981174\n",
      "[Epoch 36/101] [Batch 100/476] [D loss: 0.020507] [G loss: 1.469073, adv: 0.587356, cycle: 0.061260, identity: 0.053824] ETA: 0:32:59.477148\n",
      "[Epoch 36/101] [Batch 200/476] [D loss: 0.025365] [G loss: 1.751504, adv: 1.015834, cycle: 0.050533, identity: 0.046069] ETA: 0:36:12.326441\n",
      "[Epoch 36/101] [Batch 300/476] [D loss: 0.053716] [G loss: 1.629889, adv: 0.500872, cycle: 0.078601, identity: 0.068602] ETA: 0:35:33.701439\n",
      "[Epoch 36/101] [Batch 400/476] [D loss: 0.041397] [G loss: 1.770808, adv: 0.854056, cycle: 0.063755, identity: 0.055841] ETA: 0:38:10.850959\n",
      "[Epoch 37/101] [Batch 0/476] [D loss: 0.133262] [G loss: 1.368812, adv: 0.470096, cycle: 0.063509, identity: 0.052725] ETA: 1:46:11.678650\n",
      "[Epoch 37/101] [Batch 100/476] [D loss: 0.045322] [G loss: 1.369560, adv: 0.643965, cycle: 0.049777, identity: 0.045565] ETA: 0:38:22.269938\n",
      "[Epoch 37/101] [Batch 200/476] [D loss: 0.079549] [G loss: 1.519452, adv: 0.661424, cycle: 0.057646, identity: 0.056314] ETA: 0:34:04.620123\n",
      "[Epoch 37/101] [Batch 300/476] [D loss: 0.057050] [G loss: 1.438652, adv: 0.634033, cycle: 0.056287, identity: 0.048349] ETA: 0:34:53.822466\n",
      "[Epoch 37/101] [Batch 400/476] [D loss: 0.022586] [G loss: 1.633413, adv: 0.898105, cycle: 0.050791, identity: 0.045481] ETA: 0:34:27.900627\n",
      "[Epoch 38/101] [Batch 0/476] [D loss: 0.065258] [G loss: 1.918730, adv: 0.821136, cycle: 0.074066, identity: 0.071387] ETA: 1:43:25.414504\n",
      "[Epoch 38/101] [Batch 100/476] [D loss: 0.054201] [G loss: 1.501309, adv: 0.717161, cycle: 0.055551, identity: 0.045728] ETA: 0:36:58.257126\n",
      "[Epoch 38/101] [Batch 200/476] [D loss: 0.081992] [G loss: 1.202708, adv: 0.398126, cycle: 0.055896, identity: 0.049124] ETA: 0:31:57.394269\n",
      "[Epoch 38/101] [Batch 300/476] [D loss: 0.027711] [G loss: 1.401111, adv: 0.594301, cycle: 0.054646, identity: 0.052069] ETA: 0:38:43.345854\n",
      "[Epoch 38/101] [Batch 400/476] [D loss: 0.079838] [G loss: 1.640802, adv: 0.755213, cycle: 0.060922, identity: 0.055274] ETA: 0:36:29.071133\n",
      "[Epoch 39/101] [Batch 0/476] [D loss: 0.031935] [G loss: 1.346142, adv: 0.561809, cycle: 0.052334, identity: 0.052199] ETA: 1:40:39.494905\n",
      "[Epoch 39/101] [Batch 100/476] [D loss: 0.027075] [G loss: 1.612076, adv: 0.839668, cycle: 0.054636, identity: 0.045211] ETA: 0:39:27.999284\n",
      "[Epoch 39/101] [Batch 200/476] [D loss: 0.046543] [G loss: 1.557772, adv: 0.495828, cycle: 0.074506, identity: 0.063377] ETA: 0:32:50.372803\n",
      "[Epoch 39/101] [Batch 300/476] [D loss: 0.149056] [G loss: 1.478225, adv: 0.602059, cycle: 0.058268, identity: 0.058697] ETA: 0:35:13.635194\n",
      "[Epoch 39/101] [Batch 400/476] [D loss: 0.027914] [G loss: 1.739562, adv: 0.817703, cycle: 0.064014, identity: 0.056344] ETA: 0:32:50.733999\n",
      "[Epoch 40/101] [Batch 0/476] [D loss: 0.038482] [G loss: 1.436280, adv: 0.697386, cycle: 0.050483, identity: 0.046813] ETA: 1:43:29.778429\n",
      "[Epoch 40/101] [Batch 100/476] [D loss: 0.023933] [G loss: 1.644381, adv: 0.719755, cycle: 0.064910, identity: 0.055106] ETA: 0:34:05.172977\n",
      "[Epoch 40/101] [Batch 200/476] [D loss: 0.033216] [G loss: 1.873262, adv: 0.989276, cycle: 0.059011, identity: 0.058775] ETA: 0:34:47.756584\n",
      "[Epoch 40/101] [Batch 300/476] [D loss: 0.046572] [G loss: 1.578260, adv: 0.717865, cycle: 0.060168, identity: 0.051743] ETA: 0:35:24.795761\n",
      "[Epoch 40/101] [Batch 400/476] [D loss: 0.030937] [G loss: 1.676738, adv: 0.683021, cycle: 0.064379, identity: 0.069985] ETA: 0:29:54.535975\n",
      "[Epoch 41/101] [Batch 0/476] [D loss: 0.057641] [G loss: 1.529298, adv: 0.626976, cycle: 0.063042, identity: 0.054381] ETA: 1:40:00.801430\n",
      "[Epoch 41/101] [Batch 100/476] [D loss: 0.026210] [G loss: 1.648472, adv: 0.775637, cycle: 0.061693, identity: 0.051181] ETA: 0:38:57.364244\n",
      "[Epoch 41/101] [Batch 200/476] [D loss: 0.121422] [G loss: 1.315635, adv: 0.394944, cycle: 0.064110, identity: 0.055919] ETA: 0:35:07.480755\n",
      "[Epoch 41/101] [Batch 300/476] [D loss: 0.039807] [G loss: 1.334008, adv: 0.521374, cycle: 0.055145, identity: 0.052236] ETA: 0:37:00.876918\n",
      "[Epoch 41/101] [Batch 400/476] [D loss: 0.062539] [G loss: 1.789577, adv: 0.819264, cycle: 0.068207, identity: 0.057648] ETA: 0:34:31.375732\n",
      "[Epoch 42/101] [Batch 0/476] [D loss: 0.035017] [G loss: 1.509814, adv: 0.604704, cycle: 0.063403, identity: 0.054216] ETA: 1:41:36.123111\n",
      "[Epoch 42/101] [Batch 100/476] [D loss: 0.028507] [G loss: 1.540154, adv: 0.740275, cycle: 0.055970, identity: 0.048035] ETA: 0:33:52.409203\n",
      "[Epoch 42/101] [Batch 200/476] [D loss: 0.039037] [G loss: 1.581130, adv: 0.675535, cycle: 0.061266, identity: 0.058586] ETA: 0:34:35.485586\n",
      "[Epoch 42/101] [Batch 300/476] [D loss: 0.031708] [G loss: 1.482092, adv: 0.641360, cycle: 0.057615, identity: 0.052917] ETA: 0:32:43.425966\n",
      "[Epoch 42/101] [Batch 400/476] [D loss: 0.074214] [G loss: 1.621419, adv: 0.714442, cycle: 0.063127, identity: 0.055141] ETA: 0:32:40.438251\n",
      "[Epoch 43/101] [Batch 0/476] [D loss: 0.051248] [G loss: 1.979123, adv: 1.115015, cycle: 0.060099, identity: 0.052623] ETA: 1:40:40.809996\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "dataset_name = 'CycleGANnoSS1'\n",
    "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader1, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
