{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from CycleGAN_utils import *\n",
    "from CycleGAN_models import *\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 101\n",
    "dataset_name = 'CycleGAN3'\n",
    "batch_size = 1\n",
    "lr=0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "n_cpu = 8\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "channels = 1\n",
    "sample_interval = 500\n",
    "checkpoint_interval = 25\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"../datasets/64skulldataset.pt\")\n",
    "\n",
    "dataset = [sample for sample in dataset if sample != None]\n",
    "\n",
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "def process_gan(dataset, s):\n",
    "    \n",
    "    output = []\n",
    "    dataset = [sample[0] for sample in dataset]\n",
    "    for sample in dataset:\n",
    "        sample = sample[s][0]\n",
    "        sample /= torch.max(sample)\n",
    "        output.append(torch.unsqueeze(sample, 0))\n",
    "    return output\n",
    "\n",
    "        \n",
    "NCgan1 = process_gan(NC, 0)\n",
    "NCgan2 = process_gan(NC, 1)\n",
    "NCgan3 = process_gan(NC, 2)\n",
    "\n",
    "ADgan1 = process_gan(AD, 0)\n",
    "ADgan2 = process_gan(AD, 1)\n",
    "ADgan3 = process_gan(AD, 2)\n",
    "\n",
    "gan1 = []\n",
    "for i in range(len(ADgan1)):\n",
    "    gan1.append({\"A\": NCgan1[i], \"B\": ADgan1[i]})\n",
    "\n",
    "gan2 = []\n",
    "for i in range(len(ADgan2)):\n",
    "    gan2.append({\"A\": NCgan2[i], \"B\": ADgan2[i]})\n",
    "    \n",
    "gan3 = []\n",
    "for i in range(len(ADgan3)):\n",
    "    gan3.append({\"A\": NCgan3[i], \"B\": ADgan3[i]})\n",
    "\n",
    "batch_size = 1\n",
    "dataloader1 = DataLoader(gan1, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader2 = DataLoader(gan2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader3 = DataLoader(gan3, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "        \n",
    "def sample_images(batches_done, dataloader):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=4, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=4, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=4, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=4, normalize=True)\n",
    "\n",
    "    image_grid = torch.stack((real_A, fake_B, real_B, fake_A), 0)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n",
    "    \n",
    "def train_gan(dataloader, epoch):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "                \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "           \n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "            \n",
    "                print(\n",
    "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        n_epochs,\n",
    "                        i,\n",
    "                        len(dataloader),\n",
    "                        loss_D.item(),\n",
    "                        loss_G.item(),\n",
    "                        loss_GAN.item(),\n",
    "                        loss_cycle.item(),\n",
    "                        loss_identity.item(),\n",
    "                        time_left,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, dataloader)\n",
    "                \n",
    "            \n",
    "            G_losses.append(loss_G.item())\n",
    "            D_losses.append(loss_D.item())\n",
    "            \n",
    "            \n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/101] [Batch 0/476] [D loss: 1.366792] [G loss: 8.987213, adv: 1.431048, cycle: 0.499411, identity: 0.512411] ETA: 2:08:37.610236\n",
      "[Epoch 0/101] [Batch 100/476] [D loss: 0.488922] [G loss: 1.613842, adv: 0.293263, cycle: 0.087111, identity: 0.089894] ETA: 0:58:38.053631\n",
      "[Epoch 0/101] [Batch 200/476] [D loss: 0.280785] [G loss: 1.521287, adv: 0.252541, cycle: 0.085872, identity: 0.082006] ETA: 0:56:35.399711\n",
      "[Epoch 0/101] [Batch 300/476] [D loss: 0.224722] [G loss: 1.677528, adv: 0.327221, cycle: 0.092886, identity: 0.084289] ETA: 0:55:36.274986\n",
      "[Epoch 0/101] [Batch 400/476] [D loss: 0.252880] [G loss: 1.370674, adv: 0.400783, cycle: 0.065535, identity: 0.062908] ETA: 0:54:04.233730\n",
      "[Epoch 1/101] [Batch 0/476] [D loss: 0.167847] [G loss: 1.141171, adv: 0.343441, cycle: 0.052759, identity: 0.054027] ETA: 3:16:50.867119\n",
      "[Epoch 1/101] [Batch 100/476] [D loss: 0.214808] [G loss: 1.350230, adv: 0.268631, cycle: 0.073561, identity: 0.069199] ETA: 0:52:40.310388\n",
      "[Epoch 1/101] [Batch 200/476] [D loss: 0.151771] [G loss: 1.779119, adv: 0.421578, cycle: 0.093302, identity: 0.084904] ETA: 0:57:28.297834\n",
      "[Epoch 1/101] [Batch 300/476] [D loss: 0.142261] [G loss: 1.565821, adv: 0.566648, cycle: 0.066301, identity: 0.067233] ETA: 0:59:48.664031\n",
      "[Epoch 1/101] [Batch 400/476] [D loss: 0.192659] [G loss: 1.173146, adv: 0.130466, cycle: 0.069160, identity: 0.070215] ETA: 0:55:59.048271\n",
      "[Epoch 2/101] [Batch 0/476] [D loss: 0.201809] [G loss: 1.275632, adv: 0.304360, cycle: 0.065660, identity: 0.062935] ETA: 2:25:34.363220\n",
      "[Epoch 2/101] [Batch 100/476] [D loss: 0.182522] [G loss: 1.578725, adv: 0.386872, cycle: 0.076720, identity: 0.084931] ETA: 0:54:49.098240\n",
      "[Epoch 2/101] [Batch 200/476] [D loss: 0.395118] [G loss: 1.472597, adv: 0.248931, cycle: 0.081995, identity: 0.080743] ETA: 1:00:56.853207\n",
      "[Epoch 2/101] [Batch 300/476] [D loss: 0.258543] [G loss: 1.248670, adv: 0.175803, cycle: 0.071049, identity: 0.072476] ETA: 0:57:29.720833\n",
      "[Epoch 2/101] [Batch 400/476] [D loss: 0.285235] [G loss: 1.624012, adv: 0.640074, cycle: 0.066566, identity: 0.063656] ETA: 0:56:11.102806\n",
      "[Epoch 3/101] [Batch 0/476] [D loss: 0.284308] [G loss: 1.124462, adv: 0.214497, cycle: 0.062683, identity: 0.056627] ETA: 2:29:20.382372\n",
      "[Epoch 3/101] [Batch 100/476] [D loss: 0.136889] [G loss: 1.404387, adv: 0.255878, cycle: 0.074998, identity: 0.079707] ETA: 0:55:45.464373\n",
      "[Epoch 3/101] [Batch 200/476] [D loss: 0.171862] [G loss: 1.394189, adv: 0.499376, cycle: 0.061136, identity: 0.056690] ETA: 0:54:16.639229\n",
      "[Epoch 3/101] [Batch 300/476] [D loss: 0.177435] [G loss: 1.333499, adv: 0.396964, cycle: 0.062180, identity: 0.062948] ETA: 0:50:19.772147\n",
      "[Epoch 3/101] [Batch 400/476] [D loss: 0.151197] [G loss: 1.156900, adv: 0.392326, cycle: 0.050686, identity: 0.051543] ETA: 0:51:32.778986\n",
      "[Epoch 4/101] [Batch 0/476] [D loss: 0.118761] [G loss: 1.581768, adv: 0.311950, cycle: 0.084953, identity: 0.084057] ETA: 2:24:18.626033\n",
      "[Epoch 4/101] [Batch 100/476] [D loss: 0.288838] [G loss: 1.358710, adv: 0.220424, cycle: 0.075890, identity: 0.075877] ETA: 0:52:33.857878\n",
      "[Epoch 4/101] [Batch 200/476] [D loss: 0.278864] [G loss: 1.625705, adv: 0.192105, cycle: 0.099238, identity: 0.088244] ETA: 0:54:25.682568\n",
      "[Epoch 4/101] [Batch 300/476] [D loss: 0.193037] [G loss: 1.185258, adv: 0.378039, cycle: 0.054446, identity: 0.052552] ETA: 0:52:04.461727\n",
      "[Epoch 4/101] [Batch 400/476] [D loss: 0.186161] [G loss: 1.101953, adv: 0.151807, cycle: 0.062897, identity: 0.064236] ETA: 0:54:37.797203\n",
      "[Epoch 5/101] [Batch 0/476] [D loss: 0.145614] [G loss: 1.728472, adv: 0.715093, cycle: 0.069503, identity: 0.063669] ETA: 2:20:03.325470\n",
      "[Epoch 5/101] [Batch 100/476] [D loss: 0.172730] [G loss: 1.362200, adv: 0.356563, cycle: 0.067199, identity: 0.066730] ETA: 0:58:18.375120\n",
      "[Epoch 5/101] [Batch 200/476] [D loss: 0.157366] [G loss: 1.183126, adv: 0.277515, cycle: 0.059059, identity: 0.063004] ETA: 0:54:28.044321\n",
      "[Epoch 5/101] [Batch 300/476] [D loss: 0.172145] [G loss: 1.945336, adv: 0.814789, cycle: 0.072974, identity: 0.080161] ETA: 0:54:21.781151\n",
      "[Epoch 5/101] [Batch 400/476] [D loss: 0.279520] [G loss: 1.261830, adv: 0.326882, cycle: 0.061296, identity: 0.064397] ETA: 0:52:30.381691\n",
      "[Epoch 6/101] [Batch 0/476] [D loss: 0.285011] [G loss: 1.296542, adv: 0.190853, cycle: 0.072427, identity: 0.076283] ETA: 2:27:20.030966\n",
      "[Epoch 6/101] [Batch 100/476] [D loss: 0.208846] [G loss: 1.454333, adv: 0.612945, cycle: 0.055643, identity: 0.056992] ETA: 0:54:27.929306\n",
      "[Epoch 6/101] [Batch 200/476] [D loss: 0.433219] [G loss: 1.207748, adv: 0.208316, cycle: 0.065832, identity: 0.068222] ETA: 0:50:31.942711\n",
      "[Epoch 6/101] [Batch 300/476] [D loss: 0.102536] [G loss: 1.325423, adv: 0.443405, cycle: 0.060048, identity: 0.056307] ETA: 0:52:44.702711\n",
      "[Epoch 6/101] [Batch 400/476] [D loss: 0.271394] [G loss: 1.160552, adv: 0.180911, cycle: 0.065177, identity: 0.065574] ETA: 0:53:50.311089\n",
      "[Epoch 7/101] [Batch 0/476] [D loss: 0.151897] [G loss: 1.663423, adv: 0.225163, cycle: 0.095222, identity: 0.097207] ETA: 2:25:19.391075\n",
      "[Epoch 7/101] [Batch 100/476] [D loss: 0.403549] [G loss: 1.149816, adv: 0.101082, cycle: 0.070187, identity: 0.069373] ETA: 0:50:04.097781\n",
      "[Epoch 7/101] [Batch 200/476] [D loss: 0.156731] [G loss: 1.340022, adv: 0.396336, cycle: 0.063342, identity: 0.062053] ETA: 0:53:03.613770\n",
      "[Epoch 7/101] [Batch 300/476] [D loss: 0.278170] [G loss: 1.476088, adv: 0.299047, cycle: 0.078671, identity: 0.078067] ETA: 0:52:57.685220\n",
      "[Epoch 7/101] [Batch 400/476] [D loss: 0.224435] [G loss: 1.625778, adv: 0.444888, cycle: 0.077201, identity: 0.081776] ETA: 0:53:46.822992\n",
      "[Epoch 8/101] [Batch 0/476] [D loss: 0.118307] [G loss: 1.051101, adv: 0.241512, cycle: 0.054307, identity: 0.053304] ETA: 2:20:31.450673\n",
      "[Epoch 8/101] [Batch 100/476] [D loss: 0.144485] [G loss: 1.636512, adv: 0.508903, cycle: 0.075430, identity: 0.074662] ETA: 0:56:27.389517\n",
      "[Epoch 8/101] [Batch 200/476] [D loss: 0.196203] [G loss: 1.285397, adv: 0.321974, cycle: 0.064889, identity: 0.062906] ETA: 0:52:01.750902\n",
      "[Epoch 8/101] [Batch 300/476] [D loss: 0.170665] [G loss: 1.165261, adv: 0.361059, cycle: 0.053692, identity: 0.053456] ETA: 0:48:31.531494\n",
      "[Epoch 8/101] [Batch 400/476] [D loss: 0.098648] [G loss: 1.181981, adv: 0.330007, cycle: 0.057052, identity: 0.056292] ETA: 0:52:09.567726\n",
      "[Epoch 9/101] [Batch 0/476] [D loss: 0.073226] [G loss: 1.079897, adv: 0.200554, cycle: 0.057741, identity: 0.060386] ETA: 2:15:45.421169\n",
      "[Epoch 9/101] [Batch 100/476] [D loss: 0.377062] [G loss: 1.092925, adv: 0.042665, cycle: 0.063822, identity: 0.082409] ETA: 0:48:48.370614\n",
      "[Epoch 9/101] [Batch 200/476] [D loss: 0.109977] [G loss: 0.906782, adv: 0.180713, cycle: 0.047280, identity: 0.050654] ETA: 0:47:49.141367\n",
      "[Epoch 9/101] [Batch 300/476] [D loss: 0.041090] [G loss: 1.847060, adv: 0.730065, cycle: 0.072610, identity: 0.078179] ETA: 0:50:51.840777\n",
      "[Epoch 9/101] [Batch 400/476] [D loss: 0.243440] [G loss: 0.995292, adv: 0.068855, cycle: 0.060786, identity: 0.063716] ETA: 0:51:43.720459\n",
      "[Epoch 10/101] [Batch 0/476] [D loss: 0.109766] [G loss: 1.118907, adv: 0.178681, cycle: 0.061761, identity: 0.064523] ETA: 2:18:32.031224\n",
      "[Epoch 10/101] [Batch 100/476] [D loss: 0.030289] [G loss: 1.319835, adv: 0.380191, cycle: 0.061923, identity: 0.064083] ETA: 0:51:20.529324\n",
      "[Epoch 10/101] [Batch 200/476] [D loss: 0.188942] [G loss: 1.199474, adv: 0.373228, cycle: 0.054096, identity: 0.057058] ETA: 0:50:39.714678\n",
      "[Epoch 10/101] [Batch 300/476] [D loss: 0.066260] [G loss: 1.215931, adv: 0.415640, cycle: 0.053757, identity: 0.052544] ETA: 0:52:16.330357\n",
      "[Epoch 10/101] [Batch 400/476] [D loss: 0.128346] [G loss: 1.362146, adv: 0.431644, cycle: 0.062208, identity: 0.061684] ETA: 0:49:38.445118\n",
      "[Epoch 11/101] [Batch 0/476] [D loss: 0.148147] [G loss: 1.281733, adv: 0.298917, cycle: 0.064385, identity: 0.067794] ETA: 2:12:04.233170\n",
      "[Epoch 11/101] [Batch 100/476] [D loss: 0.235932] [G loss: 1.234417, adv: 0.296093, cycle: 0.061202, identity: 0.065260] ETA: 0:46:13.241811\n",
      "[Epoch 11/101] [Batch 200/476] [D loss: 0.186928] [G loss: 1.514558, adv: 0.442001, cycle: 0.070652, identity: 0.073207] ETA: 0:50:14.248543\n",
      "[Epoch 11/101] [Batch 300/476] [D loss: 0.201675] [G loss: 1.308711, adv: 0.312618, cycle: 0.065800, identity: 0.067618] ETA: 0:51:22.344260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/101] [Batch 400/476] [D loss: 0.056865] [G loss: 1.137306, adv: 0.211666, cycle: 0.060949, identity: 0.063230] ETA: 0:59:21.494055\n",
      "[Epoch 12/101] [Batch 0/476] [D loss: 0.213657] [G loss: 1.477808, adv: 0.494708, cycle: 0.064712, identity: 0.067196] ETA: 2:01:07.060885\n",
      "[Epoch 12/101] [Batch 100/476] [D loss: 0.141637] [G loss: 1.724855, adv: 0.754546, cycle: 0.065103, identity: 0.063856] ETA: 0:50:00.838881\n",
      "[Epoch 12/101] [Batch 200/476] [D loss: 0.263370] [G loss: 1.652520, adv: 0.470673, cycle: 0.077381, identity: 0.081608] ETA: 0:52:56.285290\n",
      "[Epoch 12/101] [Batch 300/476] [D loss: 0.100893] [G loss: 1.326479, adv: 0.396057, cycle: 0.061360, identity: 0.063365] ETA: 0:52:27.380665\n",
      "[Epoch 12/101] [Batch 400/476] [D loss: 0.309398] [G loss: 0.875916, adv: 0.087652, cycle: 0.051585, identity: 0.054484] ETA: 0:53:12.974813\n",
      "[Epoch 13/101] [Batch 0/476] [D loss: 0.088059] [G loss: 1.286273, adv: 0.282077, cycle: 0.066540, identity: 0.067759] ETA: 2:13:52.904922\n",
      "[Epoch 13/101] [Batch 100/476] [D loss: 0.088613] [G loss: 1.099907, adv: 0.218051, cycle: 0.058205, identity: 0.059961] ETA: 0:49:21.452549\n",
      "[Epoch 13/101] [Batch 200/476] [D loss: 0.329430] [G loss: 1.400730, adv: 0.414089, cycle: 0.064284, identity: 0.068760] ETA: 0:47:42.984753\n",
      "[Epoch 13/101] [Batch 300/476] [D loss: 0.045045] [G loss: 1.423440, adv: 0.536846, cycle: 0.057931, identity: 0.061457] ETA: 0:52:00.549624\n",
      "[Epoch 13/101] [Batch 400/476] [D loss: 0.287829] [G loss: 1.037368, adv: 0.077214, cycle: 0.063526, identity: 0.064979] ETA: 0:48:52.080963\n",
      "[Epoch 14/101] [Batch 0/476] [D loss: 0.086528] [G loss: 1.600691, adv: 0.371618, cycle: 0.082805, identity: 0.080205] ETA: 2:09:19.447989\n",
      "[Epoch 14/101] [Batch 100/476] [D loss: 0.126574] [G loss: 1.375129, adv: 0.600352, cycle: 0.048096, identity: 0.058763] ETA: 0:47:32.488297\n",
      "[Epoch 14/101] [Batch 200/476] [D loss: 0.154168] [G loss: 1.142116, adv: 0.331745, cycle: 0.053134, identity: 0.055806] ETA: 0:49:42.003660\n",
      "[Epoch 14/101] [Batch 300/476] [D loss: 0.178932] [G loss: 1.484598, adv: 0.592011, cycle: 0.059639, identity: 0.059240] ETA: 0:50:42.890854\n",
      "[Epoch 14/101] [Batch 400/476] [D loss: 0.143021] [G loss: 1.374336, adv: 0.261033, cycle: 0.073307, identity: 0.076046] ETA: 0:50:32.555981\n",
      "[Epoch 15/101] [Batch 0/476] [D loss: 0.099587] [G loss: 1.284543, adv: 0.345164, cycle: 0.062901, identity: 0.062073] ETA: 2:07:08.388948\n",
      "[Epoch 15/101] [Batch 100/476] [D loss: 0.169712] [G loss: 1.271631, adv: 0.379489, cycle: 0.058587, identity: 0.061255] ETA: 0:48:42.317682\n",
      "[Epoch 15/101] [Batch 200/476] [D loss: 0.112429] [G loss: 1.593272, adv: 0.689859, cycle: 0.058285, identity: 0.064112] ETA: 0:50:24.501617\n",
      "[Epoch 15/101] [Batch 300/476] [D loss: 0.181104] [G loss: 1.692120, adv: 0.777814, cycle: 0.059647, identity: 0.063567] ETA: 0:46:43.196358\n",
      "[Epoch 15/101] [Batch 400/476] [D loss: 0.146004] [G loss: 1.294991, adv: 0.390862, cycle: 0.060088, identity: 0.060649] ETA: 0:50:16.485163\n",
      "[Epoch 16/101] [Batch 0/476] [D loss: 0.203850] [G loss: 1.255092, adv: 0.238145, cycle: 0.066201, identity: 0.070987] ETA: 2:10:53.648643\n",
      "[Epoch 16/101] [Batch 100/476] [D loss: 0.037080] [G loss: 1.201458, adv: 0.321258, cycle: 0.057006, identity: 0.062028] ETA: 0:45:24.073677\n",
      "[Epoch 16/101] [Batch 200/476] [D loss: 0.075101] [G loss: 1.028945, adv: 0.338804, cycle: 0.045455, identity: 0.047119] ETA: 0:49:00.216393\n",
      "[Epoch 16/101] [Batch 300/476] [D loss: 0.069158] [G loss: 1.850662, adv: 0.517609, cycle: 0.088416, identity: 0.089779] ETA: 0:45:56.275749\n",
      "[Epoch 16/101] [Batch 400/476] [D loss: 0.219734] [G loss: 1.351874, adv: 0.470665, cycle: 0.058899, identity: 0.058443] ETA: 0:46:40.319600\n",
      "[Epoch 17/101] [Batch 0/476] [D loss: 0.078010] [G loss: 1.270530, adv: 0.293244, cycle: 0.066374, identity: 0.062709] ETA: 2:05:32.920074\n",
      "[Epoch 17/101] [Batch 100/476] [D loss: 0.109725] [G loss: 1.234003, adv: 0.548854, cycle: 0.044247, identity: 0.048535] ETA: 0:46:20.580543\n",
      "[Epoch 17/101] [Batch 200/476] [D loss: 0.081267] [G loss: 1.662519, adv: 0.745204, cycle: 0.059677, identity: 0.064108] ETA: 0:45:41.283159\n",
      "[Epoch 17/101] [Batch 300/476] [D loss: 0.093170] [G loss: 1.659358, adv: 0.659437, cycle: 0.065695, identity: 0.068594] ETA: 0:47:45.329098\n",
      "[Epoch 17/101] [Batch 400/476] [D loss: 0.051973] [G loss: 1.021759, adv: 0.322486, cycle: 0.045549, identity: 0.048756] ETA: 0:47:03.218071\n",
      "[Epoch 18/101] [Batch 0/476] [D loss: 0.132133] [G loss: 1.480406, adv: 0.408615, cycle: 0.070900, identity: 0.072558] ETA: 1:58:42.020670\n",
      "[Epoch 18/101] [Batch 100/476] [D loss: 0.189861] [G loss: 1.235310, adv: 0.344491, cycle: 0.060362, identity: 0.057439] ETA: 0:45:26.133156\n",
      "[Epoch 18/101] [Batch 200/476] [D loss: 0.032900] [G loss: 1.418784, adv: 0.450884, cycle: 0.062265, identity: 0.069051] ETA: 0:44:25.890141\n",
      "[Epoch 18/101] [Batch 300/476] [D loss: 0.091847] [G loss: 1.308423, adv: 0.547843, cycle: 0.049701, identity: 0.052714] ETA: 0:44:37.775873\n",
      "[Epoch 18/101] [Batch 400/476] [D loss: 0.062414] [G loss: 1.542515, adv: 0.624007, cycle: 0.060530, identity: 0.062641] ETA: 0:44:20.493919\n",
      "[Epoch 19/101] [Batch 0/476] [D loss: 0.192406] [G loss: 1.210224, adv: 0.226857, cycle: 0.066341, identity: 0.063991] ETA: 1:59:59.811863\n",
      "[Epoch 19/101] [Batch 100/476] [D loss: 0.139294] [G loss: 1.963063, adv: 1.082953, cycle: 0.060082, identity: 0.055858] ETA: 0:43:41.927493\n",
      "[Epoch 19/101] [Batch 200/476] [D loss: 0.053962] [G loss: 1.489875, adv: 0.304516, cycle: 0.079783, identity: 0.077506] ETA: 0:48:54.325436\n",
      "[Epoch 19/101] [Batch 300/476] [D loss: 0.159573] [G loss: 1.827575, adv: 0.550858, cycle: 0.085870, identity: 0.083604] ETA: 0:46:14.437842\n",
      "[Epoch 19/101] [Batch 400/476] [D loss: 0.131255] [G loss: 1.155813, adv: 0.193425, cycle: 0.064849, identity: 0.062779] ETA: 0:47:05.126369\n",
      "[Epoch 20/101] [Batch 0/476] [D loss: 0.101922] [G loss: 1.268022, adv: 0.502091, cycle: 0.051341, identity: 0.050504] ETA: 1:59:05.745632\n",
      "[Epoch 20/101] [Batch 100/476] [D loss: 0.171361] [G loss: 1.548239, adv: 0.694982, cycle: 0.058320, identity: 0.054011] ETA: 0:45:23.118263\n",
      "[Epoch 20/101] [Batch 200/476] [D loss: 0.139230] [G loss: 0.976750, adv: 0.231665, cycle: 0.050914, identity: 0.047189] ETA: 0:46:25.482618\n",
      "[Epoch 20/101] [Batch 300/476] [D loss: 0.153371] [G loss: 1.253032, adv: 0.267269, cycle: 0.064059, identity: 0.069035] ETA: 0:45:41.417439\n",
      "[Epoch 20/101] [Batch 400/476] [D loss: 0.104982] [G loss: 1.164025, adv: 0.490209, cycle: 0.042841, identity: 0.049081] ETA: 0:43:28.747876\n",
      "[Epoch 21/101] [Batch 0/476] [D loss: 0.039445] [G loss: 1.332597, adv: 0.519057, cycle: 0.052800, identity: 0.057107] ETA: 2:04:54.788361\n",
      "[Epoch 21/101] [Batch 100/476] [D loss: 0.041407] [G loss: 1.450983, adv: 0.493371, cycle: 0.064310, identity: 0.062903] ETA: 0:45:34.325581\n",
      "[Epoch 21/101] [Batch 200/476] [D loss: 0.113654] [G loss: 1.635134, adv: 0.780846, cycle: 0.056096, identity: 0.058666] ETA: 0:40:00.265541\n",
      "[Epoch 21/101] [Batch 300/476] [D loss: 0.064001] [G loss: 1.177672, adv: 0.274710, cycle: 0.060711, identity: 0.059171] ETA: 0:48:17.418694\n",
      "[Epoch 21/101] [Batch 400/476] [D loss: 0.139456] [G loss: 0.931848, adv: 0.256323, cycle: 0.044940, identity: 0.045225] ETA: 0:44:06.958408\n",
      "[Epoch 22/101] [Batch 0/476] [D loss: 0.143759] [G loss: 1.229575, adv: 0.503431, cycle: 0.047494, identity: 0.050240] ETA: 2:05:45.851378\n",
      "[Epoch 22/101] [Batch 100/476] [D loss: 0.032108] [G loss: 1.570381, adv: 0.635065, cycle: 0.061874, identity: 0.063315] ETA: 0:42:57.564850\n",
      "[Epoch 22/101] [Batch 200/476] [D loss: 0.117332] [G loss: 1.394892, adv: 0.490334, cycle: 0.059894, identity: 0.061123] ETA: 0:44:32.123231\n",
      "[Epoch 22/101] [Batch 300/476] [D loss: 0.074834] [G loss: 1.287767, adv: 0.459564, cycle: 0.053750, identity: 0.058140] ETA: 0:43:05.138142\n",
      "[Epoch 22/101] [Batch 400/476] [D loss: 0.056064] [G loss: 1.584836, adv: 0.694814, cycle: 0.058071, identity: 0.061863] ETA: 0:40:45.910308\n",
      "[Epoch 23/101] [Batch 0/476] [D loss: 0.042289] [G loss: 1.582316, adv: 0.468149, cycle: 0.074900, identity: 0.073033] ETA: 1:56:43.051311\n",
      "[Epoch 23/101] [Batch 100/476] [D loss: 0.122194] [G loss: 1.252267, adv: 0.470983, cycle: 0.051142, identity: 0.053972] ETA: 0:41:31.810845\n",
      "[Epoch 23/101] [Batch 200/476] [D loss: 0.083267] [G loss: 1.791078, adv: 0.855419, cycle: 0.061482, identity: 0.064168] ETA: 0:44:39.198990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/101] [Batch 300/476] [D loss: 0.092875] [G loss: 1.481480, adv: 0.769962, cycle: 0.045668, identity: 0.050967] ETA: 0:45:41.414947\n",
      "[Epoch 23/101] [Batch 400/476] [D loss: 0.158901] [G loss: 1.576725, adv: 0.528774, cycle: 0.068500, identity: 0.072591] ETA: 0:44:32.315628\n",
      "[Epoch 24/101] [Batch 0/476] [D loss: 0.049150] [G loss: 1.510031, adv: 0.685693, cycle: 0.055187, identity: 0.054493] ETA: 1:56:29.547124\n",
      "[Epoch 24/101] [Batch 100/476] [D loss: 0.067493] [G loss: 1.880628, adv: 1.154828, cycle: 0.047375, identity: 0.050410] ETA: 0:43:31.387493\n",
      "[Epoch 24/101] [Batch 200/476] [D loss: 0.235320] [G loss: 1.360986, adv: 0.461968, cycle: 0.057661, identity: 0.064481] ETA: 0:44:33.283171\n",
      "[Epoch 24/101] [Batch 300/476] [D loss: 0.185699] [G loss: 1.304598, adv: 0.511961, cycle: 0.052194, identity: 0.054139] ETA: 0:48:29.647949\n",
      "[Epoch 24/101] [Batch 400/476] [D loss: 0.130435] [G loss: 1.208992, adv: 0.523937, cycle: 0.046373, identity: 0.044265] ETA: 0:46:55.281498\n",
      "[Epoch 25/101] [Batch 0/476] [D loss: 0.040818] [G loss: 1.466301, adv: 0.600820, cycle: 0.055131, identity: 0.062835] ETA: 1:52:33.243645\n",
      "[Epoch 25/101] [Batch 100/476] [D loss: 0.078089] [G loss: 1.366405, adv: 0.584872, cycle: 0.050415, identity: 0.055476] ETA: 0:43:00.347997\n",
      "[Epoch 25/101] [Batch 200/476] [D loss: 0.100240] [G loss: 1.397799, adv: 0.657985, cycle: 0.049355, identity: 0.049253] ETA: 0:43:10.375889\n",
      "[Epoch 25/101] [Batch 300/476] [D loss: 0.041833] [G loss: 1.156369, adv: 0.427866, cycle: 0.047149, identity: 0.051403] ETA: 0:45:22.084520\n",
      "[Epoch 25/101] [Batch 400/476] [D loss: 0.073206] [G loss: 1.487197, adv: 0.601429, cycle: 0.056617, identity: 0.063920] ETA: 0:42:35.111755\n",
      "[Epoch 26/101] [Batch 0/476] [D loss: 0.216911] [G loss: 1.234098, adv: 0.236113, cycle: 0.067137, identity: 0.065323] ETA: 2:28:38.999362\n",
      "[Epoch 26/101] [Batch 100/476] [D loss: 0.085797] [G loss: 1.459535, adv: 0.524482, cycle: 0.060734, identity: 0.065543] ETA: 0:38:47.633286\n",
      "[Epoch 26/101] [Batch 200/476] [D loss: 0.033535] [G loss: 1.777064, adv: 0.908097, cycle: 0.056377, identity: 0.061040] ETA: 0:40:44.938183\n",
      "[Epoch 26/101] [Batch 300/476] [D loss: 0.025310] [G loss: 1.367972, adv: 0.636360, cycle: 0.047101, identity: 0.052120] ETA: 0:42:51.639633\n",
      "[Epoch 26/101] [Batch 400/476] [D loss: 0.178008] [G loss: 1.394175, adv: 0.620977, cycle: 0.051881, identity: 0.050878] ETA: 0:40:55.941105\n",
      "[Epoch 27/101] [Batch 0/476] [D loss: 0.098907] [G loss: 1.618690, adv: 0.799337, cycle: 0.054140, identity: 0.055591] ETA: 1:45:13.809689\n",
      "[Epoch 27/101] [Batch 100/476] [D loss: 0.069795] [G loss: 1.479272, adv: 0.614083, cycle: 0.057122, identity: 0.058793] ETA: 0:40:15.952415\n",
      "[Epoch 27/101] [Batch 200/476] [D loss: 0.096232] [G loss: 1.280852, adv: 0.451155, cycle: 0.056208, identity: 0.053523] ETA: 0:41:33.980648\n",
      "[Epoch 27/101] [Batch 300/476] [D loss: 0.274193] [G loss: 0.994815, adv: 0.125621, cycle: 0.057048, identity: 0.059742] ETA: 0:42:17.360279\n",
      "[Epoch 27/101] [Batch 400/476] [D loss: 0.034708] [G loss: 1.025798, adv: 0.251406, cycle: 0.050060, identity: 0.054758] ETA: 0:41:49.786526\n",
      "[Epoch 28/101] [Batch 0/476] [D loss: 0.064918] [G loss: 1.082574, adv: 0.470882, cycle: 0.039705, identity: 0.042927] ETA: 1:47:10.349739\n",
      "[Epoch 28/101] [Batch 100/476] [D loss: 0.060226] [G loss: 1.435754, adv: 0.654716, cycle: 0.050301, identity: 0.055605] ETA: 0:39:36.536791\n",
      "[Epoch 28/101] [Batch 200/476] [D loss: 0.053077] [G loss: 1.318673, adv: 0.556873, cycle: 0.050043, identity: 0.052273] ETA: 0:42:17.075918\n",
      "[Epoch 28/101] [Batch 300/476] [D loss: 0.127192] [G loss: 1.399589, adv: 0.654318, cycle: 0.047509, identity: 0.054036] ETA: 0:41:06.286533\n",
      "[Epoch 28/101] [Batch 400/476] [D loss: 0.211800] [G loss: 1.458024, adv: 0.644725, cycle: 0.050726, identity: 0.061209] ETA: 0:41:09.363587\n",
      "[Epoch 29/101] [Batch 0/476] [D loss: 0.159175] [G loss: 1.825995, adv: 1.007157, cycle: 0.052926, identity: 0.057915] ETA: 1:42:15.060471\n",
      "[Epoch 29/101] [Batch 100/476] [D loss: 0.082162] [G loss: 1.351381, adv: 0.645881, cycle: 0.048138, identity: 0.044824] ETA: 0:40:18.695487\n",
      "[Epoch 29/101] [Batch 200/476] [D loss: 0.173086] [G loss: 0.926612, adv: 0.253767, cycle: 0.043058, identity: 0.048454] ETA: 0:37:10.985609\n",
      "[Epoch 29/101] [Batch 300/476] [D loss: 0.140593] [G loss: 1.666319, adv: 0.784800, cycle: 0.059011, identity: 0.058281] ETA: 0:41:28.515676\n",
      "[Epoch 29/101] [Batch 400/476] [D loss: 0.175912] [G loss: 1.126859, adv: 0.371454, cycle: 0.046538, identity: 0.058005] ETA: 0:39:31.247707\n",
      "[Epoch 30/101] [Batch 0/476] [D loss: 0.032559] [G loss: 1.629799, adv: 0.689450, cycle: 0.063137, identity: 0.061796] ETA: 1:39:01.468596\n",
      "[Epoch 30/101] [Batch 100/476] [D loss: 0.064646] [G loss: 1.675550, adv: 0.698672, cycle: 0.063925, identity: 0.067526] ETA: 0:41:33.821365\n",
      "[Epoch 30/101] [Batch 200/476] [D loss: 0.107134] [G loss: 1.333547, adv: 0.572795, cycle: 0.049714, identity: 0.052722] ETA: 0:39:34.698191\n",
      "[Epoch 30/101] [Batch 300/476] [D loss: 0.058163] [G loss: 1.511409, adv: 0.624682, cycle: 0.060379, identity: 0.056587] ETA: 0:40:15.881622\n",
      "[Epoch 30/101] [Batch 400/476] [D loss: 0.070689] [G loss: 1.767581, adv: 1.029006, cycle: 0.047462, identity: 0.052791] ETA: 0:41:49.287835\n",
      "[Epoch 31/101] [Batch 0/476] [D loss: 0.098676] [G loss: 1.899334, adv: 0.705870, cycle: 0.080091, identity: 0.078511] ETA: 1:44:19.265223\n",
      "[Epoch 31/101] [Batch 100/476] [D loss: 0.035828] [G loss: 1.633807, adv: 0.739003, cycle: 0.056837, identity: 0.065287] ETA: 0:39:32.808490\n",
      "[Epoch 31/101] [Batch 200/476] [D loss: 0.028934] [G loss: 1.512905, adv: 0.596493, cycle: 0.059819, identity: 0.063645] ETA: 0:42:27.078209\n",
      "[Epoch 31/101] [Batch 300/476] [D loss: 0.073874] [G loss: 1.298662, adv: 0.509446, cycle: 0.051810, identity: 0.054224] ETA: 0:38:08.811359\n",
      "[Epoch 31/101] [Batch 400/476] [D loss: 0.100780] [G loss: 1.263714, adv: 0.386228, cycle: 0.057279, identity: 0.060939] ETA: 0:39:48.724661\n",
      "[Epoch 32/101] [Batch 0/476] [D loss: 0.086105] [G loss: 1.884529, adv: 0.986615, cycle: 0.060211, identity: 0.059160] ETA: 1:41:34.712351\n",
      "[Epoch 32/101] [Batch 100/476] [D loss: 0.041139] [G loss: 1.450903, adv: 0.639680, cycle: 0.052178, identity: 0.057888] ETA: 0:39:43.424923\n",
      "[Epoch 32/101] [Batch 200/476] [D loss: 0.026335] [G loss: 1.402961, adv: 0.508985, cycle: 0.059421, identity: 0.059954] ETA: 0:40:47.359821\n",
      "[Epoch 32/101] [Batch 300/476] [D loss: 0.136575] [G loss: 1.748663, adv: 0.927845, cycle: 0.052716, identity: 0.058732] ETA: 0:39:40.847031\n",
      "[Epoch 32/101] [Batch 400/476] [D loss: 0.071908] [G loss: 1.331364, adv: 0.362010, cycle: 0.063790, identity: 0.066291] ETA: 0:39:17.233075\n",
      "[Epoch 33/101] [Batch 0/476] [D loss: 0.118672] [G loss: 1.070701, adv: 0.275401, cycle: 0.052690, identity: 0.053680] ETA: 1:36:57.089642\n",
      "[Epoch 33/101] [Batch 100/476] [D loss: 0.072678] [G loss: 1.569712, adv: 0.714882, cycle: 0.056600, identity: 0.057766] ETA: 0:35:38.473246\n",
      "[Epoch 33/101] [Batch 200/476] [D loss: 0.065991] [G loss: 1.329122, adv: 0.536002, cycle: 0.052490, identity: 0.053645] ETA: 0:38:10.534586\n",
      "[Epoch 33/101] [Batch 300/476] [D loss: 0.120224] [G loss: 1.330318, adv: 0.527923, cycle: 0.053346, identity: 0.053787] ETA: 0:39:08.447581\n",
      "[Epoch 33/101] [Batch 400/476] [D loss: 0.056422] [G loss: 1.528980, adv: 0.602160, cycle: 0.060626, identity: 0.064112] ETA: 0:38:35.659927\n",
      "[Epoch 34/101] [Batch 0/476] [D loss: 0.071321] [G loss: 1.771227, adv: 0.997154, cycle: 0.048712, identity: 0.057391] ETA: 1:37:24.230229\n",
      "[Epoch 34/101] [Batch 100/476] [D loss: 0.055176] [G loss: 1.941314, adv: 1.176960, cycle: 0.048640, identity: 0.055591] ETA: 0:37:21.620758\n",
      "[Epoch 34/101] [Batch 200/476] [D loss: 0.072769] [G loss: 1.127621, adv: 0.300021, cycle: 0.053725, identity: 0.058069] ETA: 0:39:10.228967\n",
      "[Epoch 34/101] [Batch 300/476] [D loss: 0.031017] [G loss: 1.363052, adv: 0.666802, cycle: 0.048264, identity: 0.042721] ETA: 0:37:56.462681\n",
      "[Epoch 34/101] [Batch 400/476] [D loss: 0.081323] [G loss: 1.456400, adv: 0.721467, cycle: 0.048512, identity: 0.049963] ETA: 0:35:17.326858\n",
      "[Epoch 35/101] [Batch 0/476] [D loss: 0.094990] [G loss: 1.661418, adv: 0.598069, cycle: 0.071448, identity: 0.069775] ETA: 1:40:54.834068\n",
      "[Epoch 35/101] [Batch 100/476] [D loss: 0.049447] [G loss: 2.058548, adv: 1.203263, cycle: 0.055332, identity: 0.060394] ETA: 0:37:17.774433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/101] [Batch 200/476] [D loss: 0.063707] [G loss: 1.391783, adv: 0.603242, cycle: 0.051096, identity: 0.055516] ETA: 0:35:57.216080\n",
      "[Epoch 35/101] [Batch 300/476] [D loss: 0.122581] [G loss: 1.823287, adv: 1.044101, cycle: 0.050210, identity: 0.055418] ETA: 0:37:57.171504\n",
      "[Epoch 35/101] [Batch 400/476] [D loss: 0.031177] [G loss: 1.538566, adv: 0.809253, cycle: 0.047617, identity: 0.050628] ETA: 0:38:00.272434\n",
      "[Epoch 36/101] [Batch 0/476] [D loss: 0.097570] [G loss: 1.207276, adv: 0.484614, cycle: 0.047686, identity: 0.049161] ETA: 1:36:36.469917\n",
      "[Epoch 36/101] [Batch 100/476] [D loss: 0.043485] [G loss: 1.330435, adv: 0.744167, cycle: 0.039842, identity: 0.037571] ETA: 0:35:33.886557\n",
      "[Epoch 36/101] [Batch 200/476] [D loss: 0.082684] [G loss: 2.005897, adv: 0.920895, cycle: 0.070973, identity: 0.075054] ETA: 0:31:18.089595\n",
      "[Epoch 36/101] [Batch 300/476] [D loss: 0.049257] [G loss: 0.963295, adv: 0.227183, cycle: 0.045609, identity: 0.056003] ETA: 0:36:36.050854\n",
      "[Epoch 36/101] [Batch 400/476] [D loss: 0.018155] [G loss: 1.370222, adv: 0.545659, cycle: 0.054623, identity: 0.055667] ETA: 0:37:45.140676\n",
      "[Epoch 37/101] [Batch 0/476] [D loss: 0.090547] [G loss: 1.418202, adv: 0.703343, cycle: 0.045411, identity: 0.052149] ETA: 1:34:52.207825\n",
      "[Epoch 37/101] [Batch 100/476] [D loss: 0.171688] [G loss: 1.695978, adv: 0.842473, cycle: 0.053902, identity: 0.062897] ETA: 0:36:18.868118\n",
      "[Epoch 37/101] [Batch 200/476] [D loss: 0.089710] [G loss: 1.338766, adv: 0.710164, cycle: 0.041637, identity: 0.042447] ETA: 0:36:33.706781\n",
      "[Epoch 37/101] [Batch 300/476] [D loss: 0.074567] [G loss: 1.492127, adv: 0.749695, cycle: 0.046523, identity: 0.055441] ETA: 0:37:40.654549\n",
      "[Epoch 37/101] [Batch 400/476] [D loss: 0.064434] [G loss: 1.569406, adv: 0.529465, cycle: 0.068551, identity: 0.070885] ETA: 0:35:40.876163\n",
      "[Epoch 38/101] [Batch 0/476] [D loss: 0.021817] [G loss: 1.562064, adv: 0.755468, cycle: 0.055995, identity: 0.049330] ETA: 1:32:33.240652\n",
      "[Epoch 38/101] [Batch 100/476] [D loss: 0.051388] [G loss: 1.538269, adv: 0.652611, cycle: 0.059971, identity: 0.057189] ETA: 0:34:08.604782\n",
      "[Epoch 38/101] [Batch 200/476] [D loss: 0.016803] [G loss: 1.799831, adv: 0.960890, cycle: 0.054302, identity: 0.059185] ETA: 0:33:25.040207\n",
      "[Epoch 38/101] [Batch 300/476] [D loss: 0.033044] [G loss: 1.554139, adv: 0.771544, cycle: 0.053132, identity: 0.050255] ETA: 0:32:51.914675\n",
      "[Epoch 38/101] [Batch 400/476] [D loss: 0.059349] [G loss: 1.387515, adv: 0.680400, cycle: 0.044655, identity: 0.052114] ETA: 0:34:23.250122\n",
      "[Epoch 39/101] [Batch 0/476] [D loss: 0.074043] [G loss: 1.314313, adv: 0.625611, cycle: 0.044340, identity: 0.049059] ETA: 1:29:27.987253\n",
      "[Epoch 39/101] [Batch 100/476] [D loss: 0.056970] [G loss: 1.770512, adv: 0.936936, cycle: 0.053940, identity: 0.058836] ETA: 0:32:54.577432\n",
      "[Epoch 39/101] [Batch 200/476] [D loss: 0.109568] [G loss: 1.705788, adv: 0.926553, cycle: 0.051807, identity: 0.052234] ETA: 0:36:28.421783\n",
      "[Epoch 39/101] [Batch 300/476] [D loss: 0.016279] [G loss: 1.435055, adv: 0.845491, cycle: 0.038932, identity: 0.040048] ETA: 0:33:06.035227\n",
      "[Epoch 39/101] [Batch 400/476] [D loss: 0.081063] [G loss: 1.742985, adv: 1.131382, cycle: 0.042106, identity: 0.038108] ETA: 0:39:58.623007\n",
      "[Epoch 40/101] [Batch 0/476] [D loss: 0.044503] [G loss: 1.239166, adv: 0.549761, cycle: 0.047078, identity: 0.043726] ETA: 1:31:48.832072\n",
      "[Epoch 40/101] [Batch 100/476] [D loss: 0.101300] [G loss: 1.150214, adv: 0.512706, cycle: 0.041492, identity: 0.044517] ETA: 0:33:28.139790\n",
      "[Epoch 40/101] [Batch 200/476] [D loss: 0.058378] [G loss: 1.164331, adv: 0.397929, cycle: 0.049766, identity: 0.053748] ETA: 0:33:25.716754\n",
      "[Epoch 40/101] [Batch 300/476] [D loss: 0.063009] [G loss: 1.670915, adv: 0.699485, cycle: 0.066348, identity: 0.061591] ETA: 0:33:11.293350\n",
      "[Epoch 40/101] [Batch 400/476] [D loss: 0.091666] [G loss: 1.317815, adv: 0.490013, cycle: 0.055689, identity: 0.054182] ETA: 0:34:18.631700\n",
      "[Epoch 41/101] [Batch 0/476] [D loss: 0.118049] [G loss: 0.985092, adv: 0.205673, cycle: 0.051722, identity: 0.052440] ETA: 1:32:02.200756\n",
      "[Epoch 41/101] [Batch 100/476] [D loss: 0.032530] [G loss: 1.554988, adv: 0.909474, cycle: 0.045154, identity: 0.038795] ETA: 0:35:17.449665\n",
      "[Epoch 41/101] [Batch 200/476] [D loss: 0.048218] [G loss: 1.303339, adv: 0.630015, cycle: 0.042510, identity: 0.049645] ETA: 0:36:18.774548\n",
      "[Epoch 41/101] [Batch 300/476] [D loss: 0.068848] [G loss: 1.747519, adv: 0.947843, cycle: 0.053006, identity: 0.053923] ETA: 0:34:10.311813\n",
      "[Epoch 41/101] [Batch 400/476] [D loss: 0.048316] [G loss: 1.323948, adv: 0.494771, cycle: 0.054131, identity: 0.057573] ETA: 0:34:40.486450\n",
      "[Epoch 42/101] [Batch 0/476] [D loss: 0.124244] [G loss: 1.173519, adv: 0.423432, cycle: 0.050543, identity: 0.048931] ETA: 1:30:06.146431\n",
      "[Epoch 42/101] [Batch 100/476] [D loss: 0.067435] [G loss: 1.415590, adv: 0.581377, cycle: 0.055532, identity: 0.055779] ETA: 0:34:37.631378\n",
      "[Epoch 42/101] [Batch 200/476] [D loss: 0.057286] [G loss: 1.461474, adv: 0.541907, cycle: 0.059264, identity: 0.065386] ETA: 0:39:10.263353\n",
      "[Epoch 42/101] [Batch 300/476] [D loss: 0.055526] [G loss: 2.100341, adv: 0.942047, cycle: 0.074928, identity: 0.081803] ETA: 0:35:26.361950\n",
      "[Epoch 42/101] [Batch 400/476] [D loss: 0.035712] [G loss: 1.344452, adv: 0.589136, cycle: 0.050070, identity: 0.050924] ETA: 0:35:54.984449\n",
      "[Epoch 43/101] [Batch 0/476] [D loss: 0.091015] [G loss: 1.807999, adv: 0.813313, cycle: 0.065332, identity: 0.068274] ETA: 1:33:56.856691\n",
      "[Epoch 43/101] [Batch 100/476] [D loss: 0.040068] [G loss: 1.393690, adv: 0.629380, cycle: 0.050275, identity: 0.052313] ETA: 0:32:45.118543\n",
      "[Epoch 43/101] [Batch 200/476] [D loss: 0.019151] [G loss: 1.613651, adv: 0.950563, cycle: 0.043813, identity: 0.044992] ETA: 0:32:34.008247\n",
      "[Epoch 43/101] [Batch 300/476] [D loss: 0.026991] [G loss: 1.670549, adv: 0.917756, cycle: 0.050051, identity: 0.050457] ETA: 0:31:12.773731\n",
      "[Epoch 43/101] [Batch 400/476] [D loss: 0.086936] [G loss: 1.398225, adv: 0.523982, cycle: 0.060876, identity: 0.053096] ETA: 0:34:02.657642\n",
      "[Epoch 44/101] [Batch 0/476] [D loss: 0.020769] [G loss: 1.336478, adv: 0.510457, cycle: 0.053162, identity: 0.058881] ETA: 1:22:48.690331\n",
      "[Epoch 44/101] [Batch 100/476] [D loss: 0.087634] [G loss: 1.173904, adv: 0.465982, cycle: 0.047801, identity: 0.045983] ETA: 0:32:57.949333\n",
      "[Epoch 44/101] [Batch 200/476] [D loss: 0.040559] [G loss: 1.076804, adv: 0.349002, cycle: 0.047790, identity: 0.049980] ETA: 0:32:14.982380\n",
      "[Epoch 44/101] [Batch 300/476] [D loss: 0.123216] [G loss: 1.325853, adv: 0.522851, cycle: 0.053558, identity: 0.053484] ETA: 0:30:35.907623\n",
      "[Epoch 44/101] [Batch 400/476] [D loss: 0.026964] [G loss: 1.418443, adv: 0.688860, cycle: 0.046699, identity: 0.052519] ETA: 0:34:41.407634\n",
      "[Epoch 45/101] [Batch 0/476] [D loss: 0.036839] [G loss: 1.642334, adv: 0.919179, cycle: 0.048913, identity: 0.046804] ETA: 1:24:45.327980\n",
      "[Epoch 45/101] [Batch 100/476] [D loss: 0.022687] [G loss: 1.288874, adv: 0.567212, cycle: 0.047678, identity: 0.048976] ETA: 0:32:26.380792\n",
      "[Epoch 45/101] [Batch 200/476] [D loss: 0.034773] [G loss: 1.785939, adv: 1.131344, cycle: 0.045321, identity: 0.040277] ETA: 0:35:55.143581\n",
      "[Epoch 45/101] [Batch 300/476] [D loss: 0.024927] [G loss: 1.623123, adv: 0.995084, cycle: 0.042679, identity: 0.040249] ETA: 0:32:34.444179\n",
      "[Epoch 45/101] [Batch 400/476] [D loss: 0.080842] [G loss: 1.758586, adv: 1.028734, cycle: 0.049196, identity: 0.047578] ETA: 0:31:19.446545\n",
      "[Epoch 46/101] [Batch 0/476] [D loss: 0.019098] [G loss: 1.616302, adv: 0.601073, cycle: 0.064713, identity: 0.073620] ETA: 1:25:13.905430\n",
      "[Epoch 46/101] [Batch 100/476] [D loss: 0.030361] [G loss: 0.916767, adv: 0.396019, cycle: 0.035356, identity: 0.033438] ETA: 0:31:12.786331\n",
      "[Epoch 46/101] [Batch 200/476] [D loss: 0.044381] [G loss: 1.664028, adv: 0.884120, cycle: 0.052677, identity: 0.050628] ETA: 0:30:44.334812\n",
      "[Epoch 46/101] [Batch 300/476] [D loss: 0.057033] [G loss: 1.728449, adv: 0.927312, cycle: 0.054908, identity: 0.050412] ETA: 0:31:19.693403\n",
      "[Epoch 46/101] [Batch 400/476] [D loss: 0.062639] [G loss: 1.658410, adv: 0.846485, cycle: 0.054420, identity: 0.053545] ETA: 0:30:57.906275\n",
      "[Epoch 47/101] [Batch 0/476] [D loss: 0.042928] [G loss: 1.538676, adv: 0.619443, cycle: 0.056166, identity: 0.071514] ETA: 1:16:41.037964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/101] [Batch 100/476] [D loss: 0.041203] [G loss: 1.213850, adv: 0.522851, cycle: 0.045997, identity: 0.046205] ETA: 0:33:13.011555\n",
      "[Epoch 47/101] [Batch 200/476] [D loss: 0.016559] [G loss: 1.537128, adv: 0.781965, cycle: 0.049134, identity: 0.052766] ETA: 0:34:45.661293\n",
      "[Epoch 47/101] [Batch 300/476] [D loss: 0.081209] [G loss: 1.511706, adv: 0.709763, cycle: 0.052585, identity: 0.055219] ETA: 0:32:01.763555\n",
      "[Epoch 47/101] [Batch 400/476] [D loss: 0.066987] [G loss: 1.098828, adv: 0.307502, cycle: 0.052212, identity: 0.053841] ETA: 0:35:52.753408\n",
      "[Epoch 48/101] [Batch 0/476] [D loss: 0.064396] [G loss: 1.684035, adv: 0.966177, cycle: 0.047839, identity: 0.047893] ETA: 1:20:19.497957\n",
      "[Epoch 48/101] [Batch 100/476] [D loss: 0.020770] [G loss: 1.438363, adv: 0.743187, cycle: 0.048576, identity: 0.041884] ETA: 0:28:14.591211\n",
      "[Epoch 48/101] [Batch 200/476] [D loss: 0.029362] [G loss: 1.539478, adv: 0.722257, cycle: 0.056122, identity: 0.051200] ETA: 0:29:06.802720\n",
      "[Epoch 48/101] [Batch 300/476] [D loss: 0.015093] [G loss: 1.474615, adv: 0.705276, cycle: 0.050105, identity: 0.053657] ETA: 0:27:53.935921\n",
      "[Epoch 48/101] [Batch 400/476] [D loss: 0.046255] [G loss: 1.564939, adv: 0.796650, cycle: 0.051126, identity: 0.051405] ETA: 0:38:16.997732\n",
      "[Epoch 49/101] [Batch 0/476] [D loss: 0.090437] [G loss: 2.170400, adv: 0.967893, cycle: 0.081033, identity: 0.078435] ETA: 1:18:38.714703\n",
      "[Epoch 49/101] [Batch 100/476] [D loss: 0.038004] [G loss: 1.090737, adv: 0.405436, cycle: 0.044782, identity: 0.047496] ETA: 0:29:57.731706\n",
      "[Epoch 49/101] [Batch 200/476] [D loss: 0.061995] [G loss: 1.564345, adv: 0.849061, cycle: 0.046651, identity: 0.049754] ETA: 0:27:04.821865\n",
      "[Epoch 49/101] [Batch 300/476] [D loss: 0.046911] [G loss: 1.324965, adv: 0.630101, cycle: 0.044133, identity: 0.050706] ETA: 0:28:29.020782\n",
      "[Epoch 49/101] [Batch 400/476] [D loss: 0.035260] [G loss: 1.297879, adv: 0.500197, cycle: 0.053581, identity: 0.052375] ETA: 0:29:46.601242\n",
      "[Epoch 50/101] [Batch 0/476] [D loss: 0.028716] [G loss: 1.426019, adv: 0.698576, cycle: 0.050044, identity: 0.045401] ETA: 1:19:17.988439\n",
      "[Epoch 50/101] [Batch 100/476] [D loss: 0.073918] [G loss: 1.181994, adv: 0.456735, cycle: 0.047935, identity: 0.049182] ETA: 0:29:27.100620\n",
      "[Epoch 50/101] [Batch 200/476] [D loss: 0.086290] [G loss: 1.169678, adv: 0.537144, cycle: 0.040969, identity: 0.044569] ETA: 0:31:08.940554\n",
      "[Epoch 50/101] [Batch 300/476] [D loss: 0.071297] [G loss: 1.223588, adv: 0.481145, cycle: 0.048288, identity: 0.051912] ETA: 0:29:21.845324\n",
      "[Epoch 50/101] [Batch 400/476] [D loss: 0.065924] [G loss: 1.531335, adv: 0.732816, cycle: 0.056223, identity: 0.047258] ETA: 0:28:21.078702\n",
      "[Epoch 51/101] [Batch 0/476] [D loss: 0.018738] [G loss: 1.601203, adv: 0.950320, cycle: 0.044074, identity: 0.042028] ETA: 1:41:16.651764\n",
      "[Epoch 51/101] [Batch 100/476] [D loss: 0.029740] [G loss: 1.602748, adv: 0.794501, cycle: 0.055824, identity: 0.050001] ETA: 0:27:11.988931\n",
      "[Epoch 51/101] [Batch 200/476] [D loss: 0.052357] [G loss: 1.382194, adv: 0.618715, cycle: 0.051258, identity: 0.050181] ETA: 0:28:52.555580\n",
      "[Epoch 51/101] [Batch 300/476] [D loss: 0.060279] [G loss: 1.180596, adv: 0.558059, cycle: 0.042809, identity: 0.038889] ETA: 0:28:29.688783\n",
      "[Epoch 51/101] [Batch 400/476] [D loss: 0.033019] [G loss: 1.440853, adv: 0.763218, cycle: 0.045409, identity: 0.044709] ETA: 0:28:11.037941\n",
      "[Epoch 52/101] [Batch 0/476] [D loss: 0.043920] [G loss: 1.370547, adv: 0.730089, cycle: 0.042350, identity: 0.043392] ETA: 1:14:58.258469\n",
      "[Epoch 52/101] [Batch 100/476] [D loss: 0.068161] [G loss: 1.818474, adv: 0.952758, cycle: 0.054570, identity: 0.064003] ETA: 0:28:19.697508\n",
      "[Epoch 52/101] [Batch 200/476] [D loss: 0.181115] [G loss: 1.044264, adv: 0.224025, cycle: 0.053330, identity: 0.057388] ETA: 0:27:41.808151\n",
      "[Epoch 52/101] [Batch 300/476] [D loss: 0.058689] [G loss: 1.578254, adv: 0.802838, cycle: 0.050303, identity: 0.054477] ETA: 0:28:46.960289\n",
      "[Epoch 52/101] [Batch 400/476] [D loss: 0.095142] [G loss: 1.905317, adv: 1.081034, cycle: 0.056106, identity: 0.052645] ETA: 0:28:02.868020\n",
      "[Epoch 53/101] [Batch 0/476] [D loss: 0.018745] [G loss: 1.411826, adv: 0.690980, cycle: 0.050696, identity: 0.042778] ETA: 1:14:51.213181\n",
      "[Epoch 53/101] [Batch 100/476] [D loss: 0.018878] [G loss: 1.781385, adv: 0.799819, cycle: 0.064671, identity: 0.066971] ETA: 0:30:06.246859\n",
      "[Epoch 53/101] [Batch 200/476] [D loss: 0.060379] [G loss: 1.374172, adv: 0.629449, cycle: 0.046384, identity: 0.056177] ETA: 0:28:40.734467\n",
      "[Epoch 53/101] [Batch 300/476] [D loss: 0.011841] [G loss: 1.377316, adv: 0.673730, cycle: 0.044922, identity: 0.050873] ETA: 0:26:07.380984\n",
      "[Epoch 53/101] [Batch 400/476] [D loss: 0.016004] [G loss: 1.473029, adv: 0.660641, cycle: 0.056154, identity: 0.050170] ETA: 0:26:38.541412\n",
      "[Epoch 54/101] [Batch 0/476] [D loss: 0.058087] [G loss: 1.231883, adv: 0.426256, cycle: 0.052365, identity: 0.056395] ETA: 1:19:38.001347\n",
      "[Epoch 54/101] [Batch 100/476] [D loss: 0.016316] [G loss: 1.306227, adv: 0.776991, cycle: 0.036184, identity: 0.033479] ETA: 0:28:12.995361\n",
      "[Epoch 54/101] [Batch 200/476] [D loss: 0.068273] [G loss: 1.220294, adv: 0.604980, cycle: 0.042509, identity: 0.038045] ETA: 0:27:59.721799\n",
      "[Epoch 54/101] [Batch 300/476] [D loss: 0.013940] [G loss: 1.612137, adv: 0.642666, cycle: 0.068319, identity: 0.057256] ETA: 0:26:53.975637\n",
      "[Epoch 54/101] [Batch 400/476] [D loss: 0.163030] [G loss: 0.927820, adv: 0.324050, cycle: 0.038303, identity: 0.044149] ETA: 0:26:03.345886\n",
      "[Epoch 55/101] [Batch 0/476] [D loss: 0.020069] [G loss: 1.516490, adv: 0.881179, cycle: 0.043623, identity: 0.039816] ETA: 1:08:00.917074\n",
      "[Epoch 55/101] [Batch 100/476] [D loss: 0.034008] [G loss: 1.616424, adv: 0.864010, cycle: 0.050123, identity: 0.050236] ETA: 0:26:55.442546\n",
      "[Epoch 55/101] [Batch 200/476] [D loss: 0.016249] [G loss: 1.293250, adv: 0.728680, cycle: 0.037505, identity: 0.037903] ETA: 0:27:02.835251\n",
      "[Epoch 55/101] [Batch 300/476] [D loss: 0.023817] [G loss: 1.323657, adv: 0.762357, cycle: 0.037162, identity: 0.037935] ETA: 0:27:05.617102\n",
      "[Epoch 55/101] [Batch 400/476] [D loss: 0.062217] [G loss: 1.208288, adv: 0.508625, cycle: 0.048066, identity: 0.043801] ETA: 0:25:46.415937\n",
      "[Epoch 56/101] [Batch 0/476] [D loss: 0.011721] [G loss: 1.168333, adv: 0.445234, cycle: 0.049918, identity: 0.044785] ETA: 1:08:15.437994\n",
      "[Epoch 56/101] [Batch 100/476] [D loss: 0.019713] [G loss: 1.920155, adv: 1.195464, cycle: 0.049951, identity: 0.045036] ETA: 0:25:37.409286\n",
      "[Epoch 56/101] [Batch 200/476] [D loss: 0.036323] [G loss: 1.468116, adv: 0.760764, cycle: 0.049643, identity: 0.042185] ETA: 0:32:34.339747\n",
      "[Epoch 56/101] [Batch 300/476] [D loss: 0.053159] [G loss: 1.220707, adv: 0.402858, cycle: 0.054395, identity: 0.054781] ETA: 0:25:14.618225\n",
      "[Epoch 56/101] [Batch 400/476] [D loss: 0.017984] [G loss: 1.350955, adv: 0.687476, cycle: 0.046124, identity: 0.040447] ETA: 0:26:56.909199\n",
      "[Epoch 57/101] [Batch 0/476] [D loss: 0.037608] [G loss: 1.728076, adv: 0.887738, cycle: 0.058209, identity: 0.051649] ETA: 1:08:22.848938\n",
      "[Epoch 57/101] [Batch 100/476] [D loss: 0.043318] [G loss: 1.240051, adv: 0.709363, cycle: 0.036146, identity: 0.033846] ETA: 0:27:09.605355\n",
      "[Epoch 57/101] [Batch 200/476] [D loss: 0.021110] [G loss: 1.651141, adv: 0.848550, cycle: 0.052659, identity: 0.055200] ETA: 0:25:01.516382\n",
      "[Epoch 57/101] [Batch 300/476] [D loss: 0.016063] [G loss: 1.335580, adv: 0.711696, cycle: 0.044165, identity: 0.036446] ETA: 0:26:45.985806\n",
      "[Epoch 57/101] [Batch 400/476] [D loss: 0.040993] [G loss: 1.663179, adv: 0.952299, cycle: 0.045906, identity: 0.050364] ETA: 0:28:01.840942\n",
      "[Epoch 58/101] [Batch 0/476] [D loss: 0.082638] [G loss: 1.536423, adv: 0.742424, cycle: 0.050655, identity: 0.057490] ETA: 1:03:05.563799\n",
      "[Epoch 58/101] [Batch 100/476] [D loss: 0.058682] [G loss: 1.244136, adv: 0.554561, cycle: 0.046304, identity: 0.045308] ETA: 0:24:15.978210\n",
      "[Epoch 58/101] [Batch 200/476] [D loss: 0.081901] [G loss: 1.743255, adv: 0.755630, cycle: 0.069671, identity: 0.058183] ETA: 0:26:00.232950\n",
      "[Epoch 58/101] [Batch 300/476] [D loss: 0.017432] [G loss: 1.540594, adv: 0.774840, cycle: 0.052919, identity: 0.047313] ETA: 0:43:26.070671\n",
      "[Epoch 58/101] [Batch 400/476] [D loss: 0.020560] [G loss: 1.589156, adv: 0.966343, cycle: 0.043200, identity: 0.038163] ETA: 0:43:05.622710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 59/101] [Batch 0/476] [D loss: 0.138934] [G loss: 1.128624, adv: 0.419308, cycle: 0.047987, identity: 0.045889] ETA: 1:17:47.174017\n",
      "[Epoch 59/101] [Batch 100/476] [D loss: 0.013353] [G loss: 1.746883, adv: 1.042252, cycle: 0.047454, identity: 0.046018] ETA: 0:24:30.862676\n",
      "[Epoch 59/101] [Batch 200/476] [D loss: 0.039944] [G loss: 1.656278, adv: 1.011635, cycle: 0.043385, identity: 0.042158] ETA: 0:48:56.029957\n",
      "[Epoch 59/101] [Batch 300/476] [D loss: 0.057873] [G loss: 1.899410, adv: 0.809362, cycle: 0.073221, identity: 0.071568] ETA: 0:42:41.544448\n",
      "[Epoch 59/101] [Batch 400/476] [D loss: 0.131432] [G loss: 1.283900, adv: 0.587046, cycle: 0.046350, identity: 0.046670] ETA: 0:41:46.888903\n",
      "[Epoch 60/101] [Batch 0/476] [D loss: 0.026517] [G loss: 1.680996, adv: 0.965305, cycle: 0.050164, identity: 0.042810] ETA: 1:20:24.904490\n",
      "[Epoch 60/101] [Batch 100/476] [D loss: 0.041200] [G loss: 1.387608, adv: 0.692350, cycle: 0.048040, identity: 0.042972] ETA: 0:42:21.441479\n",
      "[Epoch 60/101] [Batch 200/476] [D loss: 0.014307] [G loss: 1.446960, adv: 0.740663, cycle: 0.047503, identity: 0.046253] ETA: 0:22:28.812110\n",
      "[Epoch 60/101] [Batch 300/476] [D loss: 0.096535] [G loss: 1.742823, adv: 0.844721, cycle: 0.061950, identity: 0.055720] ETA: 0:22:24.784271\n",
      "[Epoch 60/101] [Batch 400/476] [D loss: 0.044570] [G loss: 1.164246, adv: 0.624790, cycle: 0.038041, identity: 0.031809] ETA: 0:24:10.705353\n",
      "[Epoch 61/101] [Batch 0/476] [D loss: 0.024854] [G loss: 1.373447, adv: 0.665966, cycle: 0.048826, identity: 0.043844] ETA: 0:59:14.161720\n",
      "[Epoch 61/101] [Batch 100/476] [D loss: 0.042360] [G loss: 1.664470, adv: 0.986817, cycle: 0.040263, identity: 0.055003] ETA: 0:21:08.666759\n",
      "[Epoch 61/101] [Batch 200/476] [D loss: 0.074722] [G loss: 1.350048, adv: 0.688850, cycle: 0.044137, identity: 0.043965] ETA: 0:22:24.289742\n",
      "[Epoch 61/101] [Batch 300/476] [D loss: 0.020529] [G loss: 1.508699, adv: 0.843175, cycle: 0.047832, identity: 0.037441] ETA: 0:21:55.100574\n",
      "[Epoch 61/101] [Batch 400/476] [D loss: 0.080904] [G loss: 1.758381, adv: 0.887968, cycle: 0.059316, identity: 0.055451] ETA: 0:20:35.568218\n",
      "[Epoch 62/101] [Batch 0/476] [D loss: 0.022136] [G loss: 1.478794, adv: 0.774595, cycle: 0.051481, identity: 0.037879] ETA: 1:01:54.717344\n",
      "[Epoch 62/101] [Batch 100/476] [D loss: 0.015554] [G loss: 1.401246, adv: 0.511610, cycle: 0.063485, identity: 0.050957] ETA: 0:22:24.278992\n",
      "[Epoch 62/101] [Batch 200/476] [D loss: 0.035777] [G loss: 1.665261, adv: 0.932781, cycle: 0.050922, identity: 0.044651] ETA: 0:23:29.849298\n",
      "[Epoch 62/101] [Batch 300/476] [D loss: 0.023737] [G loss: 1.276670, adv: 0.555672, cycle: 0.050950, identity: 0.042299] ETA: 0:21:50.418869\n",
      "[Epoch 62/101] [Batch 400/476] [D loss: 0.041276] [G loss: 1.455943, adv: 0.707857, cycle: 0.049192, identity: 0.051234] ETA: 0:20:45.832767\n",
      "[Epoch 63/101] [Batch 0/476] [D loss: 0.033984] [G loss: 1.517058, adv: 0.785412, cycle: 0.050817, identity: 0.044696] ETA: 1:10:23.746445\n",
      "[Epoch 63/101] [Batch 100/476] [D loss: 0.067054] [G loss: 1.346205, adv: 0.603259, cycle: 0.052503, identity: 0.043583] ETA: 0:23:34.554591\n",
      "[Epoch 63/101] [Batch 200/476] [D loss: 0.025873] [G loss: 1.504547, adv: 0.764640, cycle: 0.052445, identity: 0.043092] ETA: 0:40:17.852417\n",
      "[Epoch 63/101] [Batch 300/476] [D loss: 0.046451] [G loss: 1.362016, adv: 0.635970, cycle: 0.049052, identity: 0.047105] ETA: 0:38:54.885353\n",
      "[Epoch 63/101] [Batch 400/476] [D loss: 0.034270] [G loss: 1.458424, adv: 0.763017, cycle: 0.048771, identity: 0.041540] ETA: 0:38:51.020027\n",
      "[Epoch 64/101] [Batch 0/476] [D loss: 0.037855] [G loss: 0.976048, adv: 0.427526, cycle: 0.037809, identity: 0.034086] ETA: 1:14:21.681417\n",
      "[Epoch 64/101] [Batch 100/476] [D loss: 0.061388] [G loss: 1.192559, adv: 0.661945, cycle: 0.036724, identity: 0.032674] ETA: 0:39:05.590357\n",
      "[Epoch 64/101] [Batch 200/476] [D loss: 0.036623] [G loss: 1.205914, adv: 0.669194, cycle: 0.037851, identity: 0.031642] ETA: 0:38:13.609423\n",
      "[Epoch 64/101] [Batch 300/476] [D loss: 0.025076] [G loss: 1.616890, adv: 0.631583, cycle: 0.068474, identity: 0.060114] ETA: 0:36:45.770325\n",
      "[Epoch 64/101] [Batch 400/476] [D loss: 0.032095] [G loss: 1.811012, adv: 0.931012, cycle: 0.060285, identity: 0.055430] ETA: 0:37:07.208434\n",
      "[Epoch 65/101] [Batch 0/476] [D loss: 0.009418] [G loss: 1.657421, adv: 1.095498, cycle: 0.038258, identity: 0.035868] ETA: 1:15:58.932381\n",
      "[Epoch 65/101] [Batch 100/476] [D loss: 0.048044] [G loss: 1.463770, adv: 0.576818, cycle: 0.059892, identity: 0.057606] ETA: 0:21:50.933633\n",
      "[Epoch 65/101] [Batch 200/476] [D loss: 0.012303] [G loss: 1.788550, adv: 0.825895, cycle: 0.064928, identity: 0.062675] ETA: 0:20:28.340343\n",
      "[Epoch 65/101] [Batch 300/476] [D loss: 0.010297] [G loss: 1.801553, adv: 1.106216, cycle: 0.048303, identity: 0.042462] ETA: 0:26:14.240564\n",
      "[Epoch 65/101] [Batch 400/476] [D loss: 0.012279] [G loss: 1.690639, adv: 0.913112, cycle: 0.054040, identity: 0.047425] ETA: 0:22:18.842812\n",
      "[Epoch 66/101] [Batch 0/476] [D loss: 0.069520] [G loss: 1.100410, adv: 0.384849, cycle: 0.047893, identity: 0.047327] ETA: 0:52:49.448476\n",
      "[Epoch 66/101] [Batch 100/476] [D loss: 0.015738] [G loss: 1.476388, adv: 0.859099, cycle: 0.043096, identity: 0.037265] ETA: 0:19:42.438068\n",
      "[Epoch 66/101] [Batch 200/476] [D loss: 0.047853] [G loss: 1.443861, adv: 0.694566, cycle: 0.050751, identity: 0.048356] ETA: 0:21:14.317441\n",
      "[Epoch 66/101] [Batch 300/476] [D loss: 0.025630] [G loss: 1.434010, adv: 0.770212, cycle: 0.046537, identity: 0.039686] ETA: 0:19:27.580137\n",
      "[Epoch 66/101] [Batch 400/476] [D loss: 0.063494] [G loss: 1.509649, adv: 0.863024, cycle: 0.043524, identity: 0.042277] ETA: 0:25:26.138892\n",
      "[Epoch 67/101] [Batch 0/476] [D loss: 0.014619] [G loss: 1.530565, adv: 0.820421, cycle: 0.049036, identity: 0.043956] ETA: 0:51:42.268000\n",
      "[Epoch 67/101] [Batch 100/476] [D loss: 0.016567] [G loss: 1.726860, adv: 0.891168, cycle: 0.058480, identity: 0.050178] ETA: 0:18:12.141021\n",
      "[Epoch 67/101] [Batch 200/476] [D loss: 0.019877] [G loss: 1.433074, adv: 0.668074, cycle: 0.053177, identity: 0.046647] ETA: 0:20:42.690697\n",
      "[Epoch 67/101] [Batch 300/476] [D loss: 0.016883] [G loss: 1.392625, adv: 0.756596, cycle: 0.043465, identity: 0.040275] ETA: 0:20:10.455609\n",
      "[Epoch 67/101] [Batch 400/476] [D loss: 0.023777] [G loss: 1.495809, adv: 0.866382, cycle: 0.042671, identity: 0.040544] ETA: 0:18:21.830755\n",
      "[Epoch 68/101] [Batch 0/476] [D loss: 0.046990] [G loss: 1.164000, adv: 0.467785, cycle: 0.045558, identity: 0.048126] ETA: 0:48:21.582378\n",
      "[Epoch 68/101] [Batch 100/476] [D loss: 0.017301] [G loss: 1.446909, adv: 0.790817, cycle: 0.044750, identity: 0.041719] ETA: 0:18:56.540260\n",
      "[Epoch 68/101] [Batch 200/476] [D loss: 0.011148] [G loss: 1.744489, adv: 0.866699, cycle: 0.059963, identity: 0.055632] ETA: 0:20:20.998253\n",
      "[Epoch 68/101] [Batch 300/476] [D loss: 0.055660] [G loss: 1.694878, adv: 1.096664, cycle: 0.041369, identity: 0.036906] ETA: 0:18:50.348728\n",
      "[Epoch 68/101] [Batch 400/476] [D loss: 0.058764] [G loss: 1.681844, adv: 0.958656, cycle: 0.047899, identity: 0.048840] ETA: 0:18:10.358643\n",
      "[Epoch 69/101] [Batch 0/476] [D loss: 0.016568] [G loss: 1.309246, adv: 0.779099, cycle: 0.036947, identity: 0.032136] ETA: 0:47:36.254211\n",
      "[Epoch 69/101] [Batch 100/476] [D loss: 0.026953] [G loss: 1.519652, adv: 0.743931, cycle: 0.051835, identity: 0.051474] ETA: 0:17:47.053376\n",
      "[Epoch 69/101] [Batch 200/476] [D loss: 0.011089] [G loss: 1.583803, adv: 0.905308, cycle: 0.047518, identity: 0.040663] ETA: 0:18:09.414875\n",
      "[Epoch 69/101] [Batch 300/476] [D loss: 0.019282] [G loss: 1.730367, adv: 1.049753, cycle: 0.046833, identity: 0.042456] ETA: 0:18:04.591975\n",
      "[Epoch 69/101] [Batch 400/476] [D loss: 0.043874] [G loss: 1.092192, adv: 0.544287, cycle: 0.037810, identity: 0.033961] ETA: 0:17:26.061138\n",
      "[Epoch 70/101] [Batch 0/476] [D loss: 0.018499] [G loss: 1.552722, adv: 0.752161, cycle: 0.055808, identity: 0.048497] ETA: 0:49:04.903296\n",
      "[Epoch 70/101] [Batch 100/476] [D loss: 0.091133] [G loss: 1.782050, adv: 1.023299, cycle: 0.052983, identity: 0.045783] ETA: 0:16:02.554062\n",
      "[Epoch 70/101] [Batch 200/476] [D loss: 0.022871] [G loss: 1.614947, adv: 0.737648, cycle: 0.055371, identity: 0.064718] ETA: 0:20:19.360125\n",
      "[Epoch 70/101] [Batch 300/476] [D loss: 0.013191] [G loss: 1.590449, adv: 0.917705, cycle: 0.047533, identity: 0.039483] ETA: 0:17:26.267641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 70/101] [Batch 400/476] [D loss: 0.032279] [G loss: 1.889623, adv: 0.965141, cycle: 0.059988, identity: 0.064921] ETA: 0:17:28.798532\n",
      "[Epoch 71/101] [Batch 0/476] [D loss: 0.028622] [G loss: 1.385433, adv: 0.719850, cycle: 0.046082, identity: 0.040953] ETA: 0:46:07.732573\n",
      "[Epoch 71/101] [Batch 100/476] [D loss: 0.010269] [G loss: 1.720196, adv: 1.007101, cycle: 0.047533, identity: 0.047552] ETA: 0:16:43.576431\n",
      "[Epoch 71/101] [Batch 200/476] [D loss: 0.015304] [G loss: 1.459889, adv: 0.793331, cycle: 0.043490, identity: 0.046332] ETA: 0:38:17.458496\n",
      "[Epoch 71/101] [Batch 300/476] [D loss: 0.013408] [G loss: 1.358763, adv: 0.630170, cycle: 0.048397, identity: 0.048924] ETA: 0:29:41.487536\n",
      "[Epoch 71/101] [Batch 400/476] [D loss: 0.011648] [G loss: 1.527033, adv: 0.821250, cycle: 0.049118, identity: 0.042921] ETA: 0:28:33.036509\n",
      "[Epoch 72/101] [Batch 0/476] [D loss: 0.037361] [G loss: 1.484689, adv: 0.634272, cycle: 0.058424, identity: 0.053236] ETA: 1:03:21.291429\n",
      "[Epoch 72/101] [Batch 100/476] [D loss: 0.022403] [G loss: 1.655828, adv: 0.829888, cycle: 0.060539, identity: 0.044109] ETA: 0:29:43.883818\n",
      "[Epoch 72/101] [Batch 200/476] [D loss: 0.024209] [G loss: 1.643378, adv: 0.957240, cycle: 0.047503, identity: 0.042221] ETA: 0:30:07.452643\n",
      "[Epoch 72/101] [Batch 300/476] [D loss: 0.010624] [G loss: 1.274923, adv: 0.722035, cycle: 0.038373, identity: 0.033832] ETA: 0:21:21.817917\n",
      "[Epoch 72/101] [Batch 400/476] [D loss: 0.017071] [G loss: 1.764430, adv: 0.887496, cycle: 0.059199, identity: 0.056988] ETA: 0:33:51.082921\n",
      "[Epoch 73/101] [Batch 0/476] [D loss: 0.028254] [G loss: 1.696496, adv: 1.003891, cycle: 0.046182, identity: 0.046157] ETA: 1:03:47.690037\n",
      "[Epoch 73/101] [Batch 100/476] [D loss: 0.008619] [G loss: 1.483817, adv: 0.804373, cycle: 0.047166, identity: 0.041557] ETA: 0:16:48.446781\n",
      "[Epoch 73/101] [Batch 200/476] [D loss: 0.020989] [G loss: 1.567290, adv: 0.733394, cycle: 0.057641, identity: 0.051498] ETA: 0:17:20.999359\n",
      "[Epoch 73/101] [Batch 300/476] [D loss: 0.023402] [G loss: 1.536736, adv: 0.887623, cycle: 0.045241, identity: 0.039341] ETA: 0:17:43.671215\n",
      "[Epoch 73/101] [Batch 400/476] [D loss: 0.020868] [G loss: 1.914275, adv: 0.963963, cycle: 0.063667, identity: 0.062729] ETA: 0:15:23.794922\n",
      "[Epoch 74/101] [Batch 0/476] [D loss: 0.025736] [G loss: 1.517412, adv: 0.809331, cycle: 0.050047, identity: 0.041523] ETA: 0:40:54.964680\n",
      "[Epoch 74/101] [Batch 100/476] [D loss: 0.024050] [G loss: 1.427009, adv: 0.835219, cycle: 0.042201, identity: 0.033955] ETA: 0:28:13.679726\n",
      "[Epoch 74/101] [Batch 200/476] [D loss: 0.023113] [G loss: 1.727064, adv: 1.171293, cycle: 0.039611, identity: 0.031932] ETA: 0:27:42.875361\n",
      "[Epoch 74/101] [Batch 300/476] [D loss: 0.012541] [G loss: 1.701187, adv: 1.006410, cycle: 0.047787, identity: 0.043382] ETA: 0:32:35.052263\n",
      "[Epoch 74/101] [Batch 400/476] [D loss: 0.019291] [G loss: 1.606962, adv: 0.910076, cycle: 0.049652, identity: 0.040074] ETA: 0:25:28.234168\n",
      "[Epoch 75/101] [Batch 0/476] [D loss: 0.010863] [G loss: 1.541665, adv: 0.831321, cycle: 0.049900, identity: 0.042269] ETA: 1:02:25.321676\n",
      "[Epoch 75/101] [Batch 100/476] [D loss: 0.032927] [G loss: 1.489804, adv: 0.844110, cycle: 0.047032, identity: 0.035075] ETA: 0:27:21.841361\n",
      "[Epoch 75/101] [Batch 200/476] [D loss: 0.036876] [G loss: 1.415572, adv: 0.715475, cycle: 0.047848, identity: 0.044324] ETA: 0:25:17.108471\n",
      "[Epoch 75/101] [Batch 300/476] [D loss: 0.018684] [G loss: 1.569324, adv: 0.945761, cycle: 0.044206, identity: 0.036300] ETA: 0:27:00.269259\n",
      "[Epoch 75/101] [Batch 400/476] [D loss: 0.016899] [G loss: 1.524235, adv: 0.842505, cycle: 0.047205, identity: 0.041935] ETA: 0:14:12.826984\n",
      "[Epoch 76/101] [Batch 0/476] [D loss: 0.026804] [G loss: 1.367178, adv: 0.752779, cycle: 0.042004, identity: 0.038871] ETA: 0:50:05.423093\n",
      "[Epoch 76/101] [Batch 100/476] [D loss: 0.044473] [G loss: 1.413378, adv: 0.631235, cycle: 0.054952, identity: 0.046525] ETA: 0:13:36.951513\n",
      "[Epoch 76/101] [Batch 200/476] [D loss: 0.013024] [G loss: 1.244958, adv: 0.525532, cycle: 0.049724, identity: 0.044436] ETA: 0:13:53.234024\n",
      "[Epoch 76/101] [Batch 300/476] [D loss: 0.022405] [G loss: 1.697885, adv: 0.847554, cycle: 0.060619, identity: 0.048827] ETA: 0:13:38.423843\n",
      "[Epoch 76/101] [Batch 400/476] [D loss: 0.028622] [G loss: 1.883469, adv: 1.034932, cycle: 0.057943, identity: 0.053822] ETA: 0:14:16.062770\n",
      "[Epoch 77/101] [Batch 0/476] [D loss: 0.021889] [G loss: 1.644252, adv: 0.859997, cycle: 0.055176, identity: 0.046499] ETA: 0:35:17.968849\n",
      "[Epoch 77/101] [Batch 100/476] [D loss: 0.005253] [G loss: 1.395213, adv: 0.689714, cycle: 0.048803, identity: 0.043493] ETA: 0:13:45.406850\n",
      "[Epoch 77/101] [Batch 200/476] [D loss: 0.020268] [G loss: 1.084187, adv: 0.499660, cycle: 0.037056, identity: 0.042794] ETA: 0:13:33.830877\n",
      "[Epoch 77/101] [Batch 300/476] [D loss: 0.021237] [G loss: 1.764155, adv: 1.006983, cycle: 0.052151, identity: 0.047132] ETA: 0:13:31.168319\n",
      "[Epoch 77/101] [Batch 400/476] [D loss: 0.074146] [G loss: 1.306780, adv: 0.544025, cycle: 0.054407, identity: 0.043736] ETA: 0:13:22.932693\n",
      "[Epoch 78/101] [Batch 0/476] [D loss: 0.045727] [G loss: 1.816139, adv: 1.107997, cycle: 0.049490, identity: 0.042649] ETA: 0:41:23.883045\n",
      "[Epoch 78/101] [Batch 100/476] [D loss: 0.055161] [G loss: 1.512779, adv: 0.894377, cycle: 0.041831, identity: 0.040018] ETA: 0:17:24.769798\n",
      "[Epoch 78/101] [Batch 200/476] [D loss: 0.065590] [G loss: 1.399308, adv: 0.716641, cycle: 0.044736, identity: 0.047061] ETA: 0:12:59.837215\n",
      "[Epoch 78/101] [Batch 300/476] [D loss: 0.049050] [G loss: 1.596613, adv: 0.731408, cycle: 0.060127, identity: 0.052788] ETA: 0:12:08.436441\n",
      "[Epoch 78/101] [Batch 400/476] [D loss: 0.028172] [G loss: 1.399579, adv: 0.664789, cycle: 0.048105, identity: 0.050748] ETA: 0:12:52.996176\n",
      "[Epoch 79/101] [Batch 0/476] [D loss: 0.075026] [G loss: 1.776484, adv: 1.129165, cycle: 0.045460, identity: 0.038543] ETA: 0:32:55.371901\n",
      "[Epoch 79/101] [Batch 100/476] [D loss: 0.007613] [G loss: 1.263616, adv: 0.627079, cycle: 0.044110, identity: 0.039087] ETA: 0:14:47.288231\n",
      "[Epoch 79/101] [Batch 200/476] [D loss: 0.018358] [G loss: 1.556885, adv: 0.783881, cycle: 0.055891, identity: 0.042819] ETA: 0:12:38.299805\n",
      "[Epoch 79/101] [Batch 300/476] [D loss: 0.014714] [G loss: 1.859931, adv: 0.985493, cycle: 0.059099, identity: 0.056690] ETA: 0:13:52.224649\n",
      "[Epoch 79/101] [Batch 400/476] [D loss: 0.020165] [G loss: 1.581953, adv: 0.768766, cycle: 0.056045, identity: 0.050547] ETA: 0:14:27.106569\n",
      "[Epoch 80/101] [Batch 0/476] [D loss: 0.027014] [G loss: 1.644265, adv: 1.010293, cycle: 0.045414, identity: 0.035967] ETA: 0:30:48.089261\n",
      "[Epoch 80/101] [Batch 100/476] [D loss: 0.032099] [G loss: 1.550232, adv: 0.719392, cycle: 0.060683, identity: 0.044802] ETA: 0:11:28.529062\n",
      "[Epoch 80/101] [Batch 200/476] [D loss: 0.049803] [G loss: 1.983983, adv: 1.296660, cycle: 0.049312, identity: 0.038840] ETA: 0:12:51.391932\n",
      "[Epoch 80/101] [Batch 300/476] [D loss: 0.010598] [G loss: 1.553492, adv: 0.915060, cycle: 0.043866, identity: 0.039955] ETA: 0:13:50.011299\n",
      "[Epoch 80/101] [Batch 400/476] [D loss: 0.018288] [G loss: 1.684732, adv: 0.941435, cycle: 0.050687, identity: 0.047286] ETA: 0:11:49.608686\n",
      "[Epoch 81/101] [Batch 0/476] [D loss: 0.044165] [G loss: 1.293587, adv: 0.782853, cycle: 0.034395, identity: 0.033356] ETA: 0:31:25.093479\n",
      "[Epoch 81/101] [Batch 100/476] [D loss: 0.024132] [G loss: 1.249800, adv: 0.543936, cycle: 0.049520, identity: 0.042133] ETA: 0:10:32.511420\n",
      "[Epoch 81/101] [Batch 200/476] [D loss: 0.052852] [G loss: 1.153533, adv: 0.525546, cycle: 0.043719, identity: 0.038160] ETA: 0:11:54.563761\n",
      "[Epoch 81/101] [Batch 300/476] [D loss: 0.049056] [G loss: 1.790343, adv: 1.052497, cycle: 0.051473, identity: 0.044624] ETA: 0:12:36.310539\n",
      "[Epoch 81/101] [Batch 400/476] [D loss: 0.009267] [G loss: 1.310093, adv: 0.737182, cycle: 0.035568, identity: 0.043447] ETA: 0:12:25.683174\n",
      "[Epoch 82/101] [Batch 0/476] [D loss: 0.028720] [G loss: 1.318407, adv: 0.494151, cycle: 0.056122, identity: 0.052607] ETA: 0:30:55.985817\n",
      "[Epoch 82/101] [Batch 100/476] [D loss: 0.015345] [G loss: 1.527075, adv: 0.813994, cycle: 0.051144, identity: 0.040328] ETA: 0:11:49.518700\n",
      "[Epoch 82/101] [Batch 200/476] [D loss: 0.027613] [G loss: 1.429468, adv: 0.704387, cycle: 0.049700, identity: 0.045616] ETA: 0:10:26.098852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 82/101] [Batch 300/476] [D loss: 0.020429] [G loss: 1.556436, adv: 0.827939, cycle: 0.044579, identity: 0.056541] ETA: 0:12:07.573572\n",
      "[Epoch 82/101] [Batch 400/476] [D loss: 0.013156] [G loss: 1.390901, adv: 0.728318, cycle: 0.045296, identity: 0.041924] ETA: 0:11:52.268383\n",
      "[Epoch 83/101] [Batch 0/476] [D loss: 0.068793] [G loss: 1.206776, adv: 0.656340, cycle: 0.037489, identity: 0.035109] ETA: 0:25:41.970926\n",
      "[Epoch 83/101] [Batch 100/476] [D loss: 0.024264] [G loss: 1.629300, adv: 0.841859, cycle: 0.054629, identity: 0.048229] ETA: 0:10:15.539005\n",
      "[Epoch 83/101] [Batch 200/476] [D loss: 0.128556] [G loss: 1.472060, adv: 0.461981, cycle: 0.067453, identity: 0.067109] ETA: 0:09:40.412609\n",
      "[Epoch 83/101] [Batch 300/476] [D loss: 0.022637] [G loss: 1.460107, adv: 0.730799, cycle: 0.048606, identity: 0.048649] ETA: 0:09:54.702876\n",
      "[Epoch 83/101] [Batch 400/476] [D loss: 0.029083] [G loss: 1.369601, adv: 0.683007, cycle: 0.048564, identity: 0.040192] ETA: 0:09:52.201344\n",
      "[Epoch 84/101] [Batch 0/476] [D loss: 0.040137] [G loss: 1.397108, adv: 0.775123, cycle: 0.042725, identity: 0.038946] ETA: 0:25:03.247263\n",
      "[Epoch 84/101] [Batch 100/476] [D loss: 0.010196] [G loss: 1.396096, adv: 0.714955, cycle: 0.049108, identity: 0.038012] ETA: 0:10:00.093962\n",
      "[Epoch 84/101] [Batch 200/476] [D loss: 0.021862] [G loss: 1.418019, adv: 0.824014, cycle: 0.041963, identity: 0.034874] ETA: 0:09:23.561607\n",
      "[Epoch 84/101] [Batch 300/476] [D loss: 0.064767] [G loss: 1.198805, adv: 0.537187, cycle: 0.046898, identity: 0.038528] ETA: 0:08:55.513481\n",
      "[Epoch 84/101] [Batch 400/476] [D loss: 0.019493] [G loss: 1.383167, adv: 0.667377, cycle: 0.047446, identity: 0.048266] ETA: 0:09:27.296150\n",
      "[Epoch 85/101] [Batch 0/476] [D loss: 0.016674] [G loss: 1.390232, adv: 0.838668, cycle: 0.038571, identity: 0.033170] ETA: 0:27:20.133987\n",
      "[Epoch 85/101] [Batch 100/476] [D loss: 0.022985] [G loss: 1.509643, adv: 0.862432, cycle: 0.045458, identity: 0.038526] ETA: 0:08:58.193685\n",
      "[Epoch 85/101] [Batch 200/476] [D loss: 0.012688] [G loss: 1.519945, adv: 0.916419, cycle: 0.042100, identity: 0.036505] ETA: 0:09:08.710630\n",
      "[Epoch 85/101] [Batch 300/476] [D loss: 0.010748] [G loss: 1.740723, adv: 1.028259, cycle: 0.050185, identity: 0.042123] ETA: 0:10:02.120372\n",
      "[Epoch 85/101] [Batch 400/476] [D loss: 0.010043] [G loss: 1.819200, adv: 0.908619, cycle: 0.062964, identity: 0.056188] ETA: 0:08:59.768948\n",
      "[Epoch 86/101] [Batch 0/476] [D loss: 0.041488] [G loss: 1.106254, adv: 0.537132, cycle: 0.039497, identity: 0.034831] ETA: 0:22:36.276970\n",
      "[Epoch 86/101] [Batch 100/476] [D loss: 0.032926] [G loss: 1.564382, adv: 0.832850, cycle: 0.050621, identity: 0.045065] ETA: 0:08:53.356323\n",
      "[Epoch 86/101] [Batch 200/476] [D loss: 0.033349] [G loss: 1.541315, adv: 0.943741, cycle: 0.040583, identity: 0.038349] ETA: 0:07:55.337343\n",
      "[Epoch 86/101] [Batch 300/476] [D loss: 0.024806] [G loss: 1.079070, adv: 0.366723, cycle: 0.048183, identity: 0.046103] ETA: 0:08:17.729673\n",
      "[Epoch 86/101] [Batch 400/476] [D loss: 0.013921] [G loss: 1.373118, adv: 0.770108, cycle: 0.041575, identity: 0.037452] ETA: 0:08:18.053756\n",
      "[Epoch 87/101] [Batch 0/476] [D loss: 0.036201] [G loss: 2.097955, adv: 0.867509, cycle: 0.081939, identity: 0.082212] ETA: 0:21:31.881811\n",
      "[Epoch 87/101] [Batch 100/476] [D loss: 0.034690] [G loss: 1.391730, adv: 0.704118, cycle: 0.048543, identity: 0.040436] ETA: 0:08:20.026617\n",
      "[Epoch 87/101] [Batch 200/476] [D loss: 0.029336] [G loss: 1.728645, adv: 0.790443, cycle: 0.065881, identity: 0.055878] ETA: 0:08:09.866028\n",
      "[Epoch 87/101] [Batch 300/476] [D loss: 0.033647] [G loss: 1.292809, adv: 0.745502, cycle: 0.038130, identity: 0.033201] ETA: 0:07:54.555515\n",
      "[Epoch 87/101] [Batch 400/476] [D loss: 0.040252] [G loss: 1.663899, adv: 0.721333, cycle: 0.066264, identity: 0.055986] ETA: 0:07:40.419914\n",
      "[Epoch 88/101] [Batch 0/476] [D loss: 0.008394] [G loss: 1.445446, adv: 0.842558, cycle: 0.043110, identity: 0.034358] ETA: 0:21:37.911956\n",
      "[Epoch 88/101] [Batch 100/476] [D loss: 0.051623] [G loss: 1.485932, adv: 0.883138, cycle: 0.041035, identity: 0.038489] ETA: 0:07:21.712334\n",
      "[Epoch 88/101] [Batch 200/476] [D loss: 0.005682] [G loss: 1.545269, adv: 0.918246, cycle: 0.043297, identity: 0.038811] ETA: 0:08:18.749685\n",
      "[Epoch 88/101] [Batch 300/476] [D loss: 0.007333] [G loss: 1.564811, adv: 1.031492, cycle: 0.036688, identity: 0.033288] ETA: 0:06:52.255066\n",
      "[Epoch 88/101] [Batch 400/476] [D loss: 0.032983] [G loss: 1.460753, adv: 0.767130, cycle: 0.048367, identity: 0.041991] ETA: 0:07:17.933824\n",
      "[Epoch 89/101] [Batch 0/476] [D loss: 0.025771] [G loss: 1.662426, adv: 1.041206, cycle: 0.042319, identity: 0.039605] ETA: 0:17:32.244644\n",
      "[Epoch 89/101] [Batch 100/476] [D loss: 0.064976] [G loss: 1.662706, adv: 0.807329, cycle: 0.059283, identity: 0.052510] ETA: 0:07:41.064504\n",
      "[Epoch 89/101] [Batch 200/476] [D loss: 0.015543] [G loss: 1.375696, adv: 0.806128, cycle: 0.040009, identity: 0.033896] ETA: 0:06:17.758842\n",
      "[Epoch 89/101] [Batch 300/476] [D loss: 0.019249] [G loss: 1.363626, adv: 0.616843, cycle: 0.052058, identity: 0.045241] ETA: 0:06:38.018975\n",
      "[Epoch 89/101] [Batch 400/476] [D loss: 0.049410] [G loss: 1.854245, adv: 1.104846, cycle: 0.049034, identity: 0.051812] ETA: 0:06:37.066650\n",
      "[Epoch 90/101] [Batch 0/476] [D loss: 0.068062] [G loss: 1.261935, adv: 0.561249, cycle: 0.049221, identity: 0.041696] ETA: 0:16:31.577087\n",
      "[Epoch 90/101] [Batch 100/476] [D loss: 0.093162] [G loss: 1.262925, adv: 0.694412, cycle: 0.040208, identity: 0.033287] ETA: 0:05:59.785378\n",
      "[Epoch 90/101] [Batch 200/476] [D loss: 0.021411] [G loss: 1.504661, adv: 0.863673, cycle: 0.046389, identity: 0.035419] ETA: 0:05:42.998303\n",
      "[Epoch 90/101] [Batch 300/476] [D loss: 0.007046] [G loss: 1.615388, adv: 0.901501, cycle: 0.049142, identity: 0.044493] ETA: 0:06:12.205561\n",
      "[Epoch 90/101] [Batch 400/476] [D loss: 0.012343] [G loss: 1.580642, adv: 0.907516, cycle: 0.045899, identity: 0.042827] ETA: 0:06:14.382348\n",
      "[Epoch 91/101] [Batch 0/476] [D loss: 0.012574] [G loss: 1.768855, adv: 0.991875, cycle: 0.054021, identity: 0.047354] ETA: 0:15:58.075199\n",
      "[Epoch 91/101] [Batch 100/476] [D loss: 0.020185] [G loss: 1.480325, adv: 0.886626, cycle: 0.042513, identity: 0.033714] ETA: 0:05:36.764479\n",
      "[Epoch 91/101] [Batch 200/476] [D loss: 0.014675] [G loss: 1.582227, adv: 0.881833, cycle: 0.048228, identity: 0.043623] ETA: 0:05:33.720188\n",
      "[Epoch 91/101] [Batch 300/476] [D loss: 0.023054] [G loss: 1.395006, adv: 0.716653, cycle: 0.047611, identity: 0.040448] ETA: 0:05:19.252882\n",
      "[Epoch 91/101] [Batch 400/476] [D loss: 0.057220] [G loss: 1.531758, adv: 0.863991, cycle: 0.045250, identity: 0.043053] ETA: 0:04:58.297396\n",
      "[Epoch 92/101] [Batch 0/476] [D loss: 0.036496] [G loss: 1.117490, adv: 0.514078, cycle: 0.041867, identity: 0.036948] ETA: 0:13:50.936688\n",
      "[Epoch 92/101] [Batch 100/476] [D loss: 0.055331] [G loss: 1.239045, adv: 0.634680, cycle: 0.041090, identity: 0.038693] ETA: 0:05:15.120947\n",
      "[Epoch 92/101] [Batch 200/476] [D loss: 0.012427] [G loss: 1.541217, adv: 0.860627, cycle: 0.046498, identity: 0.043122] ETA: 0:05:44.984381\n",
      "[Epoch 92/101] [Batch 300/476] [D loss: 0.008528] [G loss: 2.030771, adv: 0.861552, cycle: 0.081328, identity: 0.071188] ETA: 0:04:23.977386\n",
      "[Epoch 92/101] [Batch 400/476] [D loss: 0.061852] [G loss: 1.847830, adv: 1.156066, cycle: 0.047027, identity: 0.044300] ETA: 0:04:51.050160\n",
      "[Epoch 93/101] [Batch 0/476] [D loss: 0.016506] [G loss: 1.571195, adv: 0.898724, cycle: 0.047444, identity: 0.039606] ETA: 0:11:52.807930\n",
      "[Epoch 93/101] [Batch 100/476] [D loss: 0.007835] [G loss: 1.749054, adv: 0.863058, cycle: 0.062145, identity: 0.052910] ETA: 0:04:32.445754\n",
      "[Epoch 93/101] [Batch 200/476] [D loss: 0.019226] [G loss: 1.685026, adv: 1.023172, cycle: 0.046536, identity: 0.039299] ETA: 0:04:41.374355\n",
      "[Epoch 93/101] [Batch 300/476] [D loss: 0.005973] [G loss: 1.566123, adv: 0.928616, cycle: 0.046001, identity: 0.035498] ETA: 0:04:47.548168\n",
      "[Epoch 93/101] [Batch 400/476] [D loss: 0.021281] [G loss: 1.712288, adv: 1.103530, cycle: 0.043547, identity: 0.034658] ETA: 0:04:10.473907\n",
      "[Epoch 94/101] [Batch 0/476] [D loss: 0.008411] [G loss: 1.367576, adv: 0.764356, cycle: 0.042281, identity: 0.036082] ETA: 0:10:21.084589\n",
      "[Epoch 94/101] [Batch 100/476] [D loss: 0.012556] [G loss: 1.694242, adv: 1.032140, cycle: 0.045376, identity: 0.041668] ETA: 0:04:23.846626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 94/101] [Batch 200/476] [D loss: 0.030843] [G loss: 1.424352, adv: 0.752295, cycle: 0.045977, identity: 0.042457] ETA: 0:03:56.957382\n",
      "[Epoch 94/101] [Batch 300/476] [D loss: 0.016541] [G loss: 1.484006, adv: 0.745556, cycle: 0.049514, identity: 0.048663] ETA: 0:03:41.519474\n",
      "[Epoch 94/101] [Batch 400/476] [D loss: 0.032332] [G loss: 1.763585, adv: 1.024836, cycle: 0.051555, identity: 0.044640] ETA: 0:03:59.854330\n",
      "[Epoch 95/101] [Batch 0/476] [D loss: 0.055739] [G loss: 1.458405, adv: 0.889215, cycle: 0.038713, identity: 0.036411] ETA: 0:09:24.996243\n",
      "[Epoch 95/101] [Batch 100/476] [D loss: 0.036248] [G loss: 1.895295, adv: 1.155706, cycle: 0.049479, identity: 0.048960] ETA: 0:03:31.301017\n",
      "[Epoch 95/101] [Batch 200/476] [D loss: 0.022825] [G loss: 1.577539, adv: 0.773625, cycle: 0.055870, identity: 0.049044] ETA: 0:03:23.081253\n",
      "[Epoch 95/101] [Batch 300/476] [D loss: 0.007244] [G loss: 1.536377, adv: 0.873663, cycle: 0.047026, identity: 0.038491] ETA: 0:02:54.434052\n",
      "[Epoch 95/101] [Batch 400/476] [D loss: 0.010044] [G loss: 1.405695, adv: 0.709801, cycle: 0.047544, identity: 0.044092] ETA: 0:02:46.481777\n",
      "[Epoch 96/101] [Batch 0/476] [D loss: 0.010690] [G loss: 1.347674, adv: 0.682569, cycle: 0.045411, identity: 0.042199] ETA: 0:07:30.181198\n",
      "[Epoch 96/101] [Batch 100/476] [D loss: 0.018242] [G loss: 1.749711, adv: 1.042320, cycle: 0.048995, identity: 0.043489] ETA: 0:03:00.548344\n",
      "[Epoch 96/101] [Batch 200/476] [D loss: 0.022620] [G loss: 1.305547, adv: 0.578726, cycle: 0.052327, identity: 0.040711] ETA: 0:02:42.073903\n",
      "[Epoch 96/101] [Batch 300/476] [D loss: 0.027909] [G loss: 1.479067, adv: 0.807578, cycle: 0.046308, identity: 0.041683] ETA: 0:02:26.352158\n",
      "[Epoch 96/101] [Batch 400/476] [D loss: 0.019448] [G loss: 1.537125, adv: 0.879964, cycle: 0.045837, identity: 0.039758] ETA: 0:02:29.414020\n",
      "[Epoch 97/101] [Batch 0/476] [D loss: 0.012583] [G loss: 1.639965, adv: 0.949890, cycle: 0.046160, identity: 0.045696] ETA: 0:06:36.361008\n",
      "[Epoch 97/101] [Batch 100/476] [D loss: 0.018256] [G loss: 1.575321, adv: 0.879178, cycle: 0.050163, identity: 0.038902] ETA: 0:02:24.180938\n",
      "[Epoch 97/101] [Batch 200/476] [D loss: 0.012490] [G loss: 1.501821, adv: 0.823759, cycle: 0.048786, identity: 0.038039] ETA: 0:02:03.217003\n",
      "[Epoch 97/101] [Batch 300/476] [D loss: 0.009332] [G loss: 1.349513, adv: 0.755531, cycle: 0.042724, identity: 0.033348] ETA: 0:02:10.171189\n",
      "[Epoch 97/101] [Batch 400/476] [D loss: 0.013376] [G loss: 1.280166, adv: 0.688865, cycle: 0.041847, identity: 0.034566] ETA: 0:01:36.315720\n",
      "[Epoch 98/101] [Batch 0/476] [D loss: 0.010784] [G loss: 1.594692, adv: 0.835607, cycle: 0.052747, identity: 0.046323] ETA: 0:04:34.827859\n",
      "[Epoch 98/101] [Batch 100/476] [D loss: 0.031326] [G loss: 1.894945, adv: 1.006412, cycle: 0.061258, identity: 0.055191] ETA: 0:01:45.193787\n",
      "[Epoch 98/101] [Batch 200/476] [D loss: 0.027542] [G loss: 1.551597, adv: 0.834248, cycle: 0.049811, identity: 0.043848] ETA: 0:01:34.086850\n",
      "[Epoch 98/101] [Batch 300/476] [D loss: 0.037328] [G loss: 1.510976, adv: 0.854814, cycle: 0.045545, identity: 0.040142] ETA: 0:01:24.482529\n",
      "[Epoch 98/101] [Batch 400/476] [D loss: 0.024201] [G loss: 1.772516, adv: 0.934794, cycle: 0.057678, identity: 0.052188] ETA: 0:01:14.389061\n",
      "[Epoch 99/101] [Batch 0/476] [D loss: 0.056768] [G loss: 1.595508, adv: 0.743956, cycle: 0.055972, identity: 0.058365] ETA: 0:03:08.425594\n",
      "[Epoch 99/101] [Batch 100/476] [D loss: 0.019532] [G loss: 1.293166, adv: 0.764968, cycle: 0.036860, identity: 0.031920] ETA: 0:01:00.065709\n",
      "[Epoch 99/101] [Batch 200/476] [D loss: 0.009211] [G loss: 1.511303, adv: 0.890475, cycle: 0.043438, identity: 0.037289] ETA: 0:00:57.529030\n",
      "[Epoch 99/101] [Batch 300/476] [D loss: 0.025408] [G loss: 1.520432, adv: 0.799652, cycle: 0.049728, identity: 0.044700] ETA: 0:00:47.741159\n",
      "[Epoch 99/101] [Batch 400/476] [D loss: 0.010931] [G loss: 1.561440, adv: 0.913575, cycle: 0.045100, identity: 0.039372] ETA: 0:00:44.951838\n",
      "[Epoch 100/101] [Batch 0/476] [D loss: 0.023940] [G loss: 1.617580, adv: 0.851352, cycle: 0.053852, identity: 0.045542] ETA: 0:01:27.839581\n",
      "[Epoch 100/101] [Batch 100/476] [D loss: 0.009710] [G loss: 1.598064, adv: 0.936781, cycle: 0.046093, identity: 0.040070] ETA: 0:00:28.433006\n",
      "[Epoch 100/101] [Batch 200/476] [D loss: 0.020535] [G loss: 1.522043, adv: 0.931145, cycle: 0.041137, identity: 0.035905] ETA: 0:00:23.011165\n",
      "[Epoch 100/101] [Batch 300/476] [D loss: 0.015215] [G loss: 1.607677, adv: 0.837078, cycle: 0.058562, identity: 0.036995] ETA: 0:00:14.319420\n",
      "[Epoch 100/101] [Batch 400/476] [D loss: 0.011886] [G loss: 1.649249, adv: 1.011991, cycle: 0.044945, identity: 0.037562] ETA: 0:00:06.148614\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = 0\n",
    "dataset_name = 'CycleGAN1'\n",
    "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader1, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
