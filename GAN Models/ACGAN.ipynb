{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = torch.load(\"../dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "NC = [sample[0] for sample in NC]\n",
    "NCgan1 = [sample[0][0] for sample in NC]\n",
    "NCgan2 = [sample[1][0] for sample in NC]\n",
    "NCgan3 = [sample[2][0] for sample in NC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13588644c304f3892576a9f77ef04c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 1.497860, acc: 9%] [G loss: 1.499923]\n",
      "[Epoch 0/200] [Batch 1/938] [D loss: 1.497961, acc: 10%] [G loss: 1.500127]\n",
      "[Epoch 0/200] [Batch 2/938] [D loss: 1.497875, acc: 8%] [G loss: 1.499638]\n",
      "[Epoch 0/200] [Batch 3/938] [D loss: 1.497920, acc: 7%] [G loss: 1.499563]\n",
      "[Epoch 0/200] [Batch 4/938] [D loss: 1.497827, acc: 8%] [G loss: 1.499763]\n",
      "[Epoch 0/200] [Batch 5/938] [D loss: 1.497803, acc: 5%] [G loss: 1.499290]\n",
      "[Epoch 0/200] [Batch 6/938] [D loss: 1.497789, acc: 8%] [G loss: 1.499153]\n",
      "[Epoch 0/200] [Batch 7/938] [D loss: 1.497477, acc: 9%] [G loss: 1.498912]\n",
      "[Epoch 0/200] [Batch 8/938] [D loss: 1.497625, acc: 10%] [G loss: 1.498599]\n",
      "[Epoch 0/200] [Batch 9/938] [D loss: 1.497760, acc: 11%] [G loss: 1.498834]\n",
      "[Epoch 0/200] [Batch 10/938] [D loss: 1.497701, acc: 8%] [G loss: 1.498555]\n",
      "[Epoch 0/200] [Batch 11/938] [D loss: 1.497640, acc: 9%] [G loss: 1.498878]\n",
      "[Epoch 0/200] [Batch 12/938] [D loss: 1.497546, acc: 7%] [G loss: 1.498362]\n",
      "[Epoch 0/200] [Batch 13/938] [D loss: 1.497435, acc: 7%] [G loss: 1.498036]\n",
      "[Epoch 0/200] [Batch 14/938] [D loss: 1.497329, acc: 8%] [G loss: 1.498259]\n",
      "[Epoch 0/200] [Batch 15/938] [D loss: 1.496989, acc: 17%] [G loss: 1.497728]\n",
      "[Epoch 0/200] [Batch 16/938] [D loss: 1.497136, acc: 9%] [G loss: 1.497611]\n",
      "[Epoch 0/200] [Batch 17/938] [D loss: 1.497063, acc: 15%] [G loss: 1.497319]\n",
      "[Epoch 0/200] [Batch 18/938] [D loss: 1.496648, acc: 14%] [G loss: 1.496871]\n",
      "[Epoch 0/200] [Batch 19/938] [D loss: 1.497159, acc: 4%] [G loss: 1.496543]\n",
      "[Epoch 0/200] [Batch 20/938] [D loss: 1.496902, acc: 9%] [G loss: 1.496250]\n",
      "[Epoch 0/200] [Batch 21/938] [D loss: 1.497470, acc: 10%] [G loss: 1.494852]\n",
      "[Epoch 0/200] [Batch 22/938] [D loss: 1.497326, acc: 6%] [G loss: 1.494632]\n",
      "[Epoch 0/200] [Batch 23/938] [D loss: 1.497010, acc: 8%] [G loss: 1.494761]\n",
      "[Epoch 0/200] [Batch 24/938] [D loss: 1.497592, acc: 6%] [G loss: 1.494140]\n",
      "[Epoch 0/200] [Batch 25/938] [D loss: 1.497621, acc: 12%] [G loss: 1.493971]\n",
      "[Epoch 0/200] [Batch 26/938] [D loss: 1.497041, acc: 8%] [G loss: 1.494310]\n",
      "[Epoch 0/200] [Batch 27/938] [D loss: 1.496892, acc: 8%] [G loss: 1.495290]\n",
      "[Epoch 0/200] [Batch 28/938] [D loss: 1.497219, acc: 6%] [G loss: 1.495128]\n",
      "[Epoch 0/200] [Batch 29/938] [D loss: 1.496856, acc: 6%] [G loss: 1.494982]\n",
      "[Epoch 0/200] [Batch 30/938] [D loss: 1.496887, acc: 6%] [G loss: 1.495174]\n",
      "[Epoch 0/200] [Batch 31/938] [D loss: 1.496628, acc: 13%] [G loss: 1.495235]\n",
      "[Epoch 0/200] [Batch 32/938] [D loss: 1.496837, acc: 12%] [G loss: 1.496666]\n",
      "[Epoch 0/200] [Batch 33/938] [D loss: 1.496241, acc: 10%] [G loss: 1.496303]\n",
      "[Epoch 0/200] [Batch 34/938] [D loss: 1.495704, acc: 6%] [G loss: 1.498424]\n",
      "[Epoch 0/200] [Batch 35/938] [D loss: 1.495765, acc: 7%] [G loss: 1.499531]\n",
      "[Epoch 0/200] [Batch 36/938] [D loss: 1.494784, acc: 10%] [G loss: 1.500058]\n",
      "[Epoch 0/200] [Batch 37/938] [D loss: 1.494885, acc: 13%] [G loss: 1.500222]\n",
      "[Epoch 0/200] [Batch 38/938] [D loss: 1.494757, acc: 12%] [G loss: 1.498927]\n",
      "[Epoch 0/200] [Batch 39/938] [D loss: 1.496076, acc: 14%] [G loss: 1.496969]\n",
      "[Epoch 0/200] [Batch 40/938] [D loss: 1.495573, acc: 4%] [G loss: 1.496652]\n",
      "[Epoch 0/200] [Batch 41/938] [D loss: 1.496893, acc: 9%] [G loss: 1.493449]\n",
      "[Epoch 0/200] [Batch 42/938] [D loss: 1.496524, acc: 10%] [G loss: 1.493037]\n",
      "[Epoch 0/200] [Batch 43/938] [D loss: 1.495795, acc: 12%] [G loss: 1.490996]\n",
      "[Epoch 0/200] [Batch 44/938] [D loss: 1.494797, acc: 13%] [G loss: 1.489732]\n",
      "[Epoch 0/200] [Batch 45/938] [D loss: 1.495314, acc: 12%] [G loss: 1.486878]\n",
      "[Epoch 0/200] [Batch 46/938] [D loss: 1.494111, acc: 12%] [G loss: 1.487977]\n",
      "[Epoch 0/200] [Batch 47/938] [D loss: 1.491791, acc: 10%] [G loss: 1.485170]\n",
      "[Epoch 0/200] [Batch 48/938] [D loss: 1.490501, acc: 13%] [G loss: 1.485542]\n",
      "[Epoch 0/200] [Batch 49/938] [D loss: 1.490803, acc: 7%] [G loss: 1.481947]\n",
      "[Epoch 0/200] [Batch 50/938] [D loss: 1.490206, acc: 10%] [G loss: 1.473783]\n",
      "[Epoch 0/200] [Batch 51/938] [D loss: 1.489644, acc: 7%] [G loss: 1.473542]\n",
      "[Epoch 0/200] [Batch 52/938] [D loss: 1.491985, acc: 11%] [G loss: 1.468972]\n",
      "[Epoch 0/200] [Batch 53/938] [D loss: 1.497110, acc: 10%] [G loss: 1.454079]\n",
      "[Epoch 0/200] [Batch 54/938] [D loss: 1.496333, acc: 11%] [G loss: 1.458303]\n",
      "[Epoch 0/200] [Batch 55/938] [D loss: 1.495517, acc: 7%] [G loss: 1.465829]\n",
      "[Epoch 0/200] [Batch 56/938] [D loss: 1.499066, acc: 12%] [G loss: 1.463688]\n",
      "[Epoch 0/200] [Batch 57/938] [D loss: 1.497956, acc: 10%] [G loss: 1.467490]\n",
      "[Epoch 0/200] [Batch 58/938] [D loss: 1.496314, acc: 13%] [G loss: 1.475632]\n",
      "[Epoch 0/200] [Batch 59/938] [D loss: 1.496728, acc: 15%] [G loss: 1.476090]\n",
      "[Epoch 0/200] [Batch 60/938] [D loss: 1.496144, acc: 10%] [G loss: 1.479702]\n",
      "[Epoch 0/200] [Batch 61/938] [D loss: 1.497976, acc: 12%] [G loss: 1.480335]\n",
      "[Epoch 0/200] [Batch 62/938] [D loss: 1.499703, acc: 8%] [G loss: 1.488932]\n",
      "[Epoch 0/200] [Batch 63/938] [D loss: 1.495461, acc: 17%] [G loss: 1.493324]\n",
      "[Epoch 0/200] [Batch 64/938] [D loss: 1.495428, acc: 21%] [G loss: 1.497358]\n",
      "[Epoch 0/200] [Batch 65/938] [D loss: 1.492744, acc: 12%] [G loss: 1.500951]\n",
      "[Epoch 0/200] [Batch 66/938] [D loss: 1.495738, acc: 12%] [G loss: 1.505422]\n",
      "[Epoch 0/200] [Batch 67/938] [D loss: 1.494148, acc: 8%] [G loss: 1.502214]\n",
      "[Epoch 0/200] [Batch 68/938] [D loss: 1.494621, acc: 16%] [G loss: 1.502160]\n",
      "[Epoch 0/200] [Batch 69/938] [D loss: 1.496797, acc: 11%] [G loss: 1.499979]\n",
      "[Epoch 0/200] [Batch 70/938] [D loss: 1.497494, acc: 14%] [G loss: 1.493616]\n",
      "[Epoch 0/200] [Batch 71/938] [D loss: 1.497514, acc: 11%] [G loss: 1.491504]\n",
      "[Epoch 0/200] [Batch 72/938] [D loss: 1.499398, acc: 13%] [G loss: 1.489263]\n",
      "[Epoch 0/200] [Batch 73/938] [D loss: 1.500482, acc: 14%] [G loss: 1.487195]\n",
      "[Epoch 0/200] [Batch 74/938] [D loss: 1.497888, acc: 8%] [G loss: 1.489090]\n",
      "[Epoch 0/200] [Batch 75/938] [D loss: 1.497120, acc: 10%] [G loss: 1.489134]\n",
      "[Epoch 0/200] [Batch 76/938] [D loss: 1.495129, acc: 15%] [G loss: 1.486071]\n",
      "[Epoch 0/200] [Batch 77/938] [D loss: 1.495064, acc: 12%] [G loss: 1.481879]\n",
      "[Epoch 0/200] [Batch 78/938] [D loss: 1.494927, acc: 14%] [G loss: 1.477675]\n",
      "[Epoch 0/200] [Batch 79/938] [D loss: 1.499266, acc: 16%] [G loss: 1.477369]\n",
      "[Epoch 0/200] [Batch 80/938] [D loss: 1.497492, acc: 15%] [G loss: 1.476472]\n",
      "[Epoch 0/200] [Batch 81/938] [D loss: 1.502064, acc: 10%] [G loss: 1.477629]\n",
      "[Epoch 0/200] [Batch 82/938] [D loss: 1.500763, acc: 8%] [G loss: 1.478384]\n",
      "[Epoch 0/200] [Batch 83/938] [D loss: 1.500109, acc: 16%] [G loss: 1.485949]\n",
      "[Epoch 0/200] [Batch 84/938] [D loss: 1.499388, acc: 13%] [G loss: 1.496335]\n",
      "[Epoch 0/200] [Batch 85/938] [D loss: 1.499898, acc: 16%] [G loss: 1.493683]\n",
      "[Epoch 0/200] [Batch 86/938] [D loss: 1.501034, acc: 12%] [G loss: 1.501153]\n",
      "[Epoch 0/200] [Batch 87/938] [D loss: 1.499714, acc: 9%] [G loss: 1.504772]\n",
      "[Epoch 0/200] [Batch 88/938] [D loss: 1.498294, acc: 13%] [G loss: 1.507276]\n",
      "[Epoch 0/200] [Batch 89/938] [D loss: 1.497767, acc: 15%] [G loss: 1.510489]\n",
      "[Epoch 0/200] [Batch 90/938] [D loss: 1.497143, acc: 16%] [G loss: 1.512731]\n",
      "[Epoch 0/200] [Batch 91/938] [D loss: 1.496794, acc: 9%] [G loss: 1.514845]\n",
      "[Epoch 0/200] [Batch 92/938] [D loss: 1.496074, acc: 12%] [G loss: 1.513361]\n",
      "[Epoch 0/200] [Batch 93/938] [D loss: 1.497470, acc: 8%] [G loss: 1.511958]\n",
      "[Epoch 0/200] [Batch 94/938] [D loss: 1.495692, acc: 14%] [G loss: 1.508335]\n",
      "[Epoch 0/200] [Batch 95/938] [D loss: 1.496246, acc: 7%] [G loss: 1.508215]\n",
      "[Epoch 0/200] [Batch 96/938] [D loss: 1.499204, acc: 21%] [G loss: 1.508504]\n",
      "[Epoch 0/200] [Batch 97/938] [D loss: 1.499281, acc: 10%] [G loss: 1.504015]\n",
      "[Epoch 0/200] [Batch 98/938] [D loss: 1.501228, acc: 16%] [G loss: 1.500978]\n",
      "[Epoch 0/200] [Batch 99/938] [D loss: 1.498691, acc: 11%] [G loss: 1.503560]\n",
      "[Epoch 0/200] [Batch 100/938] [D loss: 1.500731, acc: 10%] [G loss: 1.502378]\n",
      "[Epoch 0/200] [Batch 101/938] [D loss: 1.498585, acc: 11%] [G loss: 1.499166]\n",
      "[Epoch 0/200] [Batch 102/938] [D loss: 1.500375, acc: 17%] [G loss: 1.498804]\n",
      "[Epoch 0/200] [Batch 103/938] [D loss: 1.500207, acc: 10%] [G loss: 1.499899]\n",
      "[Epoch 0/200] [Batch 104/938] [D loss: 1.497587, acc: 14%] [G loss: 1.502647]\n",
      "[Epoch 0/200] [Batch 105/938] [D loss: 1.497868, acc: 19%] [G loss: 1.498525]\n",
      "[Epoch 0/200] [Batch 106/938] [D loss: 1.497457, acc: 18%] [G loss: 1.500724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 107/938] [D loss: 1.496722, acc: 16%] [G loss: 1.495940]\n",
      "[Epoch 0/200] [Batch 108/938] [D loss: 1.498589, acc: 12%] [G loss: 1.493736]\n",
      "[Epoch 0/200] [Batch 109/938] [D loss: 1.496257, acc: 14%] [G loss: 1.494157]\n",
      "[Epoch 0/200] [Batch 110/938] [D loss: 1.497172, acc: 11%] [G loss: 1.492554]\n",
      "[Epoch 0/200] [Batch 111/938] [D loss: 1.497344, acc: 9%] [G loss: 1.491060]\n",
      "[Epoch 0/200] [Batch 112/938] [D loss: 1.502101, acc: 14%] [G loss: 1.487896]\n",
      "[Epoch 0/200] [Batch 113/938] [D loss: 1.499596, acc: 15%] [G loss: 1.489391]\n",
      "[Epoch 0/200] [Batch 114/938] [D loss: 1.500716, acc: 14%] [G loss: 1.493989]\n",
      "[Epoch 0/200] [Batch 115/938] [D loss: 1.499444, acc: 7%] [G loss: 1.495010]\n",
      "[Epoch 0/200] [Batch 116/938] [D loss: 1.500271, acc: 17%] [G loss: 1.499821]\n",
      "[Epoch 0/200] [Batch 117/938] [D loss: 1.498264, acc: 12%] [G loss: 1.502042]\n",
      "[Epoch 0/200] [Batch 118/938] [D loss: 1.498849, acc: 14%] [G loss: 1.503022]\n",
      "[Epoch 0/200] [Batch 119/938] [D loss: 1.498722, acc: 11%] [G loss: 1.503032]\n",
      "[Epoch 0/200] [Batch 120/938] [D loss: 1.497326, acc: 11%] [G loss: 1.504806]\n",
      "[Epoch 0/200] [Batch 121/938] [D loss: 1.496819, acc: 18%] [G loss: 1.507364]\n",
      "[Epoch 0/200] [Batch 122/938] [D loss: 1.497161, acc: 14%] [G loss: 1.509224]\n",
      "[Epoch 0/200] [Batch 123/938] [D loss: 1.496440, acc: 10%] [G loss: 1.508315]\n",
      "[Epoch 0/200] [Batch 124/938] [D loss: 1.498205, acc: 16%] [G loss: 1.505193]\n",
      "[Epoch 0/200] [Batch 125/938] [D loss: 1.496210, acc: 18%] [G loss: 1.508183]\n",
      "[Epoch 0/200] [Batch 126/938] [D loss: 1.496613, acc: 16%] [G loss: 1.508410]\n",
      "[Epoch 0/200] [Batch 127/938] [D loss: 1.496194, acc: 10%] [G loss: 1.505586]\n",
      "[Epoch 0/200] [Batch 128/938] [D loss: 1.498117, acc: 17%] [G loss: 1.507712]\n",
      "[Epoch 0/200] [Batch 129/938] [D loss: 1.496359, acc: 15%] [G loss: 1.504525]\n",
      "[Epoch 0/200] [Batch 130/938] [D loss: 1.497535, acc: 14%] [G loss: 1.505404]\n",
      "[Epoch 0/200] [Batch 131/938] [D loss: 1.499762, acc: 9%] [G loss: 1.505184]\n",
      "[Epoch 0/200] [Batch 132/938] [D loss: 1.496217, acc: 12%] [G loss: 1.502243]\n",
      "[Epoch 0/200] [Batch 133/938] [D loss: 1.498518, acc: 18%] [G loss: 1.501283]\n",
      "[Epoch 0/200] [Batch 134/938] [D loss: 1.498130, acc: 19%] [G loss: 1.503335]\n",
      "[Epoch 0/200] [Batch 135/938] [D loss: 1.497694, acc: 14%] [G loss: 1.500459]\n",
      "[Epoch 0/200] [Batch 136/938] [D loss: 1.499361, acc: 10%] [G loss: 1.501324]\n",
      "[Epoch 0/200] [Batch 137/938] [D loss: 1.498436, acc: 14%] [G loss: 1.498888]\n",
      "[Epoch 0/200] [Batch 138/938] [D loss: 1.499013, acc: 14%] [G loss: 1.499207]\n",
      "[Epoch 0/200] [Batch 139/938] [D loss: 1.496578, acc: 7%] [G loss: 1.499008]\n",
      "[Epoch 0/200] [Batch 140/938] [D loss: 1.496944, acc: 17%] [G loss: 1.498654]\n",
      "[Epoch 0/200] [Batch 141/938] [D loss: 1.499062, acc: 11%] [G loss: 1.497434]\n",
      "[Epoch 0/200] [Batch 142/938] [D loss: 1.498074, acc: 14%] [G loss: 1.497915]\n",
      "[Epoch 0/200] [Batch 143/938] [D loss: 1.497409, acc: 13%] [G loss: 1.496851]\n",
      "[Epoch 0/200] [Batch 144/938] [D loss: 1.496367, acc: 14%] [G loss: 1.497494]\n",
      "[Epoch 0/200] [Batch 145/938] [D loss: 1.496705, acc: 14%] [G loss: 1.492533]\n",
      "[Epoch 0/200] [Batch 146/938] [D loss: 1.496588, acc: 9%] [G loss: 1.494081]\n",
      "[Epoch 0/200] [Batch 147/938] [D loss: 1.496472, acc: 13%] [G loss: 1.492692]\n",
      "[Epoch 0/200] [Batch 148/938] [D loss: 1.498185, acc: 10%] [G loss: 1.491946]\n",
      "[Epoch 0/200] [Batch 149/938] [D loss: 1.497154, acc: 12%] [G loss: 1.494795]\n",
      "[Epoch 0/200] [Batch 150/938] [D loss: 1.499384, acc: 10%] [G loss: 1.489078]\n",
      "[Epoch 0/200] [Batch 151/938] [D loss: 1.499365, acc: 13%] [G loss: 1.492229]\n",
      "[Epoch 0/200] [Batch 152/938] [D loss: 1.499139, acc: 16%] [G loss: 1.494699]\n",
      "[Epoch 0/200] [Batch 153/938] [D loss: 1.499792, acc: 12%] [G loss: 1.495345]\n",
      "[Epoch 0/200] [Batch 154/938] [D loss: 1.498440, acc: 14%] [G loss: 1.497942]\n",
      "[Epoch 0/200] [Batch 155/938] [D loss: 1.497693, acc: 13%] [G loss: 1.500508]\n",
      "[Epoch 0/200] [Batch 156/938] [D loss: 1.497607, acc: 16%] [G loss: 1.500733]\n",
      "[Epoch 0/200] [Batch 157/938] [D loss: 1.496954, acc: 15%] [G loss: 1.502075]\n",
      "[Epoch 0/200] [Batch 158/938] [D loss: 1.497724, acc: 14%] [G loss: 1.501426]\n",
      "[Epoch 0/200] [Batch 159/938] [D loss: 1.498086, acc: 12%] [G loss: 1.503497]\n",
      "[Epoch 0/200] [Batch 160/938] [D loss: 1.497250, acc: 10%] [G loss: 1.502404]\n",
      "[Epoch 0/200] [Batch 161/938] [D loss: 1.495570, acc: 14%] [G loss: 1.505371]\n",
      "[Epoch 0/200] [Batch 162/938] [D loss: 1.496231, acc: 16%] [G loss: 1.502494]\n",
      "[Epoch 0/200] [Batch 163/938] [D loss: 1.497650, acc: 21%] [G loss: 1.503973]\n",
      "[Epoch 0/200] [Batch 164/938] [D loss: 1.496269, acc: 9%] [G loss: 1.504559]\n",
      "[Epoch 0/200] [Batch 165/938] [D loss: 1.497153, acc: 13%] [G loss: 1.504257]\n",
      "[Epoch 0/200] [Batch 166/938] [D loss: 1.496784, acc: 13%] [G loss: 1.504030]\n",
      "[Epoch 0/200] [Batch 167/938] [D loss: 1.497670, acc: 8%] [G loss: 1.502416]\n",
      "[Epoch 0/200] [Batch 168/938] [D loss: 1.498338, acc: 13%] [G loss: 1.502528]\n",
      "[Epoch 0/200] [Batch 169/938] [D loss: 1.497978, acc: 11%] [G loss: 1.503621]\n",
      "[Epoch 0/200] [Batch 170/938] [D loss: 1.496655, acc: 16%] [G loss: 1.500450]\n",
      "[Epoch 0/200] [Batch 171/938] [D loss: 1.498323, acc: 16%] [G loss: 1.500542]\n",
      "[Epoch 0/200] [Batch 172/938] [D loss: 1.497638, acc: 18%] [G loss: 1.500028]\n",
      "[Epoch 0/200] [Batch 173/938] [D loss: 1.497645, acc: 17%] [G loss: 1.498852]\n",
      "[Epoch 0/200] [Batch 174/938] [D loss: 1.495281, acc: 10%] [G loss: 1.497219]\n",
      "[Epoch 0/200] [Batch 175/938] [D loss: 1.496516, acc: 6%] [G loss: 1.495550]\n",
      "[Epoch 0/200] [Batch 176/938] [D loss: 1.496387, acc: 17%] [G loss: 1.495605]\n",
      "[Epoch 0/200] [Batch 177/938] [D loss: 1.496542, acc: 12%] [G loss: 1.493168]\n",
      "[Epoch 0/200] [Batch 178/938] [D loss: 1.495846, acc: 16%] [G loss: 1.496494]\n",
      "[Epoch 0/200] [Batch 179/938] [D loss: 1.496619, acc: 11%] [G loss: 1.490435]\n",
      "[Epoch 0/200] [Batch 180/938] [D loss: 1.497318, acc: 15%] [G loss: 1.488590]\n",
      "[Epoch 0/200] [Batch 181/938] [D loss: 1.496249, acc: 12%] [G loss: 1.488582]\n",
      "[Epoch 0/200] [Batch 182/938] [D loss: 1.496865, acc: 12%] [G loss: 1.487283]\n",
      "[Epoch 0/200] [Batch 183/938] [D loss: 1.497529, acc: 14%] [G loss: 1.486647]\n",
      "[Epoch 0/200] [Batch 184/938] [D loss: 1.498781, acc: 11%] [G loss: 1.487670]\n",
      "[Epoch 0/200] [Batch 185/938] [D loss: 1.496144, acc: 14%] [G loss: 1.488969]\n",
      "[Epoch 0/200] [Batch 186/938] [D loss: 1.499352, acc: 15%] [G loss: 1.487527]\n",
      "[Epoch 0/200] [Batch 187/938] [D loss: 1.497006, acc: 9%] [G loss: 1.493528]\n",
      "[Epoch 0/200] [Batch 188/938] [D loss: 1.500003, acc: 12%] [G loss: 1.491741]\n",
      "[Epoch 0/200] [Batch 189/938] [D loss: 1.498948, acc: 10%] [G loss: 1.498465]\n",
      "[Epoch 0/200] [Batch 190/938] [D loss: 1.496392, acc: 17%] [G loss: 1.499079]\n",
      "[Epoch 0/200] [Batch 191/938] [D loss: 1.497055, acc: 10%] [G loss: 1.502768]\n",
      "[Epoch 0/200] [Batch 192/938] [D loss: 1.496182, acc: 19%] [G loss: 1.503215]\n",
      "[Epoch 0/200] [Batch 193/938] [D loss: 1.496663, acc: 16%] [G loss: 1.501391]\n",
      "[Epoch 0/200] [Batch 194/938] [D loss: 1.496866, acc: 12%] [G loss: 1.504550]\n",
      "[Epoch 0/200] [Batch 195/938] [D loss: 1.495024, acc: 11%] [G loss: 1.503068]\n",
      "[Epoch 0/200] [Batch 196/938] [D loss: 1.495898, acc: 20%] [G loss: 1.500110]\n",
      "[Epoch 0/200] [Batch 197/938] [D loss: 1.498394, acc: 10%] [G loss: 1.501233]\n",
      "[Epoch 0/200] [Batch 198/938] [D loss: 1.496542, acc: 21%] [G loss: 1.499433]\n",
      "[Epoch 0/200] [Batch 199/938] [D loss: 1.499070, acc: 16%] [G loss: 1.498557]\n",
      "[Epoch 0/200] [Batch 200/938] [D loss: 1.497612, acc: 12%] [G loss: 1.497816]\n",
      "[Epoch 0/200] [Batch 201/938] [D loss: 1.497911, acc: 16%] [G loss: 1.497586]\n",
      "[Epoch 0/200] [Batch 202/938] [D loss: 1.497486, acc: 13%] [G loss: 1.499795]\n",
      "[Epoch 0/200] [Batch 203/938] [D loss: 1.498740, acc: 12%] [G loss: 1.499418]\n",
      "[Epoch 0/200] [Batch 204/938] [D loss: 1.498741, acc: 14%] [G loss: 1.496874]\n",
      "[Epoch 0/200] [Batch 205/938] [D loss: 1.495066, acc: 11%] [G loss: 1.496808]\n",
      "[Epoch 0/200] [Batch 206/938] [D loss: 1.494941, acc: 17%] [G loss: 1.495842]\n",
      "[Epoch 0/200] [Batch 207/938] [D loss: 1.494133, acc: 14%] [G loss: 1.495004]\n",
      "[Epoch 0/200] [Batch 208/938] [D loss: 1.490912, acc: 18%] [G loss: 1.495801]\n",
      "[Epoch 0/200] [Batch 209/938] [D loss: 1.490486, acc: 17%] [G loss: 1.492781]\n",
      "[Epoch 0/200] [Batch 210/938] [D loss: 1.489822, acc: 15%] [G loss: 1.487764]\n",
      "[Epoch 0/200] [Batch 211/938] [D loss: 1.493438, acc: 10%] [G loss: 1.481128]\n",
      "[Epoch 0/200] [Batch 212/938] [D loss: 1.497023, acc: 15%] [G loss: 1.470952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 213/938] [D loss: 1.497803, acc: 8%] [G loss: 1.467755]\n",
      "[Epoch 0/200] [Batch 214/938] [D loss: 1.500051, acc: 20%] [G loss: 1.472639]\n",
      "[Epoch 0/200] [Batch 215/938] [D loss: 1.502676, acc: 17%] [G loss: 1.477045]\n",
      "[Epoch 0/200] [Batch 216/938] [D loss: 1.501378, acc: 8%] [G loss: 1.482611]\n",
      "[Epoch 0/200] [Batch 217/938] [D loss: 1.500515, acc: 16%] [G loss: 1.488886]\n",
      "[Epoch 0/200] [Batch 218/938] [D loss: 1.499779, acc: 22%] [G loss: 1.495503]\n",
      "[Epoch 0/200] [Batch 219/938] [D loss: 1.497166, acc: 17%] [G loss: 1.505462]\n",
      "[Epoch 0/200] [Batch 220/938] [D loss: 1.495031, acc: 13%] [G loss: 1.507755]\n",
      "[Epoch 0/200] [Batch 221/938] [D loss: 1.496808, acc: 11%] [G loss: 1.509708]\n",
      "[Epoch 0/200] [Batch 222/938] [D loss: 1.496734, acc: 15%] [G loss: 1.512741]\n",
      "[Epoch 0/200] [Batch 223/938] [D loss: 1.496234, acc: 17%] [G loss: 1.514492]\n",
      "[Epoch 0/200] [Batch 224/938] [D loss: 1.493544, acc: 17%] [G loss: 1.515964]\n",
      "[Epoch 0/200] [Batch 225/938] [D loss: 1.496677, acc: 10%] [G loss: 1.510408]\n",
      "[Epoch 0/200] [Batch 226/938] [D loss: 1.493889, acc: 12%] [G loss: 1.512249]\n",
      "[Epoch 0/200] [Batch 227/938] [D loss: 1.491978, acc: 17%] [G loss: 1.510789]\n",
      "[Epoch 0/200] [Batch 228/938] [D loss: 1.494203, acc: 12%] [G loss: 1.507795]\n",
      "[Epoch 0/200] [Batch 229/938] [D loss: 1.495544, acc: 17%] [G loss: 1.504174]\n",
      "[Epoch 0/200] [Batch 230/938] [D loss: 1.499246, acc: 7%] [G loss: 1.498645]\n",
      "[Epoch 0/200] [Batch 231/938] [D loss: 1.495596, acc: 14%] [G loss: 1.494634]\n",
      "[Epoch 0/200] [Batch 232/938] [D loss: 1.495222, acc: 14%] [G loss: 1.487593]\n",
      "[Epoch 0/200] [Batch 233/938] [D loss: 1.494874, acc: 15%] [G loss: 1.485447]\n",
      "[Epoch 0/200] [Batch 234/938] [D loss: 1.501602, acc: 11%] [G loss: 1.480512]\n",
      "[Epoch 0/200] [Batch 235/938] [D loss: 1.495543, acc: 9%] [G loss: 1.483921]\n",
      "[Epoch 0/200] [Batch 236/938] [D loss: 1.491542, acc: 16%] [G loss: 1.484291]\n",
      "[Epoch 0/200] [Batch 237/938] [D loss: 1.490695, acc: 15%] [G loss: 1.483368]\n",
      "[Epoch 0/200] [Batch 238/938] [D loss: 1.486300, acc: 14%] [G loss: 1.485884]\n",
      "[Epoch 0/200] [Batch 239/938] [D loss: 1.485447, acc: 19%] [G loss: 1.483205]\n",
      "[Epoch 0/200] [Batch 240/938] [D loss: 1.485859, acc: 20%] [G loss: 1.470331]\n",
      "[Epoch 0/200] [Batch 241/938] [D loss: 1.490676, acc: 11%] [G loss: 1.460747]\n",
      "[Epoch 0/200] [Batch 242/938] [D loss: 1.492582, acc: 17%] [G loss: 1.454500]\n",
      "[Epoch 0/200] [Batch 243/938] [D loss: 1.498134, acc: 10%] [G loss: 1.454905]\n",
      "[Epoch 0/200] [Batch 244/938] [D loss: 1.496561, acc: 17%] [G loss: 1.450997]\n",
      "[Epoch 0/200] [Batch 245/938] [D loss: 1.503659, acc: 14%] [G loss: 1.456604]\n",
      "[Epoch 0/200] [Batch 246/938] [D loss: 1.495881, acc: 15%] [G loss: 1.464589]\n",
      "[Epoch 0/200] [Batch 247/938] [D loss: 1.499465, acc: 16%] [G loss: 1.470058]\n",
      "[Epoch 0/200] [Batch 248/938] [D loss: 1.497678, acc: 15%] [G loss: 1.486306]\n",
      "[Epoch 0/200] [Batch 249/938] [D loss: 1.498114, acc: 17%] [G loss: 1.489308]\n",
      "[Epoch 0/200] [Batch 250/938] [D loss: 1.497607, acc: 12%] [G loss: 1.495505]\n",
      "[Epoch 0/200] [Batch 251/938] [D loss: 1.497262, acc: 17%] [G loss: 1.501035]\n",
      "[Epoch 0/200] [Batch 252/938] [D loss: 1.491674, acc: 20%] [G loss: 1.501559]\n",
      "[Epoch 0/200] [Batch 253/938] [D loss: 1.493389, acc: 15%] [G loss: 1.510224]\n",
      "[Epoch 0/200] [Batch 254/938] [D loss: 1.495188, acc: 11%] [G loss: 1.520891]\n",
      "[Epoch 0/200] [Batch 255/938] [D loss: 1.491971, acc: 17%] [G loss: 1.517181]\n",
      "[Epoch 0/200] [Batch 256/938] [D loss: 1.492636, acc: 15%] [G loss: 1.528857]\n",
      "[Epoch 0/200] [Batch 257/938] [D loss: 1.488217, acc: 20%] [G loss: 1.528832]\n",
      "[Epoch 0/200] [Batch 258/938] [D loss: 1.488687, acc: 14%] [G loss: 1.528383]\n",
      "[Epoch 0/200] [Batch 259/938] [D loss: 1.488351, acc: 17%] [G loss: 1.526844]\n",
      "[Epoch 0/200] [Batch 260/938] [D loss: 1.486524, acc: 12%] [G loss: 1.526506]\n",
      "[Epoch 0/200] [Batch 261/938] [D loss: 1.490051, acc: 17%] [G loss: 1.524136]\n",
      "[Epoch 0/200] [Batch 262/938] [D loss: 1.487514, acc: 15%] [G loss: 1.518075]\n",
      "[Epoch 0/200] [Batch 263/938] [D loss: 1.491107, acc: 15%] [G loss: 1.512496]\n",
      "[Epoch 0/200] [Batch 264/938] [D loss: 1.489658, acc: 19%] [G loss: 1.505518]\n",
      "[Epoch 0/200] [Batch 265/938] [D loss: 1.493466, acc: 19%] [G loss: 1.481271]\n",
      "[Epoch 0/200] [Batch 266/938] [D loss: 1.497681, acc: 19%] [G loss: 1.483842]\n",
      "[Epoch 0/200] [Batch 267/938] [D loss: 1.497188, acc: 22%] [G loss: 1.474427]\n",
      "[Epoch 0/200] [Batch 268/938] [D loss: 1.497482, acc: 17%] [G loss: 1.468610]\n",
      "[Epoch 0/200] [Batch 269/938] [D loss: 1.484900, acc: 21%] [G loss: 1.484181]\n",
      "[Epoch 0/200] [Batch 270/938] [D loss: 1.476613, acc: 24%] [G loss: 1.485697]\n",
      "[Epoch 0/200] [Batch 271/938] [D loss: 1.473881, acc: 25%] [G loss: 1.478477]\n",
      "[Epoch 0/200] [Batch 272/938] [D loss: 1.470926, acc: 19%] [G loss: 1.465622]\n",
      "[Epoch 0/200] [Batch 273/938] [D loss: 1.485237, acc: 10%] [G loss: 1.450341]\n",
      "[Epoch 0/200] [Batch 274/938] [D loss: 1.483992, acc: 19%] [G loss: 1.440410]\n",
      "[Epoch 0/200] [Batch 275/938] [D loss: 1.494863, acc: 21%] [G loss: 1.443698]\n",
      "[Epoch 0/200] [Batch 276/938] [D loss: 1.493058, acc: 25%] [G loss: 1.441985]\n",
      "[Epoch 0/200] [Batch 277/938] [D loss: 1.478826, acc: 25%] [G loss: 1.428347]\n",
      "[Epoch 0/200] [Batch 278/938] [D loss: 1.494643, acc: 21%] [G loss: 1.452331]\n",
      "[Epoch 0/200] [Batch 279/938] [D loss: 1.489780, acc: 16%] [G loss: 1.470960]\n",
      "[Epoch 0/200] [Batch 280/938] [D loss: 1.485460, acc: 14%] [G loss: 1.483097]\n",
      "[Epoch 0/200] [Batch 281/938] [D loss: 1.496114, acc: 14%] [G loss: 1.485243]\n",
      "[Epoch 0/200] [Batch 282/938] [D loss: 1.488472, acc: 19%] [G loss: 1.495039]\n",
      "[Epoch 0/200] [Batch 283/938] [D loss: 1.487817, acc: 17%] [G loss: 1.498533]\n",
      "[Epoch 0/200] [Batch 284/938] [D loss: 1.485334, acc: 21%] [G loss: 1.509056]\n",
      "[Epoch 0/200] [Batch 285/938] [D loss: 1.483093, acc: 22%] [G loss: 1.511959]\n",
      "[Epoch 0/200] [Batch 286/938] [D loss: 1.478032, acc: 28%] [G loss: 1.516763]\n",
      "[Epoch 0/200] [Batch 287/938] [D loss: 1.478359, acc: 19%] [G loss: 1.528167]\n",
      "[Epoch 0/200] [Batch 288/938] [D loss: 1.476344, acc: 17%] [G loss: 1.522986]\n",
      "[Epoch 0/200] [Batch 289/938] [D loss: 1.480119, acc: 22%] [G loss: 1.523742]\n",
      "[Epoch 0/200] [Batch 290/938] [D loss: 1.471542, acc: 16%] [G loss: 1.526101]\n",
      "[Epoch 0/200] [Batch 291/938] [D loss: 1.466660, acc: 22%] [G loss: 1.506129]\n",
      "[Epoch 0/200] [Batch 292/938] [D loss: 1.470277, acc: 25%] [G loss: 1.497379]\n",
      "[Epoch 0/200] [Batch 293/938] [D loss: 1.473752, acc: 19%] [G loss: 1.495719]\n",
      "[Epoch 0/200] [Batch 294/938] [D loss: 1.467998, acc: 20%] [G loss: 1.472665]\n",
      "[Epoch 0/200] [Batch 295/938] [D loss: 1.463773, acc: 26%] [G loss: 1.463496]\n",
      "[Epoch 0/200] [Batch 296/938] [D loss: 1.493355, acc: 18%] [G loss: 1.443841]\n",
      "[Epoch 0/200] [Batch 297/938] [D loss: 1.465714, acc: 24%] [G loss: 1.439263]\n",
      "[Epoch 0/200] [Batch 298/938] [D loss: 1.470490, acc: 28%] [G loss: 1.422724]\n",
      "[Epoch 0/200] [Batch 299/938] [D loss: 1.484021, acc: 24%] [G loss: 1.410627]\n",
      "[Epoch 0/200] [Batch 300/938] [D loss: 1.490586, acc: 18%] [G loss: 1.427755]\n",
      "[Epoch 0/200] [Batch 301/938] [D loss: 1.474908, acc: 20%] [G loss: 1.443187]\n",
      "[Epoch 0/200] [Batch 302/938] [D loss: 1.466306, acc: 25%] [G loss: 1.463177]\n",
      "[Epoch 0/200] [Batch 303/938] [D loss: 1.469521, acc: 27%] [G loss: 1.450663]\n",
      "[Epoch 0/200] [Batch 304/938] [D loss: 1.471897, acc: 21%] [G loss: 1.476608]\n",
      "[Epoch 0/200] [Batch 305/938] [D loss: 1.455678, acc: 24%] [G loss: 1.458561]\n",
      "[Epoch 0/200] [Batch 306/938] [D loss: 1.469460, acc: 18%] [G loss: 1.490784]\n",
      "[Epoch 0/200] [Batch 307/938] [D loss: 1.457127, acc: 23%] [G loss: 1.497044]\n",
      "[Epoch 0/200] [Batch 308/938] [D loss: 1.444966, acc: 29%] [G loss: 1.499555]\n",
      "[Epoch 0/200] [Batch 309/938] [D loss: 1.435194, acc: 32%] [G loss: 1.510092]\n",
      "[Epoch 0/200] [Batch 310/938] [D loss: 1.435354, acc: 30%] [G loss: 1.508671]\n",
      "[Epoch 0/200] [Batch 311/938] [D loss: 1.429595, acc: 35%] [G loss: 1.511423]\n",
      "[Epoch 0/200] [Batch 312/938] [D loss: 1.462547, acc: 25%] [G loss: 1.497355]\n",
      "[Epoch 0/200] [Batch 313/938] [D loss: 1.448681, acc: 34%] [G loss: 1.466415]\n",
      "[Epoch 0/200] [Batch 314/938] [D loss: 1.436364, acc: 29%] [G loss: 1.497925]\n",
      "[Epoch 0/200] [Batch 315/938] [D loss: 1.441736, acc: 35%] [G loss: 1.466296]\n",
      "[Epoch 0/200] [Batch 316/938] [D loss: 1.442909, acc: 26%] [G loss: 1.485328]\n",
      "[Epoch 0/200] [Batch 317/938] [D loss: 1.449108, acc: 28%] [G loss: 1.483532]\n",
      "[Epoch 0/200] [Batch 318/938] [D loss: 1.424969, acc: 36%] [G loss: 1.467162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 319/938] [D loss: 1.412509, acc: 36%] [G loss: 1.463940]\n",
      "[Epoch 0/200] [Batch 320/938] [D loss: 1.420415, acc: 32%] [G loss: 1.470851]\n",
      "[Epoch 0/200] [Batch 321/938] [D loss: 1.436490, acc: 32%] [G loss: 1.453702]\n",
      "[Epoch 0/200] [Batch 322/938] [D loss: 1.432673, acc: 28%] [G loss: 1.471565]\n",
      "[Epoch 0/200] [Batch 323/938] [D loss: 1.427586, acc: 32%] [G loss: 1.484459]\n",
      "[Epoch 0/200] [Batch 324/938] [D loss: 1.429157, acc: 32%] [G loss: 1.478375]\n",
      "[Epoch 0/200] [Batch 325/938] [D loss: 1.417560, acc: 35%] [G loss: 1.453699]\n",
      "[Epoch 0/200] [Batch 326/938] [D loss: 1.415200, acc: 36%] [G loss: 1.481020]\n",
      "[Epoch 0/200] [Batch 327/938] [D loss: 1.424265, acc: 39%] [G loss: 1.457232]\n",
      "[Epoch 0/200] [Batch 328/938] [D loss: 1.416892, acc: 35%] [G loss: 1.477532]\n",
      "[Epoch 0/200] [Batch 329/938] [D loss: 1.398156, acc: 39%] [G loss: 1.490505]\n",
      "[Epoch 0/200] [Batch 330/938] [D loss: 1.414720, acc: 39%] [G loss: 1.481516]\n",
      "[Epoch 0/200] [Batch 331/938] [D loss: 1.378603, acc: 46%] [G loss: 1.477452]\n",
      "[Epoch 0/200] [Batch 332/938] [D loss: 1.393922, acc: 38%] [G loss: 1.449520]\n",
      "[Epoch 0/200] [Batch 333/938] [D loss: 1.407082, acc: 35%] [G loss: 1.479735]\n",
      "[Epoch 0/200] [Batch 334/938] [D loss: 1.396839, acc: 39%] [G loss: 1.499787]\n",
      "[Epoch 0/200] [Batch 335/938] [D loss: 1.390156, acc: 37%] [G loss: 1.475025]\n",
      "[Epoch 0/200] [Batch 336/938] [D loss: 1.396062, acc: 39%] [G loss: 1.440843]\n",
      "[Epoch 0/200] [Batch 337/938] [D loss: 1.419574, acc: 32%] [G loss: 1.464045]\n",
      "[Epoch 0/200] [Batch 338/938] [D loss: 1.401238, acc: 32%] [G loss: 1.461512]\n",
      "[Epoch 0/200] [Batch 339/938] [D loss: 1.391791, acc: 34%] [G loss: 1.450833]\n",
      "[Epoch 0/200] [Batch 340/938] [D loss: 1.375788, acc: 42%] [G loss: 1.423327]\n",
      "[Epoch 0/200] [Batch 341/938] [D loss: 1.399381, acc: 39%] [G loss: 1.404457]\n",
      "[Epoch 0/200] [Batch 342/938] [D loss: 1.406231, acc: 33%] [G loss: 1.404290]\n",
      "[Epoch 0/200] [Batch 343/938] [D loss: 1.421059, acc: 40%] [G loss: 1.425467]\n",
      "[Epoch 0/200] [Batch 344/938] [D loss: 1.424597, acc: 38%] [G loss: 1.483104]\n",
      "[Epoch 0/200] [Batch 345/938] [D loss: 1.415074, acc: 39%] [G loss: 1.451293]\n",
      "[Epoch 0/200] [Batch 346/938] [D loss: 1.392064, acc: 45%] [G loss: 1.459862]\n",
      "[Epoch 0/200] [Batch 347/938] [D loss: 1.402926, acc: 37%] [G loss: 1.485335]\n",
      "[Epoch 0/200] [Batch 348/938] [D loss: 1.398515, acc: 32%] [G loss: 1.506532]\n",
      "[Epoch 0/200] [Batch 349/938] [D loss: 1.392602, acc: 36%] [G loss: 1.481073]\n",
      "[Epoch 0/200] [Batch 350/938] [D loss: 1.376348, acc: 40%] [G loss: 1.444237]\n",
      "[Epoch 0/200] [Batch 351/938] [D loss: 1.388405, acc: 35%] [G loss: 1.486547]\n",
      "[Epoch 0/200] [Batch 352/938] [D loss: 1.390150, acc: 30%] [G loss: 1.509684]\n",
      "[Epoch 0/200] [Batch 353/938] [D loss: 1.366269, acc: 39%] [G loss: 1.488169]\n",
      "[Epoch 0/200] [Batch 354/938] [D loss: 1.374279, acc: 42%] [G loss: 1.497389]\n",
      "[Epoch 0/200] [Batch 355/938] [D loss: 1.343558, acc: 51%] [G loss: 1.439581]\n",
      "[Epoch 0/200] [Batch 356/938] [D loss: 1.396530, acc: 34%] [G loss: 1.484295]\n",
      "[Epoch 0/200] [Batch 357/938] [D loss: 1.391288, acc: 40%] [G loss: 1.448556]\n",
      "[Epoch 0/200] [Batch 358/938] [D loss: 1.382076, acc: 40%] [G loss: 1.421404]\n",
      "[Epoch 0/200] [Batch 359/938] [D loss: 1.369895, acc: 44%] [G loss: 1.441187]\n",
      "[Epoch 0/200] [Batch 360/938] [D loss: 1.371601, acc: 44%] [G loss: 1.459535]\n",
      "[Epoch 0/200] [Batch 361/938] [D loss: 1.364908, acc: 46%] [G loss: 1.384813]\n",
      "[Epoch 0/200] [Batch 362/938] [D loss: 1.345027, acc: 52%] [G loss: 1.416952]\n",
      "[Epoch 0/200] [Batch 363/938] [D loss: 1.391982, acc: 43%] [G loss: 1.437762]\n",
      "[Epoch 0/200] [Batch 364/938] [D loss: 1.374394, acc: 41%] [G loss: 1.427390]\n",
      "[Epoch 0/200] [Batch 365/938] [D loss: 1.375821, acc: 40%] [G loss: 1.441577]\n",
      "[Epoch 0/200] [Batch 366/938] [D loss: 1.352675, acc: 48%] [G loss: 1.408193]\n",
      "[Epoch 0/200] [Batch 367/938] [D loss: 1.383373, acc: 36%] [G loss: 1.452967]\n",
      "[Epoch 0/200] [Batch 368/938] [D loss: 1.388354, acc: 39%] [G loss: 1.491198]\n",
      "[Epoch 0/200] [Batch 369/938] [D loss: 1.375234, acc: 45%] [G loss: 1.456805]\n",
      "[Epoch 0/200] [Batch 370/938] [D loss: 1.353605, acc: 46%] [G loss: 1.438186]\n",
      "[Epoch 0/200] [Batch 371/938] [D loss: 1.361014, acc: 43%] [G loss: 1.504713]\n",
      "[Epoch 0/200] [Batch 372/938] [D loss: 1.357538, acc: 45%] [G loss: 1.493515]\n",
      "[Epoch 0/200] [Batch 373/938] [D loss: 1.355070, acc: 49%] [G loss: 1.483350]\n",
      "[Epoch 0/200] [Batch 374/938] [D loss: 1.370534, acc: 42%] [G loss: 1.477278]\n",
      "[Epoch 0/200] [Batch 375/938] [D loss: 1.356593, acc: 40%] [G loss: 1.475248]\n",
      "[Epoch 0/200] [Batch 376/938] [D loss: 1.346791, acc: 50%] [G loss: 1.413020]\n",
      "[Epoch 0/200] [Batch 377/938] [D loss: 1.345636, acc: 45%] [G loss: 1.447564]\n",
      "[Epoch 0/200] [Batch 378/938] [D loss: 1.349530, acc: 46%] [G loss: 1.464486]\n",
      "[Epoch 0/200] [Batch 379/938] [D loss: 1.370210, acc: 44%] [G loss: 1.456743]\n",
      "[Epoch 0/200] [Batch 380/938] [D loss: 1.352621, acc: 46%] [G loss: 1.413107]\n",
      "[Epoch 0/200] [Batch 381/938] [D loss: 1.358157, acc: 44%] [G loss: 1.472928]\n",
      "[Epoch 0/200] [Batch 382/938] [D loss: 1.351859, acc: 49%] [G loss: 1.469080]\n",
      "[Epoch 0/200] [Batch 383/938] [D loss: 1.341361, acc: 50%] [G loss: 1.434050]\n",
      "[Epoch 0/200] [Batch 384/938] [D loss: 1.327535, acc: 53%] [G loss: 1.465329]\n",
      "[Epoch 0/200] [Batch 385/938] [D loss: 1.363890, acc: 46%] [G loss: 1.435178]\n",
      "[Epoch 0/200] [Batch 386/938] [D loss: 1.377698, acc: 37%] [G loss: 1.444460]\n",
      "[Epoch 0/200] [Batch 387/938] [D loss: 1.322047, acc: 50%] [G loss: 1.432188]\n",
      "[Epoch 0/200] [Batch 388/938] [D loss: 1.353348, acc: 48%] [G loss: 1.439491]\n",
      "[Epoch 0/200] [Batch 389/938] [D loss: 1.346679, acc: 51%] [G loss: 1.446440]\n",
      "[Epoch 0/200] [Batch 390/938] [D loss: 1.344320, acc: 50%] [G loss: 1.434819]\n",
      "[Epoch 0/200] [Batch 391/938] [D loss: 1.332003, acc: 53%] [G loss: 1.434493]\n",
      "[Epoch 0/200] [Batch 392/938] [D loss: 1.374492, acc: 42%] [G loss: 1.442287]\n",
      "[Epoch 0/200] [Batch 393/938] [D loss: 1.389357, acc: 37%] [G loss: 1.459188]\n",
      "[Epoch 0/200] [Batch 394/938] [D loss: 1.344921, acc: 49%] [G loss: 1.440391]\n",
      "[Epoch 0/200] [Batch 395/938] [D loss: 1.338957, acc: 47%] [G loss: 1.444627]\n",
      "[Epoch 0/200] [Batch 396/938] [D loss: 1.353936, acc: 46%] [G loss: 1.441969]\n",
      "[Epoch 0/200] [Batch 397/938] [D loss: 1.322294, acc: 55%] [G loss: 1.432544]\n",
      "[Epoch 0/200] [Batch 398/938] [D loss: 1.343444, acc: 48%] [G loss: 1.408839]\n",
      "[Epoch 0/200] [Batch 399/938] [D loss: 1.302834, acc: 57%] [G loss: 1.420504]\n",
      "[Epoch 0/200] [Batch 400/938] [D loss: 1.345029, acc: 48%] [G loss: 1.484220]\n",
      "[Epoch 0/200] [Batch 401/938] [D loss: 1.344963, acc: 46%] [G loss: 1.468969]\n",
      "[Epoch 0/200] [Batch 402/938] [D loss: 1.318784, acc: 53%] [G loss: 1.443230]\n",
      "[Epoch 0/200] [Batch 403/938] [D loss: 1.337755, acc: 53%] [G loss: 1.442621]\n",
      "[Epoch 0/200] [Batch 404/938] [D loss: 1.344310, acc: 50%] [G loss: 1.449536]\n",
      "[Epoch 0/200] [Batch 405/938] [D loss: 1.307946, acc: 57%] [G loss: 1.364292]\n",
      "[Epoch 0/200] [Batch 406/938] [D loss: 1.316359, acc: 53%] [G loss: 1.412763]\n",
      "[Epoch 0/200] [Batch 407/938] [D loss: 1.346810, acc: 46%] [G loss: 1.463821]\n",
      "[Epoch 0/200] [Batch 408/938] [D loss: 1.309501, acc: 57%] [G loss: 1.421237]\n",
      "[Epoch 0/200] [Batch 409/938] [D loss: 1.332108, acc: 50%] [G loss: 1.446621]\n",
      "[Epoch 0/200] [Batch 410/938] [D loss: 1.337565, acc: 52%] [G loss: 1.468657]\n",
      "[Epoch 0/200] [Batch 411/938] [D loss: 1.332163, acc: 50%] [G loss: 1.463975]\n",
      "[Epoch 0/200] [Batch 412/938] [D loss: 1.313053, acc: 53%] [G loss: 1.450041]\n",
      "[Epoch 0/200] [Batch 413/938] [D loss: 1.338414, acc: 50%] [G loss: 1.441729]\n",
      "[Epoch 0/200] [Batch 414/938] [D loss: 1.326739, acc: 51%] [G loss: 1.459233]\n",
      "[Epoch 0/200] [Batch 415/938] [D loss: 1.339910, acc: 52%] [G loss: 1.431139]\n",
      "[Epoch 0/200] [Batch 416/938] [D loss: 1.308962, acc: 57%] [G loss: 1.405530]\n",
      "[Epoch 0/200] [Batch 417/938] [D loss: 1.304388, acc: 57%] [G loss: 1.418192]\n",
      "[Epoch 0/200] [Batch 418/938] [D loss: 1.329831, acc: 46%] [G loss: 1.444664]\n",
      "[Epoch 0/200] [Batch 419/938] [D loss: 1.339454, acc: 50%] [G loss: 1.426492]\n",
      "[Epoch 0/200] [Batch 420/938] [D loss: 1.307007, acc: 54%] [G loss: 1.411418]\n",
      "[Epoch 0/200] [Batch 421/938] [D loss: 1.297727, acc: 57%] [G loss: 1.433133]\n",
      "[Epoch 0/200] [Batch 422/938] [D loss: 1.293741, acc: 56%] [G loss: 1.447672]\n",
      "[Epoch 0/200] [Batch 423/938] [D loss: 1.318676, acc: 52%] [G loss: 1.484450]\n",
      "[Epoch 0/200] [Batch 424/938] [D loss: 1.306534, acc: 57%] [G loss: 1.517655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 425/938] [D loss: 1.302352, acc: 53%] [G loss: 1.468830]\n",
      "[Epoch 0/200] [Batch 426/938] [D loss: 1.350518, acc: 46%] [G loss: 1.498071]\n",
      "[Epoch 0/200] [Batch 427/938] [D loss: 1.345151, acc: 52%] [G loss: 1.408945]\n",
      "[Epoch 0/200] [Batch 428/938] [D loss: 1.310450, acc: 52%] [G loss: 1.430921]\n",
      "[Epoch 0/200] [Batch 429/938] [D loss: 1.283890, acc: 59%] [G loss: 1.425553]\n",
      "[Epoch 0/200] [Batch 430/938] [D loss: 1.308175, acc: 50%] [G loss: 1.442699]\n",
      "[Epoch 0/200] [Batch 431/938] [D loss: 1.309632, acc: 55%] [G loss: 1.373698]\n",
      "[Epoch 0/200] [Batch 432/938] [D loss: 1.297606, acc: 56%] [G loss: 1.422570]\n",
      "[Epoch 0/200] [Batch 433/938] [D loss: 1.289515, acc: 60%] [G loss: 1.369601]\n",
      "[Epoch 0/200] [Batch 434/938] [D loss: 1.318810, acc: 53%] [G loss: 1.412051]\n",
      "[Epoch 0/200] [Batch 435/938] [D loss: 1.292098, acc: 62%] [G loss: 1.408541]\n",
      "[Epoch 0/200] [Batch 436/938] [D loss: 1.272146, acc: 63%] [G loss: 1.396607]\n",
      "[Epoch 0/200] [Batch 437/938] [D loss: 1.318908, acc: 50%] [G loss: 1.478392]\n",
      "[Epoch 0/200] [Batch 438/938] [D loss: 1.303264, acc: 51%] [G loss: 1.442813]\n",
      "[Epoch 0/200] [Batch 439/938] [D loss: 1.323555, acc: 50%] [G loss: 1.436738]\n",
      "[Epoch 0/200] [Batch 440/938] [D loss: 1.305727, acc: 56%] [G loss: 1.409135]\n",
      "[Epoch 0/200] [Batch 441/938] [D loss: 1.311676, acc: 52%] [G loss: 1.392702]\n",
      "[Epoch 0/200] [Batch 442/938] [D loss: 1.296485, acc: 57%] [G loss: 1.352464]\n",
      "[Epoch 0/200] [Batch 443/938] [D loss: 1.327668, acc: 50%] [G loss: 1.432440]\n",
      "[Epoch 0/200] [Batch 444/938] [D loss: 1.302013, acc: 55%] [G loss: 1.398813]\n",
      "[Epoch 0/200] [Batch 445/938] [D loss: 1.311782, acc: 55%] [G loss: 1.431467]\n",
      "[Epoch 0/200] [Batch 446/938] [D loss: 1.281290, acc: 62%] [G loss: 1.379177]\n",
      "[Epoch 0/200] [Batch 447/938] [D loss: 1.295232, acc: 53%] [G loss: 1.413350]\n",
      "[Epoch 0/200] [Batch 448/938] [D loss: 1.298706, acc: 55%] [G loss: 1.401175]\n",
      "[Epoch 0/200] [Batch 449/938] [D loss: 1.317727, acc: 56%] [G loss: 1.429653]\n",
      "[Epoch 0/200] [Batch 450/938] [D loss: 1.276440, acc: 62%] [G loss: 1.382409]\n",
      "[Epoch 0/200] [Batch 451/938] [D loss: 1.309824, acc: 56%] [G loss: 1.388642]\n",
      "[Epoch 0/200] [Batch 452/938] [D loss: 1.284362, acc: 59%] [G loss: 1.421103]\n",
      "[Epoch 0/200] [Batch 453/938] [D loss: 1.293821, acc: 57%] [G loss: 1.387917]\n",
      "[Epoch 0/200] [Batch 454/938] [D loss: 1.298457, acc: 54%] [G loss: 1.402155]\n",
      "[Epoch 0/200] [Batch 455/938] [D loss: 1.273856, acc: 62%] [G loss: 1.396201]\n",
      "[Epoch 0/200] [Batch 456/938] [D loss: 1.295187, acc: 58%] [G loss: 1.405805]\n",
      "[Epoch 0/200] [Batch 457/938] [D loss: 1.265115, acc: 60%] [G loss: 1.372107]\n",
      "[Epoch 0/200] [Batch 458/938] [D loss: 1.282595, acc: 62%] [G loss: 1.368051]\n",
      "[Epoch 0/200] [Batch 459/938] [D loss: 1.278917, acc: 59%] [G loss: 1.369851]\n",
      "[Epoch 0/200] [Batch 460/938] [D loss: 1.259071, acc: 61%] [G loss: 1.414595]\n",
      "[Epoch 0/200] [Batch 461/938] [D loss: 1.284056, acc: 60%] [G loss: 1.380398]\n",
      "[Epoch 0/200] [Batch 462/938] [D loss: 1.268692, acc: 60%] [G loss: 1.389190]\n",
      "[Epoch 0/200] [Batch 463/938] [D loss: 1.280133, acc: 59%] [G loss: 1.402413]\n",
      "[Epoch 0/200] [Batch 464/938] [D loss: 1.303505, acc: 51%] [G loss: 1.421214]\n",
      "[Epoch 0/200] [Batch 465/938] [D loss: 1.280103, acc: 65%] [G loss: 1.363825]\n",
      "[Epoch 0/200] [Batch 466/938] [D loss: 1.290646, acc: 56%] [G loss: 1.442178]\n",
      "[Epoch 0/200] [Batch 467/938] [D loss: 1.269103, acc: 64%] [G loss: 1.375802]\n",
      "[Epoch 0/200] [Batch 468/938] [D loss: 1.264414, acc: 62%] [G loss: 1.393906]\n",
      "[Epoch 0/200] [Batch 469/938] [D loss: 1.255720, acc: 59%] [G loss: 1.392376]\n",
      "[Epoch 0/200] [Batch 470/938] [D loss: 1.290124, acc: 54%] [G loss: 1.402223]\n",
      "[Epoch 0/200] [Batch 471/938] [D loss: 1.260568, acc: 63%] [G loss: 1.353463]\n",
      "[Epoch 0/200] [Batch 472/938] [D loss: 1.270318, acc: 61%] [G loss: 1.372965]\n",
      "[Epoch 0/200] [Batch 473/938] [D loss: 1.275338, acc: 58%] [G loss: 1.392627]\n",
      "[Epoch 0/200] [Batch 474/938] [D loss: 1.275311, acc: 61%] [G loss: 1.399634]\n",
      "[Epoch 0/200] [Batch 475/938] [D loss: 1.273542, acc: 57%] [G loss: 1.399081]\n",
      "[Epoch 0/200] [Batch 476/938] [D loss: 1.264835, acc: 62%] [G loss: 1.419274]\n",
      "[Epoch 0/200] [Batch 477/938] [D loss: 1.265010, acc: 64%] [G loss: 1.349542]\n",
      "[Epoch 0/200] [Batch 478/938] [D loss: 1.240173, acc: 64%] [G loss: 1.334191]\n",
      "[Epoch 0/200] [Batch 479/938] [D loss: 1.259809, acc: 63%] [G loss: 1.416238]\n",
      "[Epoch 0/200] [Batch 480/938] [D loss: 1.262425, acc: 64%] [G loss: 1.358878]\n",
      "[Epoch 0/200] [Batch 481/938] [D loss: 1.252193, acc: 69%] [G loss: 1.331124]\n",
      "[Epoch 0/200] [Batch 482/938] [D loss: 1.265218, acc: 59%] [G loss: 1.406938]\n",
      "[Epoch 0/200] [Batch 483/938] [D loss: 1.252967, acc: 64%] [G loss: 1.371605]\n",
      "[Epoch 0/200] [Batch 484/938] [D loss: 1.256853, acc: 62%] [G loss: 1.414513]\n",
      "[Epoch 0/200] [Batch 485/938] [D loss: 1.262044, acc: 65%] [G loss: 1.366134]\n",
      "[Epoch 0/200] [Batch 486/938] [D loss: 1.216647, acc: 73%] [G loss: 1.336499]\n",
      "[Epoch 0/200] [Batch 487/938] [D loss: 1.247027, acc: 64%] [G loss: 1.370133]\n",
      "[Epoch 0/200] [Batch 488/938] [D loss: 1.231882, acc: 66%] [G loss: 1.341721]\n",
      "[Epoch 0/200] [Batch 489/938] [D loss: 1.222180, acc: 67%] [G loss: 1.313743]\n",
      "[Epoch 0/200] [Batch 490/938] [D loss: 1.279708, acc: 61%] [G loss: 1.365049]\n",
      "[Epoch 0/200] [Batch 491/938] [D loss: 1.239250, acc: 64%] [G loss: 1.381102]\n",
      "[Epoch 0/200] [Batch 492/938] [D loss: 1.269834, acc: 61%] [G loss: 1.365815]\n",
      "[Epoch 0/200] [Batch 493/938] [D loss: 1.242264, acc: 67%] [G loss: 1.315644]\n",
      "[Epoch 0/200] [Batch 494/938] [D loss: 1.248732, acc: 67%] [G loss: 1.379097]\n",
      "[Epoch 0/200] [Batch 495/938] [D loss: 1.249240, acc: 64%] [G loss: 1.363341]\n",
      "[Epoch 0/200] [Batch 496/938] [D loss: 1.251212, acc: 62%] [G loss: 1.379629]\n",
      "[Epoch 0/200] [Batch 497/938] [D loss: 1.252874, acc: 63%] [G loss: 1.373379]\n",
      "[Epoch 0/200] [Batch 498/938] [D loss: 1.242150, acc: 64%] [G loss: 1.385615]\n",
      "[Epoch 0/200] [Batch 499/938] [D loss: 1.259282, acc: 61%] [G loss: 1.395847]\n",
      "[Epoch 0/200] [Batch 500/938] [D loss: 1.268981, acc: 60%] [G loss: 1.415441]\n",
      "[Epoch 0/200] [Batch 501/938] [D loss: 1.232213, acc: 66%] [G loss: 1.411540]\n",
      "[Epoch 0/200] [Batch 502/938] [D loss: 1.229088, acc: 69%] [G loss: 1.340393]\n",
      "[Epoch 0/200] [Batch 503/938] [D loss: 1.218181, acc: 66%] [G loss: 1.361449]\n",
      "[Epoch 0/200] [Batch 504/938] [D loss: 1.251342, acc: 61%] [G loss: 1.334289]\n",
      "[Epoch 0/200] [Batch 505/938] [D loss: 1.213989, acc: 72%] [G loss: 1.349497]\n",
      "[Epoch 0/200] [Batch 506/938] [D loss: 1.268785, acc: 62%] [G loss: 1.387336]\n",
      "[Epoch 0/200] [Batch 507/938] [D loss: 1.210763, acc: 71%] [G loss: 1.326236]\n",
      "[Epoch 0/200] [Batch 508/938] [D loss: 1.250745, acc: 67%] [G loss: 1.331118]\n",
      "[Epoch 0/200] [Batch 509/938] [D loss: 1.238003, acc: 69%] [G loss: 1.364055]\n",
      "[Epoch 0/200] [Batch 510/938] [D loss: 1.225175, acc: 67%] [G loss: 1.312305]\n",
      "[Epoch 0/200] [Batch 511/938] [D loss: 1.226339, acc: 70%] [G loss: 1.329476]\n",
      "[Epoch 0/200] [Batch 512/938] [D loss: 1.212551, acc: 75%] [G loss: 1.328851]\n",
      "[Epoch 0/200] [Batch 513/938] [D loss: 1.236579, acc: 68%] [G loss: 1.303063]\n",
      "[Epoch 0/200] [Batch 514/938] [D loss: 1.218232, acc: 68%] [G loss: 1.327237]\n",
      "[Epoch 0/200] [Batch 515/938] [D loss: 1.215979, acc: 69%] [G loss: 1.313126]\n",
      "[Epoch 0/200] [Batch 516/938] [D loss: 1.221802, acc: 67%] [G loss: 1.333077]\n",
      "[Epoch 0/200] [Batch 517/938] [D loss: 1.259639, acc: 61%] [G loss: 1.403334]\n",
      "[Epoch 0/200] [Batch 518/938] [D loss: 1.205358, acc: 72%] [G loss: 1.338944]\n",
      "[Epoch 0/200] [Batch 519/938] [D loss: 1.241328, acc: 66%] [G loss: 1.356326]\n",
      "[Epoch 0/200] [Batch 520/938] [D loss: 1.228970, acc: 64%] [G loss: 1.367121]\n",
      "[Epoch 0/200] [Batch 521/938] [D loss: 1.211894, acc: 73%] [G loss: 1.321207]\n",
      "[Epoch 0/200] [Batch 522/938] [D loss: 1.205493, acc: 71%] [G loss: 1.320176]\n",
      "[Epoch 0/200] [Batch 523/938] [D loss: 1.207951, acc: 71%] [G loss: 1.315394]\n",
      "[Epoch 0/200] [Batch 524/938] [D loss: 1.224788, acc: 66%] [G loss: 1.311136]\n",
      "[Epoch 0/200] [Batch 525/938] [D loss: 1.203215, acc: 75%] [G loss: 1.292461]\n",
      "[Epoch 0/200] [Batch 526/938] [D loss: 1.213229, acc: 70%] [G loss: 1.377052]\n",
      "[Epoch 0/200] [Batch 527/938] [D loss: 1.193412, acc: 76%] [G loss: 1.307890]\n",
      "[Epoch 0/200] [Batch 528/938] [D loss: 1.225799, acc: 64%] [G loss: 1.345133]\n",
      "[Epoch 0/200] [Batch 529/938] [D loss: 1.223776, acc: 69%] [G loss: 1.356809]\n",
      "[Epoch 0/200] [Batch 530/938] [D loss: 1.215372, acc: 69%] [G loss: 1.323396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 531/938] [D loss: 1.211837, acc: 71%] [G loss: 1.305914]\n",
      "[Epoch 0/200] [Batch 532/938] [D loss: 1.190339, acc: 72%] [G loss: 1.288852]\n",
      "[Epoch 0/200] [Batch 533/938] [D loss: 1.181157, acc: 74%] [G loss: 1.324173]\n",
      "[Epoch 0/200] [Batch 534/938] [D loss: 1.210908, acc: 69%] [G loss: 1.343301]\n",
      "[Epoch 0/200] [Batch 535/938] [D loss: 1.249559, acc: 66%] [G loss: 1.346565]\n",
      "[Epoch 0/200] [Batch 536/938] [D loss: 1.189540, acc: 73%] [G loss: 1.296181]\n",
      "[Epoch 0/200] [Batch 537/938] [D loss: 1.170967, acc: 76%] [G loss: 1.300472]\n",
      "[Epoch 0/200] [Batch 538/938] [D loss: 1.200087, acc: 74%] [G loss: 1.313733]\n",
      "[Epoch 0/200] [Batch 539/938] [D loss: 1.203631, acc: 72%] [G loss: 1.309347]\n",
      "[Epoch 0/200] [Batch 540/938] [D loss: 1.198447, acc: 72%] [G loss: 1.304553]\n",
      "[Epoch 0/200] [Batch 541/938] [D loss: 1.171266, acc: 75%] [G loss: 1.252444]\n",
      "[Epoch 0/200] [Batch 542/938] [D loss: 1.193377, acc: 76%] [G loss: 1.284203]\n",
      "[Epoch 0/200] [Batch 543/938] [D loss: 1.168046, acc: 77%] [G loss: 1.299054]\n",
      "[Epoch 0/200] [Batch 544/938] [D loss: 1.211184, acc: 71%] [G loss: 1.337875]\n",
      "[Epoch 0/200] [Batch 545/938] [D loss: 1.178259, acc: 74%] [G loss: 1.319155]\n",
      "[Epoch 0/200] [Batch 546/938] [D loss: 1.200138, acc: 74%] [G loss: 1.332186]\n",
      "[Epoch 0/200] [Batch 547/938] [D loss: 1.198288, acc: 72%] [G loss: 1.385297]\n",
      "[Epoch 0/200] [Batch 548/938] [D loss: 1.205511, acc: 67%] [G loss: 1.324073]\n",
      "[Epoch 0/200] [Batch 549/938] [D loss: 1.172300, acc: 73%] [G loss: 1.308745]\n",
      "[Epoch 0/200] [Batch 550/938] [D loss: 1.210460, acc: 69%] [G loss: 1.336964]\n",
      "[Epoch 0/200] [Batch 551/938] [D loss: 1.218163, acc: 70%] [G loss: 1.309150]\n",
      "[Epoch 0/200] [Batch 552/938] [D loss: 1.212278, acc: 70%] [G loss: 1.331486]\n",
      "[Epoch 0/200] [Batch 553/938] [D loss: 1.171066, acc: 76%] [G loss: 1.256059]\n",
      "[Epoch 0/200] [Batch 554/938] [D loss: 1.210296, acc: 67%] [G loss: 1.328359]\n",
      "[Epoch 0/200] [Batch 555/938] [D loss: 1.217518, acc: 69%] [G loss: 1.287717]\n",
      "[Epoch 0/200] [Batch 556/938] [D loss: 1.205192, acc: 71%] [G loss: 1.333820]\n",
      "[Epoch 0/200] [Batch 557/938] [D loss: 1.196610, acc: 70%] [G loss: 1.290910]\n",
      "[Epoch 0/200] [Batch 558/938] [D loss: 1.183180, acc: 74%] [G loss: 1.304271]\n",
      "[Epoch 0/200] [Batch 559/938] [D loss: 1.210561, acc: 71%] [G loss: 1.296515]\n",
      "[Epoch 0/200] [Batch 560/938] [D loss: 1.178748, acc: 75%] [G loss: 1.271558]\n",
      "[Epoch 0/200] [Batch 561/938] [D loss: 1.162257, acc: 76%] [G loss: 1.247251]\n",
      "[Epoch 0/200] [Batch 562/938] [D loss: 1.187848, acc: 74%] [G loss: 1.250742]\n",
      "[Epoch 0/200] [Batch 563/938] [D loss: 1.180109, acc: 75%] [G loss: 1.305105]\n",
      "[Epoch 0/200] [Batch 564/938] [D loss: 1.204649, acc: 70%] [G loss: 1.305733]\n",
      "[Epoch 0/200] [Batch 565/938] [D loss: 1.232722, acc: 67%] [G loss: 1.336607]\n",
      "[Epoch 0/200] [Batch 566/938] [D loss: 1.232704, acc: 68%] [G loss: 1.309523]\n",
      "[Epoch 0/200] [Batch 567/938] [D loss: 1.229347, acc: 66%] [G loss: 1.274103]\n",
      "[Epoch 0/200] [Batch 568/938] [D loss: 1.176520, acc: 75%] [G loss: 1.272897]\n",
      "[Epoch 0/200] [Batch 569/938] [D loss: 1.169671, acc: 74%] [G loss: 1.303363]\n",
      "[Epoch 0/200] [Batch 570/938] [D loss: 1.205473, acc: 64%] [G loss: 1.355240]\n",
      "[Epoch 0/200] [Batch 571/938] [D loss: 1.189086, acc: 71%] [G loss: 1.378268]\n",
      "[Epoch 0/200] [Batch 572/938] [D loss: 1.177523, acc: 78%] [G loss: 1.312949]\n",
      "[Epoch 0/200] [Batch 573/938] [D loss: 1.181382, acc: 75%] [G loss: 1.282384]\n",
      "[Epoch 0/200] [Batch 574/938] [D loss: 1.153590, acc: 78%] [G loss: 1.269683]\n",
      "[Epoch 0/200] [Batch 575/938] [D loss: 1.174880, acc: 80%] [G loss: 1.256531]\n",
      "[Epoch 0/200] [Batch 576/938] [D loss: 1.164463, acc: 74%] [G loss: 1.314164]\n",
      "[Epoch 0/200] [Batch 577/938] [D loss: 1.129956, acc: 83%] [G loss: 1.230415]\n",
      "[Epoch 0/200] [Batch 578/938] [D loss: 1.133784, acc: 80%] [G loss: 1.255384]\n",
      "[Epoch 0/200] [Batch 579/938] [D loss: 1.167131, acc: 75%] [G loss: 1.308727]\n",
      "[Epoch 0/200] [Batch 580/938] [D loss: 1.153108, acc: 78%] [G loss: 1.229495]\n",
      "[Epoch 0/200] [Batch 581/938] [D loss: 1.183017, acc: 71%] [G loss: 1.256017]\n",
      "[Epoch 0/200] [Batch 582/938] [D loss: 1.183453, acc: 72%] [G loss: 1.288750]\n",
      "[Epoch 0/200] [Batch 583/938] [D loss: 1.176849, acc: 74%] [G loss: 1.288460]\n",
      "[Epoch 0/200] [Batch 584/938] [D loss: 1.196612, acc: 73%] [G loss: 1.308688]\n",
      "[Epoch 0/200] [Batch 585/938] [D loss: 1.175180, acc: 75%] [G loss: 1.267039]\n",
      "[Epoch 0/200] [Batch 586/938] [D loss: 1.172726, acc: 75%] [G loss: 1.278998]\n",
      "[Epoch 0/200] [Batch 587/938] [D loss: 1.171602, acc: 77%] [G loss: 1.288888]\n",
      "[Epoch 0/200] [Batch 588/938] [D loss: 1.130907, acc: 79%] [G loss: 1.275560]\n",
      "[Epoch 0/200] [Batch 589/938] [D loss: 1.144264, acc: 80%] [G loss: 1.279168]\n",
      "[Epoch 0/200] [Batch 590/938] [D loss: 1.187748, acc: 71%] [G loss: 1.320125]\n",
      "[Epoch 0/200] [Batch 591/938] [D loss: 1.140029, acc: 82%] [G loss: 1.264486]\n",
      "[Epoch 0/200] [Batch 592/938] [D loss: 1.198205, acc: 71%] [G loss: 1.298136]\n",
      "[Epoch 0/200] [Batch 593/938] [D loss: 1.174001, acc: 75%] [G loss: 1.321645]\n",
      "[Epoch 0/200] [Batch 594/938] [D loss: 1.155034, acc: 82%] [G loss: 1.254367]\n",
      "[Epoch 0/200] [Batch 595/938] [D loss: 1.141912, acc: 82%] [G loss: 1.266263]\n",
      "[Epoch 0/200] [Batch 596/938] [D loss: 1.153009, acc: 78%] [G loss: 1.251570]\n",
      "[Epoch 0/200] [Batch 597/938] [D loss: 1.190661, acc: 74%] [G loss: 1.302658]\n",
      "[Epoch 0/200] [Batch 598/938] [D loss: 1.129922, acc: 85%] [G loss: 1.252273]\n",
      "[Epoch 0/200] [Batch 599/938] [D loss: 1.132071, acc: 82%] [G loss: 1.246695]\n",
      "[Epoch 0/200] [Batch 600/938] [D loss: 1.156261, acc: 80%] [G loss: 1.309853]\n",
      "[Epoch 0/200] [Batch 601/938] [D loss: 1.173480, acc: 77%] [G loss: 1.289408]\n",
      "[Epoch 0/200] [Batch 602/938] [D loss: 1.157727, acc: 80%] [G loss: 1.258885]\n",
      "[Epoch 0/200] [Batch 603/938] [D loss: 1.133812, acc: 82%] [G loss: 1.251134]\n",
      "[Epoch 0/200] [Batch 604/938] [D loss: 1.183426, acc: 78%] [G loss: 1.285077]\n",
      "[Epoch 0/200] [Batch 605/938] [D loss: 1.141741, acc: 81%] [G loss: 1.272239]\n",
      "[Epoch 0/200] [Batch 606/938] [D loss: 1.143512, acc: 78%] [G loss: 1.290922]\n",
      "[Epoch 0/200] [Batch 607/938] [D loss: 1.173597, acc: 76%] [G loss: 1.305470]\n",
      "[Epoch 0/200] [Batch 608/938] [D loss: 1.155242, acc: 80%] [G loss: 1.292939]\n",
      "[Epoch 0/200] [Batch 609/938] [D loss: 1.135065, acc: 78%] [G loss: 1.298504]\n",
      "[Epoch 0/200] [Batch 610/938] [D loss: 1.144105, acc: 79%] [G loss: 1.298302]\n",
      "[Epoch 0/200] [Batch 611/938] [D loss: 1.173744, acc: 74%] [G loss: 1.254478]\n",
      "[Epoch 0/200] [Batch 612/938] [D loss: 1.145623, acc: 74%] [G loss: 1.245031]\n",
      "[Epoch 0/200] [Batch 613/938] [D loss: 1.141484, acc: 81%] [G loss: 1.274219]\n",
      "[Epoch 0/200] [Batch 614/938] [D loss: 1.113094, acc: 83%] [G loss: 1.303439]\n",
      "[Epoch 0/200] [Batch 615/938] [D loss: 1.148305, acc: 78%] [G loss: 1.300254]\n",
      "[Epoch 0/200] [Batch 616/938] [D loss: 1.144178, acc: 78%] [G loss: 1.276399]\n",
      "[Epoch 0/200] [Batch 617/938] [D loss: 1.172480, acc: 74%] [G loss: 1.327351]\n",
      "[Epoch 0/200] [Batch 618/938] [D loss: 1.142850, acc: 81%] [G loss: 1.236976]\n",
      "[Epoch 0/200] [Batch 619/938] [D loss: 1.151043, acc: 78%] [G loss: 1.293805]\n",
      "[Epoch 0/200] [Batch 620/938] [D loss: 1.143497, acc: 82%] [G loss: 1.265085]\n",
      "[Epoch 0/200] [Batch 621/938] [D loss: 1.151469, acc: 79%] [G loss: 1.240851]\n",
      "[Epoch 0/200] [Batch 622/938] [D loss: 1.130483, acc: 81%] [G loss: 1.258227]\n",
      "[Epoch 0/200] [Batch 623/938] [D loss: 1.123492, acc: 85%] [G loss: 1.245749]\n",
      "[Epoch 0/200] [Batch 624/938] [D loss: 1.139943, acc: 83%] [G loss: 1.270618]\n",
      "[Epoch 0/200] [Batch 625/938] [D loss: 1.161637, acc: 76%] [G loss: 1.257811]\n",
      "[Epoch 0/200] [Batch 626/938] [D loss: 1.137437, acc: 82%] [G loss: 1.247090]\n",
      "[Epoch 0/200] [Batch 627/938] [D loss: 1.145363, acc: 82%] [G loss: 1.271680]\n",
      "[Epoch 0/200] [Batch 628/938] [D loss: 1.139683, acc: 83%] [G loss: 1.205229]\n",
      "[Epoch 0/200] [Batch 629/938] [D loss: 1.145611, acc: 81%] [G loss: 1.318302]\n",
      "[Epoch 0/200] [Batch 630/938] [D loss: 1.167407, acc: 72%] [G loss: 1.275251]\n",
      "[Epoch 0/200] [Batch 631/938] [D loss: 1.168898, acc: 75%] [G loss: 1.293996]\n",
      "[Epoch 0/200] [Batch 632/938] [D loss: 1.147940, acc: 79%] [G loss: 1.297263]\n",
      "[Epoch 0/200] [Batch 633/938] [D loss: 1.162329, acc: 73%] [G loss: 1.313928]\n",
      "[Epoch 0/200] [Batch 634/938] [D loss: 1.138878, acc: 82%] [G loss: 1.262929]\n",
      "[Epoch 0/200] [Batch 635/938] [D loss: 1.125088, acc: 85%] [G loss: 1.199759]\n",
      "[Epoch 0/200] [Batch 636/938] [D loss: 1.157110, acc: 79%] [G loss: 1.281573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 637/938] [D loss: 1.127314, acc: 85%] [G loss: 1.214546]\n",
      "[Epoch 0/200] [Batch 638/938] [D loss: 1.139230, acc: 77%] [G loss: 1.282900]\n",
      "[Epoch 0/200] [Batch 639/938] [D loss: 1.168656, acc: 75%] [G loss: 1.289750]\n",
      "[Epoch 0/200] [Batch 640/938] [D loss: 1.131597, acc: 85%] [G loss: 1.191791]\n",
      "[Epoch 0/200] [Batch 641/938] [D loss: 1.134357, acc: 81%] [G loss: 1.265537]\n",
      "[Epoch 0/200] [Batch 642/938] [D loss: 1.153714, acc: 82%] [G loss: 1.274092]\n",
      "[Epoch 0/200] [Batch 643/938] [D loss: 1.149964, acc: 78%] [G loss: 1.239231]\n",
      "[Epoch 0/200] [Batch 644/938] [D loss: 1.172099, acc: 77%] [G loss: 1.267990]\n",
      "[Epoch 0/200] [Batch 645/938] [D loss: 1.145396, acc: 78%] [G loss: 1.284514]\n",
      "[Epoch 0/200] [Batch 646/938] [D loss: 1.119187, acc: 86%] [G loss: 1.325196]\n",
      "[Epoch 0/200] [Batch 647/938] [D loss: 1.143323, acc: 77%] [G loss: 1.264494]\n",
      "[Epoch 0/200] [Batch 648/938] [D loss: 1.203052, acc: 71%] [G loss: 1.281310]\n",
      "[Epoch 0/200] [Batch 649/938] [D loss: 1.119683, acc: 82%] [G loss: 1.257736]\n",
      "[Epoch 0/200] [Batch 650/938] [D loss: 1.160316, acc: 76%] [G loss: 1.309275]\n",
      "[Epoch 0/200] [Batch 651/938] [D loss: 1.156251, acc: 83%] [G loss: 1.252509]\n",
      "[Epoch 0/200] [Batch 652/938] [D loss: 1.134907, acc: 82%] [G loss: 1.250664]\n",
      "[Epoch 0/200] [Batch 653/938] [D loss: 1.136022, acc: 85%] [G loss: 1.260643]\n",
      "[Epoch 0/200] [Batch 654/938] [D loss: 1.159221, acc: 78%] [G loss: 1.246232]\n",
      "[Epoch 0/200] [Batch 655/938] [D loss: 1.117312, acc: 86%] [G loss: 1.274623]\n",
      "[Epoch 0/200] [Batch 656/938] [D loss: 1.119558, acc: 85%] [G loss: 1.201750]\n",
      "[Epoch 0/200] [Batch 657/938] [D loss: 1.142379, acc: 80%] [G loss: 1.212359]\n",
      "[Epoch 0/200] [Batch 658/938] [D loss: 1.162190, acc: 79%] [G loss: 1.233204]\n",
      "[Epoch 0/200] [Batch 659/938] [D loss: 1.140311, acc: 82%] [G loss: 1.228358]\n",
      "[Epoch 0/200] [Batch 660/938] [D loss: 1.134053, acc: 85%] [G loss: 1.247876]\n",
      "[Epoch 0/200] [Batch 661/938] [D loss: 1.133820, acc: 82%] [G loss: 1.230252]\n",
      "[Epoch 0/200] [Batch 662/938] [D loss: 1.159353, acc: 75%] [G loss: 1.261281]\n",
      "[Epoch 0/200] [Batch 663/938] [D loss: 1.152811, acc: 80%] [G loss: 1.219489]\n",
      "[Epoch 0/200] [Batch 664/938] [D loss: 1.137563, acc: 84%] [G loss: 1.239121]\n",
      "[Epoch 0/200] [Batch 665/938] [D loss: 1.146132, acc: 78%] [G loss: 1.333996]\n",
      "[Epoch 0/200] [Batch 666/938] [D loss: 1.095799, acc: 88%] [G loss: 1.204468]\n",
      "[Epoch 0/200] [Batch 667/938] [D loss: 1.149149, acc: 78%] [G loss: 1.257200]\n",
      "[Epoch 0/200] [Batch 668/938] [D loss: 1.163333, acc: 75%] [G loss: 1.345378]\n",
      "[Epoch 0/200] [Batch 669/938] [D loss: 1.145952, acc: 78%] [G loss: 1.234576]\n",
      "[Epoch 0/200] [Batch 670/938] [D loss: 1.137945, acc: 82%] [G loss: 1.260403]\n",
      "[Epoch 0/200] [Batch 671/938] [D loss: 1.163397, acc: 81%] [G loss: 1.210915]\n",
      "[Epoch 0/200] [Batch 672/938] [D loss: 1.128093, acc: 83%] [G loss: 1.250282]\n",
      "[Epoch 0/200] [Batch 673/938] [D loss: 1.129859, acc: 88%] [G loss: 1.232219]\n",
      "[Epoch 0/200] [Batch 674/938] [D loss: 1.129585, acc: 85%] [G loss: 1.216436]\n",
      "[Epoch 0/200] [Batch 675/938] [D loss: 1.151502, acc: 82%] [G loss: 1.250711]\n",
      "[Epoch 0/200] [Batch 676/938] [D loss: 1.143309, acc: 80%] [G loss: 1.265271]\n",
      "[Epoch 0/200] [Batch 677/938] [D loss: 1.145813, acc: 79%] [G loss: 1.277005]\n",
      "[Epoch 0/200] [Batch 678/938] [D loss: 1.146680, acc: 79%] [G loss: 1.265307]\n",
      "[Epoch 0/200] [Batch 679/938] [D loss: 1.113732, acc: 85%] [G loss: 1.267169]\n",
      "[Epoch 0/200] [Batch 680/938] [D loss: 1.133315, acc: 85%] [G loss: 1.249269]\n",
      "[Epoch 0/200] [Batch 681/938] [D loss: 1.130550, acc: 80%] [G loss: 1.250001]\n",
      "[Epoch 0/200] [Batch 682/938] [D loss: 1.105473, acc: 92%] [G loss: 1.229633]\n",
      "[Epoch 0/200] [Batch 683/938] [D loss: 1.117200, acc: 83%] [G loss: 1.215167]\n",
      "[Epoch 0/200] [Batch 684/938] [D loss: 1.114447, acc: 84%] [G loss: 1.208766]\n",
      "[Epoch 0/200] [Batch 685/938] [D loss: 1.139145, acc: 85%] [G loss: 1.285477]\n",
      "[Epoch 0/200] [Batch 686/938] [D loss: 1.140762, acc: 85%] [G loss: 1.277493]\n",
      "[Epoch 0/200] [Batch 687/938] [D loss: 1.108868, acc: 89%] [G loss: 1.229604]\n",
      "[Epoch 0/200] [Batch 688/938] [D loss: 1.123076, acc: 80%] [G loss: 1.245309]\n",
      "[Epoch 0/200] [Batch 689/938] [D loss: 1.139306, acc: 82%] [G loss: 1.259194]\n",
      "[Epoch 0/200] [Batch 690/938] [D loss: 1.115242, acc: 85%] [G loss: 1.218760]\n",
      "[Epoch 0/200] [Batch 691/938] [D loss: 1.120359, acc: 86%] [G loss: 1.252855]\n",
      "[Epoch 0/200] [Batch 692/938] [D loss: 1.118183, acc: 83%] [G loss: 1.221369]\n",
      "[Epoch 0/200] [Batch 693/938] [D loss: 1.119049, acc: 86%] [G loss: 1.202265]\n",
      "[Epoch 0/200] [Batch 694/938] [D loss: 1.180930, acc: 76%] [G loss: 1.201613]\n",
      "[Epoch 0/200] [Batch 695/938] [D loss: 1.171934, acc: 82%] [G loss: 1.229303]\n",
      "[Epoch 0/200] [Batch 696/938] [D loss: 1.148512, acc: 78%] [G loss: 1.208160]\n",
      "[Epoch 0/200] [Batch 697/938] [D loss: 1.150865, acc: 83%] [G loss: 1.203811]\n",
      "[Epoch 0/200] [Batch 698/938] [D loss: 1.130047, acc: 85%] [G loss: 1.271143]\n",
      "[Epoch 0/200] [Batch 699/938] [D loss: 1.111587, acc: 89%] [G loss: 1.230436]\n",
      "[Epoch 0/200] [Batch 700/938] [D loss: 1.128848, acc: 83%] [G loss: 1.251701]\n",
      "[Epoch 0/200] [Batch 701/938] [D loss: 1.132733, acc: 82%] [G loss: 1.261474]\n",
      "[Epoch 0/200] [Batch 702/938] [D loss: 1.162539, acc: 78%] [G loss: 1.282297]\n",
      "[Epoch 0/200] [Batch 703/938] [D loss: 1.183468, acc: 73%] [G loss: 1.310981]\n",
      "[Epoch 0/200] [Batch 704/938] [D loss: 1.145381, acc: 79%] [G loss: 1.268027]\n",
      "[Epoch 0/200] [Batch 705/938] [D loss: 1.102341, acc: 87%] [G loss: 1.176600]\n",
      "[Epoch 0/200] [Batch 706/938] [D loss: 1.151160, acc: 82%] [G loss: 1.212301]\n",
      "[Epoch 0/200] [Batch 707/938] [D loss: 1.149432, acc: 76%] [G loss: 1.254315]\n",
      "[Epoch 0/200] [Batch 708/938] [D loss: 1.104239, acc: 83%] [G loss: 1.303313]\n",
      "[Epoch 0/200] [Batch 709/938] [D loss: 1.146755, acc: 82%] [G loss: 1.242770]\n",
      "[Epoch 0/200] [Batch 710/938] [D loss: 1.096286, acc: 87%] [G loss: 1.280774]\n",
      "[Epoch 0/200] [Batch 711/938] [D loss: 1.088069, acc: 88%] [G loss: 1.267515]\n",
      "[Epoch 0/200] [Batch 712/938] [D loss: 1.121326, acc: 83%] [G loss: 1.226284]\n",
      "[Epoch 0/200] [Batch 713/938] [D loss: 1.137916, acc: 82%] [G loss: 1.169470]\n",
      "[Epoch 0/200] [Batch 714/938] [D loss: 1.139899, acc: 84%] [G loss: 1.206060]\n",
      "[Epoch 0/200] [Batch 715/938] [D loss: 1.133328, acc: 82%] [G loss: 1.273839]\n",
      "[Epoch 0/200] [Batch 716/938] [D loss: 1.125561, acc: 83%] [G loss: 1.235104]\n",
      "[Epoch 0/200] [Batch 717/938] [D loss: 1.123909, acc: 85%] [G loss: 1.186752]\n",
      "[Epoch 0/200] [Batch 718/938] [D loss: 1.115733, acc: 85%] [G loss: 1.204863]\n",
      "[Epoch 0/200] [Batch 719/938] [D loss: 1.123197, acc: 85%] [G loss: 1.264615]\n",
      "[Epoch 0/200] [Batch 720/938] [D loss: 1.122138, acc: 84%] [G loss: 1.215040]\n",
      "[Epoch 0/200] [Batch 721/938] [D loss: 1.112837, acc: 82%] [G loss: 1.294605]\n",
      "[Epoch 0/200] [Batch 722/938] [D loss: 1.156666, acc: 84%] [G loss: 1.240211]\n",
      "[Epoch 0/200] [Batch 723/938] [D loss: 1.107639, acc: 87%] [G loss: 1.226463]\n",
      "[Epoch 0/200] [Batch 724/938] [D loss: 1.115834, acc: 89%] [G loss: 1.256456]\n",
      "[Epoch 0/200] [Batch 725/938] [D loss: 1.098988, acc: 86%] [G loss: 1.268132]\n",
      "[Epoch 0/200] [Batch 726/938] [D loss: 1.143235, acc: 80%] [G loss: 1.239262]\n",
      "[Epoch 0/200] [Batch 727/938] [D loss: 1.142852, acc: 83%] [G loss: 1.204531]\n",
      "[Epoch 0/200] [Batch 728/938] [D loss: 1.160640, acc: 80%] [G loss: 1.211561]\n",
      "[Epoch 0/200] [Batch 729/938] [D loss: 1.143838, acc: 82%] [G loss: 1.253030]\n",
      "[Epoch 0/200] [Batch 730/938] [D loss: 1.156214, acc: 83%] [G loss: 1.250056]\n",
      "[Epoch 0/200] [Batch 731/938] [D loss: 1.108420, acc: 88%] [G loss: 1.179480]\n",
      "[Epoch 0/200] [Batch 732/938] [D loss: 1.129248, acc: 84%] [G loss: 1.206427]\n",
      "[Epoch 0/200] [Batch 733/938] [D loss: 1.100814, acc: 82%] [G loss: 1.255770]\n",
      "[Epoch 0/200] [Batch 734/938] [D loss: 1.129395, acc: 85%] [G loss: 1.238058]\n",
      "[Epoch 0/200] [Batch 735/938] [D loss: 1.148345, acc: 82%] [G loss: 1.255083]\n",
      "[Epoch 0/200] [Batch 736/938] [D loss: 1.124690, acc: 89%] [G loss: 1.210237]\n",
      "[Epoch 0/200] [Batch 737/938] [D loss: 1.147166, acc: 84%] [G loss: 1.240706]\n",
      "[Epoch 0/200] [Batch 738/938] [D loss: 1.126596, acc: 87%] [G loss: 1.233604]\n",
      "[Epoch 0/200] [Batch 739/938] [D loss: 1.135587, acc: 82%] [G loss: 1.233122]\n",
      "[Epoch 0/200] [Batch 740/938] [D loss: 1.106742, acc: 88%] [G loss: 1.291887]\n",
      "[Epoch 0/200] [Batch 741/938] [D loss: 1.141264, acc: 85%] [G loss: 1.231003]\n",
      "[Epoch 0/200] [Batch 742/938] [D loss: 1.133911, acc: 87%] [G loss: 1.208166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 743/938] [D loss: 1.155443, acc: 82%] [G loss: 1.221905]\n",
      "[Epoch 0/200] [Batch 744/938] [D loss: 1.195340, acc: 69%] [G loss: 1.273813]\n",
      "[Epoch 0/200] [Batch 745/938] [D loss: 1.116896, acc: 90%] [G loss: 1.197152]\n",
      "[Epoch 0/200] [Batch 746/938] [D loss: 1.127751, acc: 85%] [G loss: 1.231905]\n",
      "[Epoch 0/200] [Batch 747/938] [D loss: 1.121937, acc: 87%] [G loss: 1.236087]\n",
      "[Epoch 0/200] [Batch 748/938] [D loss: 1.133804, acc: 85%] [G loss: 1.173582]\n",
      "[Epoch 0/200] [Batch 749/938] [D loss: 1.150907, acc: 84%] [G loss: 1.231973]\n",
      "[Epoch 0/200] [Batch 750/938] [D loss: 1.101211, acc: 89%] [G loss: 1.258257]\n",
      "[Epoch 0/200] [Batch 751/938] [D loss: 1.129358, acc: 89%] [G loss: 1.214611]\n",
      "[Epoch 0/200] [Batch 752/938] [D loss: 1.119880, acc: 82%] [G loss: 1.240353]\n",
      "[Epoch 0/200] [Batch 753/938] [D loss: 1.135186, acc: 79%] [G loss: 1.227312]\n",
      "[Epoch 0/200] [Batch 754/938] [D loss: 1.145901, acc: 83%] [G loss: 1.218812]\n",
      "[Epoch 0/200] [Batch 755/938] [D loss: 1.139558, acc: 86%] [G loss: 1.171604]\n",
      "[Epoch 0/200] [Batch 756/938] [D loss: 1.110289, acc: 87%] [G loss: 1.218166]\n",
      "[Epoch 0/200] [Batch 757/938] [D loss: 1.085281, acc: 89%] [G loss: 1.228678]\n",
      "[Epoch 0/200] [Batch 758/938] [D loss: 1.092322, acc: 89%] [G loss: 1.249209]\n",
      "[Epoch 0/200] [Batch 759/938] [D loss: 1.092982, acc: 89%] [G loss: 1.209383]\n",
      "[Epoch 0/200] [Batch 760/938] [D loss: 1.107987, acc: 89%] [G loss: 1.245197]\n",
      "[Epoch 0/200] [Batch 761/938] [D loss: 1.134121, acc: 81%] [G loss: 1.275108]\n",
      "[Epoch 0/200] [Batch 762/938] [D loss: 1.108050, acc: 88%] [G loss: 1.235852]\n",
      "[Epoch 0/200] [Batch 763/938] [D loss: 1.114819, acc: 83%] [G loss: 1.232182]\n",
      "[Epoch 0/200] [Batch 764/938] [D loss: 1.091844, acc: 88%] [G loss: 1.239161]\n",
      "[Epoch 0/200] [Batch 765/938] [D loss: 1.145053, acc: 85%] [G loss: 1.199217]\n",
      "[Epoch 0/200] [Batch 766/938] [D loss: 1.118649, acc: 85%] [G loss: 1.242454]\n",
      "[Epoch 0/200] [Batch 767/938] [D loss: 1.123081, acc: 86%] [G loss: 1.194387]\n",
      "[Epoch 0/200] [Batch 768/938] [D loss: 1.106586, acc: 89%] [G loss: 1.207498]\n",
      "[Epoch 0/200] [Batch 769/938] [D loss: 1.137824, acc: 85%] [G loss: 1.286203]\n",
      "[Epoch 0/200] [Batch 770/938] [D loss: 1.079900, acc: 89%] [G loss: 1.246082]\n",
      "[Epoch 0/200] [Batch 771/938] [D loss: 1.123915, acc: 88%] [G loss: 1.234269]\n",
      "[Epoch 0/200] [Batch 772/938] [D loss: 1.131814, acc: 85%] [G loss: 1.244140]\n",
      "[Epoch 0/200] [Batch 773/938] [D loss: 1.151412, acc: 81%] [G loss: 1.197624]\n",
      "[Epoch 0/200] [Batch 774/938] [D loss: 1.127530, acc: 85%] [G loss: 1.194163]\n",
      "[Epoch 0/200] [Batch 775/938] [D loss: 1.118849, acc: 86%] [G loss: 1.254872]\n",
      "[Epoch 0/200] [Batch 776/938] [D loss: 1.153348, acc: 82%] [G loss: 1.237926]\n",
      "[Epoch 0/200] [Batch 777/938] [D loss: 1.124349, acc: 89%] [G loss: 1.186050]\n",
      "[Epoch 0/200] [Batch 778/938] [D loss: 1.171008, acc: 77%] [G loss: 1.228040]\n",
      "[Epoch 0/200] [Batch 779/938] [D loss: 1.115832, acc: 85%] [G loss: 1.190919]\n",
      "[Epoch 0/200] [Batch 780/938] [D loss: 1.136351, acc: 85%] [G loss: 1.175790]\n",
      "[Epoch 0/200] [Batch 781/938] [D loss: 1.125400, acc: 82%] [G loss: 1.216691]\n",
      "[Epoch 0/200] [Batch 782/938] [D loss: 1.124569, acc: 85%] [G loss: 1.251727]\n",
      "[Epoch 0/200] [Batch 783/938] [D loss: 1.110154, acc: 89%] [G loss: 1.182103]\n",
      "[Epoch 0/200] [Batch 784/938] [D loss: 1.121062, acc: 87%] [G loss: 1.188563]\n",
      "[Epoch 0/200] [Batch 785/938] [D loss: 1.122525, acc: 85%] [G loss: 1.226366]\n",
      "[Epoch 0/200] [Batch 786/938] [D loss: 1.142808, acc: 82%] [G loss: 1.232793]\n",
      "[Epoch 0/200] [Batch 787/938] [D loss: 1.157798, acc: 90%] [G loss: 1.169998]\n",
      "[Epoch 0/200] [Batch 788/938] [D loss: 1.125146, acc: 85%] [G loss: 1.248745]\n",
      "[Epoch 0/200] [Batch 789/938] [D loss: 1.125329, acc: 85%] [G loss: 1.141099]\n",
      "[Epoch 0/200] [Batch 790/938] [D loss: 1.102107, acc: 89%] [G loss: 1.183842]\n",
      "[Epoch 0/200] [Batch 791/938] [D loss: 1.149985, acc: 85%] [G loss: 1.178333]\n",
      "[Epoch 0/200] [Batch 792/938] [D loss: 1.123820, acc: 88%] [G loss: 1.221948]\n",
      "[Epoch 0/200] [Batch 793/938] [D loss: 1.104779, acc: 85%] [G loss: 1.233675]\n",
      "[Epoch 0/200] [Batch 794/938] [D loss: 1.120098, acc: 92%] [G loss: 1.213255]\n",
      "[Epoch 0/200] [Batch 795/938] [D loss: 1.140372, acc: 85%] [G loss: 1.255326]\n",
      "[Epoch 0/200] [Batch 796/938] [D loss: 1.121799, acc: 85%] [G loss: 1.189864]\n",
      "[Epoch 0/200] [Batch 797/938] [D loss: 1.085743, acc: 89%] [G loss: 1.173937]\n",
      "[Epoch 0/200] [Batch 798/938] [D loss: 1.123398, acc: 91%] [G loss: 1.187092]\n",
      "[Epoch 0/200] [Batch 799/938] [D loss: 1.082005, acc: 91%] [G loss: 1.207126]\n",
      "[Epoch 0/200] [Batch 800/938] [D loss: 1.123385, acc: 87%] [G loss: 1.244442]\n",
      "[Epoch 0/200] [Batch 801/938] [D loss: 1.116642, acc: 85%] [G loss: 1.188911]\n",
      "[Epoch 0/200] [Batch 802/938] [D loss: 1.121778, acc: 89%] [G loss: 1.175876]\n",
      "[Epoch 0/200] [Batch 803/938] [D loss: 1.158450, acc: 78%] [G loss: 1.205297]\n",
      "[Epoch 0/200] [Batch 804/938] [D loss: 1.134541, acc: 82%] [G loss: 1.235137]\n",
      "[Epoch 0/200] [Batch 805/938] [D loss: 1.114344, acc: 84%] [G loss: 1.201766]\n",
      "[Epoch 0/200] [Batch 806/938] [D loss: 1.104835, acc: 85%] [G loss: 1.274941]\n",
      "[Epoch 0/200] [Batch 807/938] [D loss: 1.125500, acc: 83%] [G loss: 1.244144]\n",
      "[Epoch 0/200] [Batch 808/938] [D loss: 1.129652, acc: 80%] [G loss: 1.211955]\n",
      "[Epoch 0/200] [Batch 809/938] [D loss: 1.144439, acc: 82%] [G loss: 1.220816]\n",
      "[Epoch 0/200] [Batch 810/938] [D loss: 1.125681, acc: 85%] [G loss: 1.219311]\n",
      "[Epoch 0/200] [Batch 811/938] [D loss: 1.104161, acc: 87%] [G loss: 1.289673]\n",
      "[Epoch 0/200] [Batch 812/938] [D loss: 1.118660, acc: 88%] [G loss: 1.232594]\n",
      "[Epoch 0/200] [Batch 813/938] [D loss: 1.089219, acc: 91%] [G loss: 1.195814]\n",
      "[Epoch 0/200] [Batch 814/938] [D loss: 1.107595, acc: 89%] [G loss: 1.232657]\n",
      "[Epoch 0/200] [Batch 815/938] [D loss: 1.106924, acc: 89%] [G loss: 1.198467]\n",
      "[Epoch 0/200] [Batch 816/938] [D loss: 1.114921, acc: 84%] [G loss: 1.215495]\n",
      "[Epoch 0/200] [Batch 817/938] [D loss: 1.112878, acc: 86%] [G loss: 1.177569]\n",
      "[Epoch 0/200] [Batch 818/938] [D loss: 1.131578, acc: 86%] [G loss: 1.253152]\n",
      "[Epoch 0/200] [Batch 819/938] [D loss: 1.103277, acc: 87%] [G loss: 1.217535]\n",
      "[Epoch 0/200] [Batch 820/938] [D loss: 1.107953, acc: 89%] [G loss: 1.269488]\n",
      "[Epoch 0/200] [Batch 821/938] [D loss: 1.113526, acc: 85%] [G loss: 1.230028]\n",
      "[Epoch 0/200] [Batch 822/938] [D loss: 1.135453, acc: 89%] [G loss: 1.174984]\n",
      "[Epoch 0/200] [Batch 823/938] [D loss: 1.128130, acc: 85%] [G loss: 1.181435]\n",
      "[Epoch 0/200] [Batch 824/938] [D loss: 1.115945, acc: 87%] [G loss: 1.193151]\n",
      "[Epoch 0/200] [Batch 825/938] [D loss: 1.099032, acc: 88%] [G loss: 1.190136]\n",
      "[Epoch 0/200] [Batch 826/938] [D loss: 1.116327, acc: 89%] [G loss: 1.151073]\n",
      "[Epoch 0/200] [Batch 827/938] [D loss: 1.101989, acc: 87%] [G loss: 1.196497]\n",
      "[Epoch 0/200] [Batch 828/938] [D loss: 1.133561, acc: 85%] [G loss: 1.168517]\n",
      "[Epoch 0/200] [Batch 829/938] [D loss: 1.125756, acc: 85%] [G loss: 1.176609]\n",
      "[Epoch 0/200] [Batch 830/938] [D loss: 1.139445, acc: 84%] [G loss: 1.263393]\n",
      "[Epoch 0/200] [Batch 831/938] [D loss: 1.108462, acc: 86%] [G loss: 1.238311]\n",
      "[Epoch 0/200] [Batch 832/938] [D loss: 1.088748, acc: 91%] [G loss: 1.223220]\n",
      "[Epoch 0/200] [Batch 833/938] [D loss: 1.123253, acc: 79%] [G loss: 1.266907]\n",
      "[Epoch 0/200] [Batch 834/938] [D loss: 1.102642, acc: 86%] [G loss: 1.237711]\n",
      "[Epoch 0/200] [Batch 835/938] [D loss: 1.135856, acc: 85%] [G loss: 1.222273]\n",
      "[Epoch 0/200] [Batch 836/938] [D loss: 1.144840, acc: 82%] [G loss: 1.247118]\n",
      "[Epoch 0/200] [Batch 837/938] [D loss: 1.120409, acc: 90%] [G loss: 1.130333]\n",
      "[Epoch 0/200] [Batch 838/938] [D loss: 1.111177, acc: 88%] [G loss: 1.160693]\n",
      "[Epoch 0/200] [Batch 839/938] [D loss: 1.119450, acc: 87%] [G loss: 1.152667]\n",
      "[Epoch 0/200] [Batch 840/938] [D loss: 1.097561, acc: 87%] [G loss: 1.205613]\n",
      "[Epoch 0/200] [Batch 841/938] [D loss: 1.076035, acc: 89%] [G loss: 1.185114]\n",
      "[Epoch 0/200] [Batch 842/938] [D loss: 1.119906, acc: 88%] [G loss: 1.223179]\n",
      "[Epoch 0/200] [Batch 843/938] [D loss: 1.136044, acc: 83%] [G loss: 1.233247]\n",
      "[Epoch 0/200] [Batch 844/938] [D loss: 1.114143, acc: 88%] [G loss: 1.289649]\n",
      "[Epoch 0/200] [Batch 845/938] [D loss: 1.137135, acc: 84%] [G loss: 1.252438]\n",
      "[Epoch 0/200] [Batch 846/938] [D loss: 1.076776, acc: 88%] [G loss: 1.185896]\n",
      "[Epoch 0/200] [Batch 847/938] [D loss: 1.096231, acc: 89%] [G loss: 1.167523]\n",
      "[Epoch 0/200] [Batch 848/938] [D loss: 1.103500, acc: 85%] [G loss: 1.272282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 849/938] [D loss: 1.106120, acc: 89%] [G loss: 1.228664]\n",
      "[Epoch 0/200] [Batch 850/938] [D loss: 1.101913, acc: 89%] [G loss: 1.254090]\n",
      "[Epoch 0/200] [Batch 851/938] [D loss: 1.113562, acc: 89%] [G loss: 1.193347]\n",
      "[Epoch 0/200] [Batch 852/938] [D loss: 1.107153, acc: 86%] [G loss: 1.248516]\n",
      "[Epoch 0/200] [Batch 853/938] [D loss: 1.120986, acc: 89%] [G loss: 1.200638]\n",
      "[Epoch 0/200] [Batch 854/938] [D loss: 1.149227, acc: 82%] [G loss: 1.225447]\n",
      "[Epoch 0/200] [Batch 855/938] [D loss: 1.162904, acc: 80%] [G loss: 1.185517]\n",
      "[Epoch 0/200] [Batch 856/938] [D loss: 1.139245, acc: 80%] [G loss: 1.229130]\n",
      "[Epoch 0/200] [Batch 857/938] [D loss: 1.112870, acc: 86%] [G loss: 1.171428]\n",
      "[Epoch 0/200] [Batch 858/938] [D loss: 1.137400, acc: 85%] [G loss: 1.231788]\n",
      "[Epoch 0/200] [Batch 859/938] [D loss: 1.103832, acc: 88%] [G loss: 1.213947]\n",
      "[Epoch 0/200] [Batch 860/938] [D loss: 1.110473, acc: 88%] [G loss: 1.254379]\n",
      "[Epoch 0/200] [Batch 861/938] [D loss: 1.077203, acc: 85%] [G loss: 1.291141]\n",
      "[Epoch 0/200] [Batch 862/938] [D loss: 1.100814, acc: 91%] [G loss: 1.169763]\n",
      "[Epoch 0/200] [Batch 863/938] [D loss: 1.095967, acc: 86%] [G loss: 1.235011]\n",
      "[Epoch 0/200] [Batch 864/938] [D loss: 1.112983, acc: 84%] [G loss: 1.275733]\n",
      "[Epoch 0/200] [Batch 865/938] [D loss: 1.119982, acc: 85%] [G loss: 1.258643]\n",
      "[Epoch 0/200] [Batch 866/938] [D loss: 1.109803, acc: 85%] [G loss: 1.250468]\n",
      "[Epoch 0/200] [Batch 867/938] [D loss: 1.107168, acc: 89%] [G loss: 1.226525]\n",
      "[Epoch 0/200] [Batch 868/938] [D loss: 1.126027, acc: 85%] [G loss: 1.241944]\n",
      "[Epoch 0/200] [Batch 869/938] [D loss: 1.108717, acc: 85%] [G loss: 1.208089]\n",
      "[Epoch 0/200] [Batch 870/938] [D loss: 1.108580, acc: 89%] [G loss: 1.201619]\n",
      "[Epoch 0/200] [Batch 871/938] [D loss: 1.113298, acc: 89%] [G loss: 1.230983]\n",
      "[Epoch 0/200] [Batch 872/938] [D loss: 1.095035, acc: 87%] [G loss: 1.198785]\n",
      "[Epoch 0/200] [Batch 873/938] [D loss: 1.127569, acc: 85%] [G loss: 1.205847]\n",
      "[Epoch 0/200] [Batch 874/938] [D loss: 1.111946, acc: 86%] [G loss: 1.215789]\n",
      "[Epoch 0/200] [Batch 875/938] [D loss: 1.092789, acc: 89%] [G loss: 1.163705]\n",
      "[Epoch 0/200] [Batch 876/938] [D loss: 1.109041, acc: 89%] [G loss: 1.202600]\n",
      "[Epoch 0/200] [Batch 877/938] [D loss: 1.107480, acc: 88%] [G loss: 1.170110]\n",
      "[Epoch 0/200] [Batch 878/938] [D loss: 1.127269, acc: 86%] [G loss: 1.239637]\n",
      "[Epoch 0/200] [Batch 879/938] [D loss: 1.103958, acc: 87%] [G loss: 1.203085]\n",
      "[Epoch 0/200] [Batch 880/938] [D loss: 1.112729, acc: 88%] [G loss: 1.204297]\n",
      "[Epoch 0/200] [Batch 881/938] [D loss: 1.120252, acc: 89%] [G loss: 1.203485]\n",
      "[Epoch 0/200] [Batch 882/938] [D loss: 1.092795, acc: 90%] [G loss: 1.242827]\n",
      "[Epoch 0/200] [Batch 883/938] [D loss: 1.087008, acc: 91%] [G loss: 1.152036]\n",
      "[Epoch 0/200] [Batch 884/938] [D loss: 1.085597, acc: 88%] [G loss: 1.239330]\n",
      "[Epoch 0/200] [Batch 885/938] [D loss: 1.079909, acc: 94%] [G loss: 1.210480]\n",
      "[Epoch 0/200] [Batch 886/938] [D loss: 1.134336, acc: 86%] [G loss: 1.231402]\n",
      "[Epoch 0/200] [Batch 887/938] [D loss: 1.098377, acc: 91%] [G loss: 1.192967]\n",
      "[Epoch 0/200] [Batch 888/938] [D loss: 1.119456, acc: 87%] [G loss: 1.206037]\n",
      "[Epoch 0/200] [Batch 889/938] [D loss: 1.099974, acc: 90%] [G loss: 1.189245]\n",
      "[Epoch 0/200] [Batch 890/938] [D loss: 1.101029, acc: 89%] [G loss: 1.155038]\n",
      "[Epoch 0/200] [Batch 891/938] [D loss: 1.122432, acc: 89%] [G loss: 1.191054]\n",
      "[Epoch 0/200] [Batch 892/938] [D loss: 1.105678, acc: 89%] [G loss: 1.244471]\n",
      "[Epoch 0/200] [Batch 893/938] [D loss: 1.129596, acc: 85%] [G loss: 1.251052]\n",
      "[Epoch 0/200] [Batch 894/938] [D loss: 1.136742, acc: 89%] [G loss: 1.159763]\n",
      "[Epoch 0/200] [Batch 895/938] [D loss: 1.107268, acc: 87%] [G loss: 1.209534]\n",
      "[Epoch 0/200] [Batch 896/938] [D loss: 1.098081, acc: 88%] [G loss: 1.182292]\n",
      "[Epoch 0/200] [Batch 897/938] [D loss: 1.090528, acc: 92%] [G loss: 1.256832]\n",
      "[Epoch 0/200] [Batch 898/938] [D loss: 1.112403, acc: 85%] [G loss: 1.244703]\n",
      "[Epoch 0/200] [Batch 899/938] [D loss: 1.116047, acc: 92%] [G loss: 1.155467]\n",
      "[Epoch 0/200] [Batch 900/938] [D loss: 1.097788, acc: 88%] [G loss: 1.275475]\n",
      "[Epoch 0/200] [Batch 901/938] [D loss: 1.104456, acc: 90%] [G loss: 1.175675]\n",
      "[Epoch 0/200] [Batch 902/938] [D loss: 1.067596, acc: 91%] [G loss: 1.179480]\n",
      "[Epoch 0/200] [Batch 903/938] [D loss: 1.104460, acc: 89%] [G loss: 1.171256]\n",
      "[Epoch 0/200] [Batch 904/938] [D loss: 1.127898, acc: 86%] [G loss: 1.183967]\n",
      "[Epoch 0/200] [Batch 905/938] [D loss: 1.121942, acc: 89%] [G loss: 1.173421]\n",
      "[Epoch 0/200] [Batch 906/938] [D loss: 1.118183, acc: 85%] [G loss: 1.218340]\n",
      "[Epoch 0/200] [Batch 907/938] [D loss: 1.098238, acc: 85%] [G loss: 1.245865]\n",
      "[Epoch 0/200] [Batch 908/938] [D loss: 1.152385, acc: 84%] [G loss: 1.138040]\n",
      "[Epoch 0/200] [Batch 909/938] [D loss: 1.093975, acc: 92%] [G loss: 1.212181]\n",
      "[Epoch 0/200] [Batch 910/938] [D loss: 1.111617, acc: 90%] [G loss: 1.254298]\n",
      "[Epoch 0/200] [Batch 911/938] [D loss: 1.100558, acc: 89%] [G loss: 1.198221]\n",
      "[Epoch 0/200] [Batch 912/938] [D loss: 1.123756, acc: 86%] [G loss: 1.222592]\n",
      "[Epoch 0/200] [Batch 913/938] [D loss: 1.089775, acc: 87%] [G loss: 1.249259]\n",
      "[Epoch 0/200] [Batch 914/938] [D loss: 1.094604, acc: 89%] [G loss: 1.219005]\n",
      "[Epoch 0/200] [Batch 915/938] [D loss: 1.096509, acc: 89%] [G loss: 1.199844]\n",
      "[Epoch 0/200] [Batch 916/938] [D loss: 1.090168, acc: 92%] [G loss: 1.279051]\n",
      "[Epoch 0/200] [Batch 917/938] [D loss: 1.094282, acc: 90%] [G loss: 1.302817]\n",
      "[Epoch 0/200] [Batch 918/938] [D loss: 1.082889, acc: 92%] [G loss: 1.176431]\n",
      "[Epoch 0/200] [Batch 919/938] [D loss: 1.138211, acc: 89%] [G loss: 1.137790]\n",
      "[Epoch 0/200] [Batch 920/938] [D loss: 1.086116, acc: 92%] [G loss: 1.161355]\n",
      "[Epoch 0/200] [Batch 921/938] [D loss: 1.124498, acc: 87%] [G loss: 1.206184]\n",
      "[Epoch 0/200] [Batch 922/938] [D loss: 1.108080, acc: 85%] [G loss: 1.225865]\n",
      "[Epoch 0/200] [Batch 923/938] [D loss: 1.104599, acc: 88%] [G loss: 1.265600]\n",
      "[Epoch 0/200] [Batch 924/938] [D loss: 1.117012, acc: 91%] [G loss: 1.206614]\n",
      "[Epoch 0/200] [Batch 925/938] [D loss: 1.145748, acc: 87%] [G loss: 1.207866]\n",
      "[Epoch 0/200] [Batch 926/938] [D loss: 1.090637, acc: 91%] [G loss: 1.199823]\n",
      "[Epoch 0/200] [Batch 927/938] [D loss: 1.099310, acc: 93%] [G loss: 1.171177]\n",
      "[Epoch 0/200] [Batch 928/938] [D loss: 1.081102, acc: 88%] [G loss: 1.191949]\n",
      "[Epoch 0/200] [Batch 929/938] [D loss: 1.089543, acc: 91%] [G loss: 1.285004]\n",
      "[Epoch 0/200] [Batch 930/938] [D loss: 1.079852, acc: 92%] [G loss: 1.244534]\n",
      "[Epoch 0/200] [Batch 931/938] [D loss: 1.110222, acc: 87%] [G loss: 1.215088]\n",
      "[Epoch 0/200] [Batch 932/938] [D loss: 1.114906, acc: 89%] [G loss: 1.196288]\n",
      "[Epoch 0/200] [Batch 933/938] [D loss: 1.117363, acc: 89%] [G loss: 1.210325]\n",
      "[Epoch 0/200] [Batch 934/938] [D loss: 1.161863, acc: 79%] [G loss: 1.167943]\n",
      "[Epoch 0/200] [Batch 935/938] [D loss: 1.127239, acc: 89%] [G loss: 1.159479]\n",
      "[Epoch 0/200] [Batch 936/938] [D loss: 1.103585, acc: 93%] [G loss: 1.118266]\n",
      "[Epoch 0/200] [Batch 937/938] [D loss: 1.094018, acc: 92%] [G loss: 1.171125]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4081042f4d1b4fbab488700c325cbdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 0/938] [D loss: 1.108180, acc: 94%] [G loss: 1.205769]\n",
      "[Epoch 1/200] [Batch 1/938] [D loss: 1.096688, acc: 86%] [G loss: 1.315556]\n",
      "[Epoch 1/200] [Batch 2/938] [D loss: 1.099220, acc: 86%] [G loss: 1.163362]\n",
      "[Epoch 1/200] [Batch 3/938] [D loss: 1.132441, acc: 86%] [G loss: 1.218190]\n",
      "[Epoch 1/200] [Batch 4/938] [D loss: 1.134670, acc: 89%] [G loss: 1.170672]\n",
      "[Epoch 1/200] [Batch 5/938] [D loss: 1.120017, acc: 90%] [G loss: 1.158303]\n",
      "[Epoch 1/200] [Batch 6/938] [D loss: 1.111984, acc: 87%] [G loss: 1.263340]\n",
      "[Epoch 1/200] [Batch 7/938] [D loss: 1.091568, acc: 92%] [G loss: 1.148262]\n",
      "[Epoch 1/200] [Batch 8/938] [D loss: 1.113729, acc: 85%] [G loss: 1.210886]\n",
      "[Epoch 1/200] [Batch 9/938] [D loss: 1.099926, acc: 88%] [G loss: 1.248154]\n",
      "[Epoch 1/200] [Batch 10/938] [D loss: 1.113055, acc: 88%] [G loss: 1.234864]\n",
      "[Epoch 1/200] [Batch 11/938] [D loss: 1.124036, acc: 85%] [G loss: 1.199798]\n",
      "[Epoch 1/200] [Batch 12/938] [D loss: 1.101206, acc: 85%] [G loss: 1.233539]\n",
      "[Epoch 1/200] [Batch 13/938] [D loss: 1.128566, acc: 85%] [G loss: 1.178922]\n",
      "[Epoch 1/200] [Batch 14/938] [D loss: 1.109940, acc: 91%] [G loss: 1.222996]\n",
      "[Epoch 1/200] [Batch 15/938] [D loss: 1.116984, acc: 87%] [G loss: 1.249568]\n",
      "[Epoch 1/200] [Batch 16/938] [D loss: 1.099360, acc: 89%] [G loss: 1.164947]\n",
      "[Epoch 1/200] [Batch 17/938] [D loss: 1.088866, acc: 88%] [G loss: 1.215831]\n",
      "[Epoch 1/200] [Batch 18/938] [D loss: 1.135270, acc: 87%] [G loss: 1.215242]\n",
      "[Epoch 1/200] [Batch 19/938] [D loss: 1.113955, acc: 87%] [G loss: 1.224854]\n",
      "[Epoch 1/200] [Batch 20/938] [D loss: 1.099746, acc: 90%] [G loss: 1.210207]\n",
      "[Epoch 1/200] [Batch 21/938] [D loss: 1.103526, acc: 84%] [G loss: 1.203169]\n",
      "[Epoch 1/200] [Batch 22/938] [D loss: 1.089672, acc: 87%] [G loss: 1.230724]\n",
      "[Epoch 1/200] [Batch 23/938] [D loss: 1.101489, acc: 89%] [G loss: 1.274719]\n",
      "[Epoch 1/200] [Batch 24/938] [D loss: 1.083879, acc: 90%] [G loss: 1.158370]\n",
      "[Epoch 1/200] [Batch 25/938] [D loss: 1.106195, acc: 92%] [G loss: 1.166089]\n",
      "[Epoch 1/200] [Batch 26/938] [D loss: 1.092052, acc: 93%] [G loss: 1.175270]\n",
      "[Epoch 1/200] [Batch 27/938] [D loss: 1.117187, acc: 88%] [G loss: 1.212594]\n",
      "[Epoch 1/200] [Batch 28/938] [D loss: 1.098267, acc: 88%] [G loss: 1.210424]\n",
      "[Epoch 1/200] [Batch 29/938] [D loss: 1.102905, acc: 87%] [G loss: 1.221774]\n",
      "[Epoch 1/200] [Batch 30/938] [D loss: 1.106493, acc: 87%] [G loss: 1.273665]\n",
      "[Epoch 1/200] [Batch 31/938] [D loss: 1.092891, acc: 92%] [G loss: 1.185418]\n",
      "[Epoch 1/200] [Batch 32/938] [D loss: 1.110064, acc: 87%] [G loss: 1.185812]\n",
      "[Epoch 1/200] [Batch 33/938] [D loss: 1.082728, acc: 92%] [G loss: 1.188083]\n",
      "[Epoch 1/200] [Batch 34/938] [D loss: 1.102907, acc: 89%] [G loss: 1.168924]\n",
      "[Epoch 1/200] [Batch 35/938] [D loss: 1.109773, acc: 85%] [G loss: 1.307936]\n",
      "[Epoch 1/200] [Batch 36/938] [D loss: 1.091997, acc: 89%] [G loss: 1.184351]\n",
      "[Epoch 1/200] [Batch 37/938] [D loss: 1.082665, acc: 92%] [G loss: 1.236371]\n",
      "[Epoch 1/200] [Batch 38/938] [D loss: 1.113086, acc: 85%] [G loss: 1.220717]\n",
      "[Epoch 1/200] [Batch 39/938] [D loss: 1.108054, acc: 83%] [G loss: 1.237353]\n",
      "[Epoch 1/200] [Batch 40/938] [D loss: 1.113232, acc: 87%] [G loss: 1.177390]\n",
      "[Epoch 1/200] [Batch 41/938] [D loss: 1.074231, acc: 92%] [G loss: 1.218521]\n",
      "[Epoch 1/200] [Batch 42/938] [D loss: 1.105453, acc: 86%] [G loss: 1.235164]\n",
      "[Epoch 1/200] [Batch 43/938] [D loss: 1.114507, acc: 89%] [G loss: 1.202820]\n",
      "[Epoch 1/200] [Batch 44/938] [D loss: 1.074053, acc: 89%] [G loss: 1.200735]\n",
      "[Epoch 1/200] [Batch 45/938] [D loss: 1.141690, acc: 89%] [G loss: 1.193300]\n",
      "[Epoch 1/200] [Batch 46/938] [D loss: 1.109195, acc: 90%] [G loss: 1.212779]\n",
      "[Epoch 1/200] [Batch 47/938] [D loss: 1.112122, acc: 90%] [G loss: 1.199797]\n",
      "[Epoch 1/200] [Batch 48/938] [D loss: 1.110370, acc: 89%] [G loss: 1.234651]\n",
      "[Epoch 1/200] [Batch 49/938] [D loss: 1.079363, acc: 89%] [G loss: 1.184139]\n",
      "[Epoch 1/200] [Batch 50/938] [D loss: 1.123510, acc: 87%] [G loss: 1.214789]\n",
      "[Epoch 1/200] [Batch 51/938] [D loss: 1.086895, acc: 89%] [G loss: 1.207066]\n",
      "[Epoch 1/200] [Batch 52/938] [D loss: 1.093089, acc: 91%] [G loss: 1.198089]\n",
      "[Epoch 1/200] [Batch 53/938] [D loss: 1.075614, acc: 90%] [G loss: 1.238805]\n",
      "[Epoch 1/200] [Batch 54/938] [D loss: 1.100590, acc: 85%] [G loss: 1.251178]\n",
      "[Epoch 1/200] [Batch 55/938] [D loss: 1.103474, acc: 92%] [G loss: 1.218200]\n",
      "[Epoch 1/200] [Batch 56/938] [D loss: 1.104227, acc: 85%] [G loss: 1.227845]\n",
      "[Epoch 1/200] [Batch 57/938] [D loss: 1.101466, acc: 88%] [G loss: 1.189418]\n",
      "[Epoch 1/200] [Batch 58/938] [D loss: 1.112715, acc: 89%] [G loss: 1.211107]\n",
      "[Epoch 1/200] [Batch 59/938] [D loss: 1.162252, acc: 83%] [G loss: 1.147908]\n",
      "[Epoch 1/200] [Batch 60/938] [D loss: 1.121809, acc: 90%] [G loss: 1.224463]\n",
      "[Epoch 1/200] [Batch 61/938] [D loss: 1.121453, acc: 86%] [G loss: 1.183654]\n",
      "[Epoch 1/200] [Batch 62/938] [D loss: 1.093876, acc: 88%] [G loss: 1.196779]\n",
      "[Epoch 1/200] [Batch 63/938] [D loss: 1.131388, acc: 89%] [G loss: 1.155933]\n",
      "[Epoch 1/200] [Batch 64/938] [D loss: 1.114089, acc: 83%] [G loss: 1.251741]\n",
      "[Epoch 1/200] [Batch 65/938] [D loss: 1.075371, acc: 91%] [G loss: 1.187271]\n",
      "[Epoch 1/200] [Batch 66/938] [D loss: 1.122095, acc: 89%] [G loss: 1.179911]\n",
      "[Epoch 1/200] [Batch 67/938] [D loss: 1.105501, acc: 90%] [G loss: 1.193300]\n",
      "[Epoch 1/200] [Batch 68/938] [D loss: 1.114780, acc: 82%] [G loss: 1.212254]\n",
      "[Epoch 1/200] [Batch 69/938] [D loss: 1.118613, acc: 84%] [G loss: 1.220938]\n",
      "[Epoch 1/200] [Batch 70/938] [D loss: 1.088698, acc: 88%] [G loss: 1.187948]\n",
      "[Epoch 1/200] [Batch 71/938] [D loss: 1.080827, acc: 94%] [G loss: 1.170518]\n",
      "[Epoch 1/200] [Batch 72/938] [D loss: 1.124809, acc: 89%] [G loss: 1.256759]\n",
      "[Epoch 1/200] [Batch 73/938] [D loss: 1.073224, acc: 92%] [G loss: 1.203717]\n",
      "[Epoch 1/200] [Batch 74/938] [D loss: 1.106725, acc: 85%] [G loss: 1.180853]\n",
      "[Epoch 1/200] [Batch 75/938] [D loss: 1.096935, acc: 89%] [G loss: 1.199305]\n",
      "[Epoch 1/200] [Batch 76/938] [D loss: 1.092648, acc: 89%] [G loss: 1.172814]\n",
      "[Epoch 1/200] [Batch 77/938] [D loss: 1.100756, acc: 91%] [G loss: 1.179629]\n",
      "[Epoch 1/200] [Batch 78/938] [D loss: 1.103379, acc: 90%] [G loss: 1.158606]\n",
      "[Epoch 1/200] [Batch 79/938] [D loss: 1.078511, acc: 90%] [G loss: 1.202397]\n",
      "[Epoch 1/200] [Batch 80/938] [D loss: 1.105280, acc: 85%] [G loss: 1.267189]\n",
      "[Epoch 1/200] [Batch 81/938] [D loss: 1.071689, acc: 92%] [G loss: 1.219231]\n",
      "[Epoch 1/200] [Batch 82/938] [D loss: 1.098944, acc: 89%] [G loss: 1.183728]\n",
      "[Epoch 1/200] [Batch 83/938] [D loss: 1.120499, acc: 89%] [G loss: 1.196690]\n",
      "[Epoch 1/200] [Batch 84/938] [D loss: 1.125925, acc: 83%] [G loss: 1.243989]\n",
      "[Epoch 1/200] [Batch 85/938] [D loss: 1.088798, acc: 90%] [G loss: 1.130112]\n",
      "[Epoch 1/200] [Batch 86/938] [D loss: 1.102841, acc: 90%] [G loss: 1.226348]\n",
      "[Epoch 1/200] [Batch 87/938] [D loss: 1.088223, acc: 91%] [G loss: 1.180484]\n",
      "[Epoch 1/200] [Batch 88/938] [D loss: 1.120340, acc: 88%] [G loss: 1.181218]\n",
      "[Epoch 1/200] [Batch 89/938] [D loss: 1.112644, acc: 87%] [G loss: 1.153563]\n",
      "[Epoch 1/200] [Batch 90/938] [D loss: 1.092029, acc: 92%] [G loss: 1.203998]\n",
      "[Epoch 1/200] [Batch 91/938] [D loss: 1.092341, acc: 92%] [G loss: 1.236568]\n",
      "[Epoch 1/200] [Batch 92/938] [D loss: 1.091990, acc: 89%] [G loss: 1.226934]\n",
      "[Epoch 1/200] [Batch 93/938] [D loss: 1.083512, acc: 91%] [G loss: 1.221836]\n",
      "[Epoch 1/200] [Batch 94/938] [D loss: 1.077752, acc: 91%] [G loss: 1.194734]\n",
      "[Epoch 1/200] [Batch 95/938] [D loss: 1.122048, acc: 86%] [G loss: 1.226024]\n",
      "[Epoch 1/200] [Batch 96/938] [D loss: 1.114316, acc: 93%] [G loss: 1.156401]\n",
      "[Epoch 1/200] [Batch 97/938] [D loss: 1.109284, acc: 86%] [G loss: 1.184044]\n",
      "[Epoch 1/200] [Batch 98/938] [D loss: 1.098049, acc: 87%] [G loss: 1.239838]\n",
      "[Epoch 1/200] [Batch 99/938] [D loss: 1.101759, acc: 87%] [G loss: 1.269387]\n",
      "[Epoch 1/200] [Batch 100/938] [D loss: 1.126817, acc: 84%] [G loss: 1.213224]\n",
      "[Epoch 1/200] [Batch 101/938] [D loss: 1.120382, acc: 87%] [G loss: 1.160553]\n",
      "[Epoch 1/200] [Batch 102/938] [D loss: 1.074326, acc: 93%] [G loss: 1.156915]\n",
      "[Epoch 1/200] [Batch 103/938] [D loss: 1.116575, acc: 87%] [G loss: 1.194027]\n",
      "[Epoch 1/200] [Batch 104/938] [D loss: 1.131618, acc: 88%] [G loss: 1.209509]\n",
      "[Epoch 1/200] [Batch 105/938] [D loss: 1.088146, acc: 92%] [G loss: 1.219772]\n",
      "[Epoch 1/200] [Batch 106/938] [D loss: 1.103493, acc: 88%] [G loss: 1.243548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 107/938] [D loss: 1.118294, acc: 88%] [G loss: 1.213712]\n",
      "[Epoch 1/200] [Batch 108/938] [D loss: 1.086601, acc: 90%] [G loss: 1.153314]\n",
      "[Epoch 1/200] [Batch 109/938] [D loss: 1.085255, acc: 92%] [G loss: 1.221283]\n",
      "[Epoch 1/200] [Batch 110/938] [D loss: 1.094269, acc: 88%] [G loss: 1.211517]\n",
      "[Epoch 1/200] [Batch 111/938] [D loss: 1.119561, acc: 88%] [G loss: 1.213824]\n",
      "[Epoch 1/200] [Batch 112/938] [D loss: 1.078109, acc: 93%] [G loss: 1.217906]\n",
      "[Epoch 1/200] [Batch 113/938] [D loss: 1.098314, acc: 91%] [G loss: 1.176881]\n",
      "[Epoch 1/200] [Batch 114/938] [D loss: 1.079640, acc: 90%] [G loss: 1.178744]\n",
      "[Epoch 1/200] [Batch 115/938] [D loss: 1.089525, acc: 92%] [G loss: 1.179978]\n",
      "[Epoch 1/200] [Batch 116/938] [D loss: 1.091253, acc: 92%] [G loss: 1.210203]\n",
      "[Epoch 1/200] [Batch 117/938] [D loss: 1.083983, acc: 89%] [G loss: 1.158182]\n",
      "[Epoch 1/200] [Batch 118/938] [D loss: 1.129988, acc: 85%] [G loss: 1.263620]\n",
      "[Epoch 1/200] [Batch 119/938] [D loss: 1.095906, acc: 88%] [G loss: 1.290810]\n",
      "[Epoch 1/200] [Batch 120/938] [D loss: 1.085726, acc: 92%] [G loss: 1.191748]\n",
      "[Epoch 1/200] [Batch 121/938] [D loss: 1.104454, acc: 90%] [G loss: 1.177933]\n",
      "[Epoch 1/200] [Batch 122/938] [D loss: 1.124700, acc: 88%] [G loss: 1.105049]\n",
      "[Epoch 1/200] [Batch 123/938] [D loss: 1.095486, acc: 85%] [G loss: 1.178446]\n",
      "[Epoch 1/200] [Batch 124/938] [D loss: 1.118781, acc: 88%] [G loss: 1.209080]\n",
      "[Epoch 1/200] [Batch 125/938] [D loss: 1.090843, acc: 90%] [G loss: 1.214848]\n",
      "[Epoch 1/200] [Batch 126/938] [D loss: 1.124196, acc: 87%] [G loss: 1.230687]\n",
      "[Epoch 1/200] [Batch 127/938] [D loss: 1.093815, acc: 90%] [G loss: 1.215171]\n",
      "[Epoch 1/200] [Batch 128/938] [D loss: 1.096099, acc: 92%] [G loss: 1.142410]\n",
      "[Epoch 1/200] [Batch 129/938] [D loss: 1.087192, acc: 85%] [G loss: 1.208882]\n",
      "[Epoch 1/200] [Batch 130/938] [D loss: 1.097351, acc: 89%] [G loss: 1.218440]\n",
      "[Epoch 1/200] [Batch 131/938] [D loss: 1.098835, acc: 90%] [G loss: 1.223585]\n",
      "[Epoch 1/200] [Batch 132/938] [D loss: 1.080876, acc: 89%] [G loss: 1.139536]\n",
      "[Epoch 1/200] [Batch 133/938] [D loss: 1.110389, acc: 85%] [G loss: 1.207038]\n",
      "[Epoch 1/200] [Batch 134/938] [D loss: 1.109803, acc: 92%] [G loss: 1.196683]\n",
      "[Epoch 1/200] [Batch 135/938] [D loss: 1.104927, acc: 87%] [G loss: 1.192561]\n",
      "[Epoch 1/200] [Batch 136/938] [D loss: 1.095517, acc: 89%] [G loss: 1.195433]\n",
      "[Epoch 1/200] [Batch 137/938] [D loss: 1.102979, acc: 89%] [G loss: 1.147431]\n",
      "[Epoch 1/200] [Batch 138/938] [D loss: 1.109675, acc: 89%] [G loss: 1.222681]\n",
      "[Epoch 1/200] [Batch 139/938] [D loss: 1.101919, acc: 88%] [G loss: 1.215269]\n",
      "[Epoch 1/200] [Batch 140/938] [D loss: 1.086094, acc: 92%] [G loss: 1.229862]\n",
      "[Epoch 1/200] [Batch 141/938] [D loss: 1.095917, acc: 89%] [G loss: 1.180096]\n",
      "[Epoch 1/200] [Batch 142/938] [D loss: 1.068085, acc: 90%] [G loss: 1.170911]\n",
      "[Epoch 1/200] [Batch 143/938] [D loss: 1.125001, acc: 85%] [G loss: 1.175313]\n",
      "[Epoch 1/200] [Batch 144/938] [D loss: 1.118177, acc: 90%] [G loss: 1.164586]\n",
      "[Epoch 1/200] [Batch 145/938] [D loss: 1.084807, acc: 92%] [G loss: 1.150409]\n",
      "[Epoch 1/200] [Batch 146/938] [D loss: 1.103533, acc: 90%] [G loss: 1.208825]\n",
      "[Epoch 1/200] [Batch 147/938] [D loss: 1.084471, acc: 89%] [G loss: 1.233891]\n",
      "[Epoch 1/200] [Batch 148/938] [D loss: 1.106471, acc: 89%] [G loss: 1.196617]\n",
      "[Epoch 1/200] [Batch 149/938] [D loss: 1.102543, acc: 91%] [G loss: 1.183827]\n",
      "[Epoch 1/200] [Batch 150/938] [D loss: 1.106045, acc: 90%] [G loss: 1.148178]\n",
      "[Epoch 1/200] [Batch 151/938] [D loss: 1.095137, acc: 89%] [G loss: 1.182908]\n",
      "[Epoch 1/200] [Batch 152/938] [D loss: 1.106848, acc: 92%] [G loss: 1.202912]\n",
      "[Epoch 1/200] [Batch 153/938] [D loss: 1.082550, acc: 90%] [G loss: 1.202262]\n",
      "[Epoch 1/200] [Batch 154/938] [D loss: 1.078858, acc: 92%] [G loss: 1.224321]\n",
      "[Epoch 1/200] [Batch 155/938] [D loss: 1.112120, acc: 88%] [G loss: 1.163852]\n",
      "[Epoch 1/200] [Batch 156/938] [D loss: 1.137431, acc: 85%] [G loss: 1.181860]\n",
      "[Epoch 1/200] [Batch 157/938] [D loss: 1.125719, acc: 88%] [G loss: 1.163357]\n",
      "[Epoch 1/200] [Batch 158/938] [D loss: 1.111627, acc: 88%] [G loss: 1.170833]\n",
      "[Epoch 1/200] [Batch 159/938] [D loss: 1.089082, acc: 92%] [G loss: 1.192285]\n",
      "[Epoch 1/200] [Batch 160/938] [D loss: 1.104368, acc: 89%] [G loss: 1.188064]\n",
      "[Epoch 1/200] [Batch 161/938] [D loss: 1.076047, acc: 95%] [G loss: 1.176237]\n",
      "[Epoch 1/200] [Batch 162/938] [D loss: 1.082721, acc: 96%] [G loss: 1.191203]\n",
      "[Epoch 1/200] [Batch 163/938] [D loss: 1.062256, acc: 93%] [G loss: 1.181622]\n",
      "[Epoch 1/200] [Batch 164/938] [D loss: 1.116712, acc: 91%] [G loss: 1.191267]\n",
      "[Epoch 1/200] [Batch 165/938] [D loss: 1.123959, acc: 89%] [G loss: 1.221593]\n",
      "[Epoch 1/200] [Batch 166/938] [D loss: 1.107309, acc: 89%] [G loss: 1.171578]\n",
      "[Epoch 1/200] [Batch 167/938] [D loss: 1.116455, acc: 88%] [G loss: 1.149956]\n",
      "[Epoch 1/200] [Batch 168/938] [D loss: 1.103765, acc: 89%] [G loss: 1.203910]\n",
      "[Epoch 1/200] [Batch 169/938] [D loss: 1.080055, acc: 94%] [G loss: 1.189938]\n",
      "[Epoch 1/200] [Batch 170/938] [D loss: 1.118191, acc: 90%] [G loss: 1.244707]\n",
      "[Epoch 1/200] [Batch 171/938] [D loss: 1.110962, acc: 89%] [G loss: 1.230103]\n",
      "[Epoch 1/200] [Batch 172/938] [D loss: 1.095083, acc: 90%] [G loss: 1.172088]\n",
      "[Epoch 1/200] [Batch 173/938] [D loss: 1.123922, acc: 90%] [G loss: 1.197482]\n",
      "[Epoch 1/200] [Batch 174/938] [D loss: 1.082196, acc: 95%] [G loss: 1.186896]\n",
      "[Epoch 1/200] [Batch 175/938] [D loss: 1.079293, acc: 88%] [G loss: 1.268779]\n",
      "[Epoch 1/200] [Batch 176/938] [D loss: 1.077354, acc: 90%] [G loss: 1.209169]\n",
      "[Epoch 1/200] [Batch 177/938] [D loss: 1.121030, acc: 85%] [G loss: 1.272187]\n",
      "[Epoch 1/200] [Batch 178/938] [D loss: 1.079015, acc: 88%] [G loss: 1.162737]\n",
      "[Epoch 1/200] [Batch 179/938] [D loss: 1.096022, acc: 89%] [G loss: 1.200043]\n",
      "[Epoch 1/200] [Batch 180/938] [D loss: 1.077060, acc: 89%] [G loss: 1.151318]\n",
      "[Epoch 1/200] [Batch 181/938] [D loss: 1.075438, acc: 91%] [G loss: 1.192802]\n",
      "[Epoch 1/200] [Batch 182/938] [D loss: 1.100144, acc: 89%] [G loss: 1.154278]\n",
      "[Epoch 1/200] [Batch 183/938] [D loss: 1.102847, acc: 89%] [G loss: 1.176130]\n",
      "[Epoch 1/200] [Batch 184/938] [D loss: 1.097745, acc: 89%] [G loss: 1.258211]\n",
      "[Epoch 1/200] [Batch 185/938] [D loss: 1.096233, acc: 88%] [G loss: 1.234353]\n",
      "[Epoch 1/200] [Batch 186/938] [D loss: 1.121910, acc: 85%] [G loss: 1.168340]\n",
      "[Epoch 1/200] [Batch 187/938] [D loss: 1.085487, acc: 92%] [G loss: 1.149830]\n",
      "[Epoch 1/200] [Batch 188/938] [D loss: 1.096943, acc: 90%] [G loss: 1.134030]\n",
      "[Epoch 1/200] [Batch 189/938] [D loss: 1.098987, acc: 89%] [G loss: 1.252623]\n",
      "[Epoch 1/200] [Batch 190/938] [D loss: 1.122925, acc: 89%] [G loss: 1.229542]\n",
      "[Epoch 1/200] [Batch 191/938] [D loss: 1.133816, acc: 86%] [G loss: 1.206595]\n",
      "[Epoch 1/200] [Batch 192/938] [D loss: 1.083625, acc: 92%] [G loss: 1.221373]\n",
      "[Epoch 1/200] [Batch 193/938] [D loss: 1.088023, acc: 91%] [G loss: 1.218872]\n",
      "[Epoch 1/200] [Batch 194/938] [D loss: 1.101443, acc: 87%] [G loss: 1.230033]\n",
      "[Epoch 1/200] [Batch 195/938] [D loss: 1.085809, acc: 93%] [G loss: 1.176858]\n",
      "[Epoch 1/200] [Batch 196/938] [D loss: 1.082698, acc: 89%] [G loss: 1.200677]\n",
      "[Epoch 1/200] [Batch 197/938] [D loss: 1.132732, acc: 85%] [G loss: 1.177309]\n",
      "[Epoch 1/200] [Batch 198/938] [D loss: 1.090748, acc: 87%] [G loss: 1.193325]\n",
      "[Epoch 1/200] [Batch 199/938] [D loss: 1.083669, acc: 91%] [G loss: 1.274924]\n",
      "[Epoch 1/200] [Batch 200/938] [D loss: 1.085537, acc: 91%] [G loss: 1.234395]\n",
      "[Epoch 1/200] [Batch 201/938] [D loss: 1.112209, acc: 87%] [G loss: 1.249800]\n",
      "[Epoch 1/200] [Batch 202/938] [D loss: 1.079914, acc: 89%] [G loss: 1.219665]\n",
      "[Epoch 1/200] [Batch 203/938] [D loss: 1.100558, acc: 96%] [G loss: 1.142474]\n",
      "[Epoch 1/200] [Batch 204/938] [D loss: 1.104303, acc: 92%] [G loss: 1.146625]\n",
      "[Epoch 1/200] [Batch 205/938] [D loss: 1.080436, acc: 93%] [G loss: 1.135431]\n",
      "[Epoch 1/200] [Batch 206/938] [D loss: 1.087768, acc: 92%] [G loss: 1.182123]\n",
      "[Epoch 1/200] [Batch 207/938] [D loss: 1.109720, acc: 88%] [G loss: 1.202929]\n",
      "[Epoch 1/200] [Batch 208/938] [D loss: 1.076622, acc: 92%] [G loss: 1.251056]\n",
      "[Epoch 1/200] [Batch 209/938] [D loss: 1.125337, acc: 86%] [G loss: 1.208920]\n",
      "[Epoch 1/200] [Batch 210/938] [D loss: 1.085075, acc: 90%] [G loss: 1.250257]\n",
      "[Epoch 1/200] [Batch 211/938] [D loss: 1.112637, acc: 87%] [G loss: 1.200459]\n",
      "[Epoch 1/200] [Batch 212/938] [D loss: 1.105223, acc: 93%] [G loss: 1.195036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 213/938] [D loss: 1.126500, acc: 91%] [G loss: 1.195155]\n",
      "[Epoch 1/200] [Batch 214/938] [D loss: 1.087652, acc: 92%] [G loss: 1.222519]\n",
      "[Epoch 1/200] [Batch 215/938] [D loss: 1.085091, acc: 89%] [G loss: 1.223668]\n",
      "[Epoch 1/200] [Batch 216/938] [D loss: 1.089824, acc: 91%] [G loss: 1.152036]\n",
      "[Epoch 1/200] [Batch 217/938] [D loss: 1.083424, acc: 92%] [G loss: 1.153476]\n",
      "[Epoch 1/200] [Batch 218/938] [D loss: 1.089445, acc: 91%] [G loss: 1.202356]\n",
      "[Epoch 1/200] [Batch 219/938] [D loss: 1.097722, acc: 86%] [G loss: 1.155118]\n",
      "[Epoch 1/200] [Batch 220/938] [D loss: 1.075624, acc: 95%] [G loss: 1.204418]\n",
      "[Epoch 1/200] [Batch 221/938] [D loss: 1.104521, acc: 88%] [G loss: 1.273599]\n",
      "[Epoch 1/200] [Batch 222/938] [D loss: 1.097127, acc: 90%] [G loss: 1.199897]\n",
      "[Epoch 1/200] [Batch 223/938] [D loss: 1.086652, acc: 88%] [G loss: 1.191427]\n",
      "[Epoch 1/200] [Batch 224/938] [D loss: 1.074934, acc: 87%] [G loss: 1.219679]\n",
      "[Epoch 1/200] [Batch 225/938] [D loss: 1.085733, acc: 91%] [G loss: 1.178772]\n",
      "[Epoch 1/200] [Batch 226/938] [D loss: 1.119306, acc: 90%] [G loss: 1.181567]\n",
      "[Epoch 1/200] [Batch 227/938] [D loss: 1.109983, acc: 88%] [G loss: 1.217889]\n",
      "[Epoch 1/200] [Batch 228/938] [D loss: 1.100746, acc: 91%] [G loss: 1.180550]\n",
      "[Epoch 1/200] [Batch 229/938] [D loss: 1.067920, acc: 93%] [G loss: 1.159632]\n",
      "[Epoch 1/200] [Batch 230/938] [D loss: 1.115745, acc: 92%] [G loss: 1.165680]\n",
      "[Epoch 1/200] [Batch 231/938] [D loss: 1.110218, acc: 86%] [G loss: 1.229874]\n",
      "[Epoch 1/200] [Batch 232/938] [D loss: 1.151637, acc: 85%] [G loss: 1.181082]\n",
      "[Epoch 1/200] [Batch 233/938] [D loss: 1.101686, acc: 88%] [G loss: 1.202392]\n",
      "[Epoch 1/200] [Batch 234/938] [D loss: 1.102553, acc: 89%] [G loss: 1.204412]\n",
      "[Epoch 1/200] [Batch 235/938] [D loss: 1.083333, acc: 94%] [G loss: 1.162788]\n",
      "[Epoch 1/200] [Batch 236/938] [D loss: 1.129077, acc: 90%] [G loss: 1.226925]\n",
      "[Epoch 1/200] [Batch 237/938] [D loss: 1.102819, acc: 89%] [G loss: 1.216753]\n",
      "[Epoch 1/200] [Batch 238/938] [D loss: 1.104727, acc: 89%] [G loss: 1.198940]\n",
      "[Epoch 1/200] [Batch 239/938] [D loss: 1.128469, acc: 89%] [G loss: 1.124625]\n",
      "[Epoch 1/200] [Batch 240/938] [D loss: 1.076796, acc: 92%] [G loss: 1.223568]\n",
      "[Epoch 1/200] [Batch 241/938] [D loss: 1.078762, acc: 94%] [G loss: 1.228519]\n",
      "[Epoch 1/200] [Batch 242/938] [D loss: 1.094127, acc: 92%] [G loss: 1.213107]\n",
      "[Epoch 1/200] [Batch 243/938] [D loss: 1.100441, acc: 91%] [G loss: 1.224007]\n",
      "[Epoch 1/200] [Batch 244/938] [D loss: 1.107247, acc: 88%] [G loss: 1.204541]\n",
      "[Epoch 1/200] [Batch 245/938] [D loss: 1.063854, acc: 90%] [G loss: 1.189799]\n",
      "[Epoch 1/200] [Batch 246/938] [D loss: 1.083485, acc: 90%] [G loss: 1.204497]\n",
      "[Epoch 1/200] [Batch 247/938] [D loss: 1.089066, acc: 85%] [G loss: 1.166071]\n",
      "[Epoch 1/200] [Batch 248/938] [D loss: 1.096223, acc: 91%] [G loss: 1.248439]\n",
      "[Epoch 1/200] [Batch 249/938] [D loss: 1.090205, acc: 89%] [G loss: 1.207647]\n",
      "[Epoch 1/200] [Batch 250/938] [D loss: 1.116679, acc: 88%] [G loss: 1.220188]\n",
      "[Epoch 1/200] [Batch 251/938] [D loss: 1.105791, acc: 92%] [G loss: 1.182887]\n",
      "[Epoch 1/200] [Batch 252/938] [D loss: 1.074439, acc: 92%] [G loss: 1.245065]\n",
      "[Epoch 1/200] [Batch 253/938] [D loss: 1.082587, acc: 92%] [G loss: 1.239467]\n",
      "[Epoch 1/200] [Batch 254/938] [D loss: 1.091794, acc: 92%] [G loss: 1.222552]\n",
      "[Epoch 1/200] [Batch 255/938] [D loss: 1.075526, acc: 92%] [G loss: 1.221062]\n",
      "[Epoch 1/200] [Batch 256/938] [D loss: 1.110181, acc: 90%] [G loss: 1.188687]\n",
      "[Epoch 1/200] [Batch 257/938] [D loss: 1.115908, acc: 91%] [G loss: 1.177206]\n",
      "[Epoch 1/200] [Batch 258/938] [D loss: 1.111665, acc: 89%] [G loss: 1.214061]\n",
      "[Epoch 1/200] [Batch 259/938] [D loss: 1.052648, acc: 91%] [G loss: 1.214968]\n",
      "[Epoch 1/200] [Batch 260/938] [D loss: 1.041466, acc: 93%] [G loss: 1.205723]\n",
      "[Epoch 1/200] [Batch 261/938] [D loss: 1.088132, acc: 92%] [G loss: 1.174550]\n",
      "[Epoch 1/200] [Batch 262/938] [D loss: 1.113530, acc: 88%] [G loss: 1.202953]\n",
      "[Epoch 1/200] [Batch 263/938] [D loss: 1.115025, acc: 84%] [G loss: 1.227607]\n",
      "[Epoch 1/200] [Batch 264/938] [D loss: 1.079970, acc: 90%] [G loss: 1.164663]\n",
      "[Epoch 1/200] [Batch 265/938] [D loss: 1.094947, acc: 92%] [G loss: 1.178954]\n",
      "[Epoch 1/200] [Batch 266/938] [D loss: 1.087729, acc: 92%] [G loss: 1.225494]\n",
      "[Epoch 1/200] [Batch 267/938] [D loss: 1.111821, acc: 87%] [G loss: 1.175707]\n",
      "[Epoch 1/200] [Batch 268/938] [D loss: 1.098320, acc: 89%] [G loss: 1.159226]\n",
      "[Epoch 1/200] [Batch 269/938] [D loss: 1.110204, acc: 88%] [G loss: 1.239190]\n",
      "[Epoch 1/200] [Batch 270/938] [D loss: 1.112659, acc: 91%] [G loss: 1.186686]\n",
      "[Epoch 1/200] [Batch 271/938] [D loss: 1.074332, acc: 91%] [G loss: 1.221522]\n",
      "[Epoch 1/200] [Batch 272/938] [D loss: 1.091807, acc: 88%] [G loss: 1.250220]\n",
      "[Epoch 1/200] [Batch 273/938] [D loss: 1.091802, acc: 89%] [G loss: 1.240999]\n",
      "[Epoch 1/200] [Batch 274/938] [D loss: 1.108575, acc: 90%] [G loss: 1.175693]\n",
      "[Epoch 1/200] [Batch 275/938] [D loss: 1.097772, acc: 91%] [G loss: 1.190408]\n",
      "[Epoch 1/200] [Batch 276/938] [D loss: 1.121038, acc: 92%] [G loss: 1.230444]\n",
      "[Epoch 1/200] [Batch 277/938] [D loss: 1.091025, acc: 91%] [G loss: 1.185137]\n",
      "[Epoch 1/200] [Batch 278/938] [D loss: 1.101397, acc: 88%] [G loss: 1.206782]\n",
      "[Epoch 1/200] [Batch 279/938] [D loss: 1.078703, acc: 92%] [G loss: 1.185497]\n",
      "[Epoch 1/200] [Batch 280/938] [D loss: 1.125727, acc: 88%] [G loss: 1.241625]\n",
      "[Epoch 1/200] [Batch 281/938] [D loss: 1.078327, acc: 92%] [G loss: 1.222568]\n",
      "[Epoch 1/200] [Batch 282/938] [D loss: 1.122578, acc: 89%] [G loss: 1.215401]\n",
      "[Epoch 1/200] [Batch 283/938] [D loss: 1.082088, acc: 89%] [G loss: 1.199712]\n",
      "[Epoch 1/200] [Batch 284/938] [D loss: 1.139277, acc: 84%] [G loss: 1.180898]\n",
      "[Epoch 1/200] [Batch 285/938] [D loss: 1.092241, acc: 92%] [G loss: 1.186186]\n",
      "[Epoch 1/200] [Batch 286/938] [D loss: 1.098184, acc: 93%] [G loss: 1.210864]\n",
      "[Epoch 1/200] [Batch 287/938] [D loss: 1.139555, acc: 89%] [G loss: 1.260109]\n",
      "[Epoch 1/200] [Batch 288/938] [D loss: 1.103282, acc: 92%] [G loss: 1.236286]\n",
      "[Epoch 1/200] [Batch 289/938] [D loss: 1.097567, acc: 84%] [G loss: 1.270978]\n",
      "[Epoch 1/200] [Batch 290/938] [D loss: 1.080361, acc: 91%] [G loss: 1.212740]\n",
      "[Epoch 1/200] [Batch 291/938] [D loss: 1.122744, acc: 89%] [G loss: 1.165966]\n",
      "[Epoch 1/200] [Batch 292/938] [D loss: 1.077137, acc: 88%] [G loss: 1.238090]\n",
      "[Epoch 1/200] [Batch 293/938] [D loss: 1.095367, acc: 90%] [G loss: 1.175612]\n",
      "[Epoch 1/200] [Batch 294/938] [D loss: 1.066885, acc: 92%] [G loss: 1.164750]\n",
      "[Epoch 1/200] [Batch 295/938] [D loss: 1.106938, acc: 85%] [G loss: 1.233226]\n",
      "[Epoch 1/200] [Batch 296/938] [D loss: 1.122220, acc: 85%] [G loss: 1.168713]\n",
      "[Epoch 1/200] [Batch 297/938] [D loss: 1.113955, acc: 88%] [G loss: 1.222614]\n",
      "[Epoch 1/200] [Batch 298/938] [D loss: 1.069646, acc: 92%] [G loss: 1.259801]\n",
      "[Epoch 1/200] [Batch 299/938] [D loss: 1.072581, acc: 93%] [G loss: 1.185406]\n",
      "[Epoch 1/200] [Batch 300/938] [D loss: 1.094154, acc: 91%] [G loss: 1.194906]\n",
      "[Epoch 1/200] [Batch 301/938] [D loss: 1.077573, acc: 91%] [G loss: 1.200420]\n",
      "[Epoch 1/200] [Batch 302/938] [D loss: 1.095501, acc: 88%] [G loss: 1.187329]\n",
      "[Epoch 1/200] [Batch 303/938] [D loss: 1.038730, acc: 96%] [G loss: 1.185351]\n",
      "[Epoch 1/200] [Batch 304/938] [D loss: 1.080997, acc: 89%] [G loss: 1.188442]\n",
      "[Epoch 1/200] [Batch 305/938] [D loss: 1.073998, acc: 92%] [G loss: 1.255647]\n",
      "[Epoch 1/200] [Batch 306/938] [D loss: 1.118456, acc: 86%] [G loss: 1.221239]\n",
      "[Epoch 1/200] [Batch 307/938] [D loss: 1.091895, acc: 88%] [G loss: 1.203703]\n",
      "[Epoch 1/200] [Batch 308/938] [D loss: 1.083605, acc: 90%] [G loss: 1.183457]\n",
      "[Epoch 1/200] [Batch 309/938] [D loss: 1.101129, acc: 89%] [G loss: 1.222080]\n",
      "[Epoch 1/200] [Batch 310/938] [D loss: 1.062606, acc: 92%] [G loss: 1.180432]\n",
      "[Epoch 1/200] [Batch 311/938] [D loss: 1.135723, acc: 88%] [G loss: 1.157121]\n",
      "[Epoch 1/200] [Batch 312/938] [D loss: 1.084212, acc: 91%] [G loss: 1.203192]\n",
      "[Epoch 1/200] [Batch 313/938] [D loss: 1.105351, acc: 90%] [G loss: 1.181038]\n",
      "[Epoch 1/200] [Batch 314/938] [D loss: 1.100299, acc: 90%] [G loss: 1.247747]\n",
      "[Epoch 1/200] [Batch 315/938] [D loss: 1.122211, acc: 89%] [G loss: 1.208590]\n",
      "[Epoch 1/200] [Batch 316/938] [D loss: 1.105385, acc: 91%] [G loss: 1.221567]\n",
      "[Epoch 1/200] [Batch 317/938] [D loss: 1.094539, acc: 92%] [G loss: 1.163092]\n",
      "[Epoch 1/200] [Batch 318/938] [D loss: 1.097466, acc: 90%] [G loss: 1.157001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 319/938] [D loss: 1.079158, acc: 90%] [G loss: 1.185488]\n",
      "[Epoch 1/200] [Batch 320/938] [D loss: 1.136659, acc: 86%] [G loss: 1.197274]\n",
      "[Epoch 1/200] [Batch 321/938] [D loss: 1.100170, acc: 90%] [G loss: 1.201092]\n",
      "[Epoch 1/200] [Batch 322/938] [D loss: 1.097133, acc: 95%] [G loss: 1.210666]\n",
      "[Epoch 1/200] [Batch 323/938] [D loss: 1.098103, acc: 92%] [G loss: 1.158438]\n",
      "[Epoch 1/200] [Batch 324/938] [D loss: 1.109005, acc: 89%] [G loss: 1.237116]\n",
      "[Epoch 1/200] [Batch 325/938] [D loss: 1.111045, acc: 91%] [G loss: 1.187021]\n",
      "[Epoch 1/200] [Batch 326/938] [D loss: 1.113437, acc: 91%] [G loss: 1.155132]\n",
      "[Epoch 1/200] [Batch 327/938] [D loss: 1.080951, acc: 86%] [G loss: 1.321503]\n",
      "[Epoch 1/200] [Batch 328/938] [D loss: 1.084764, acc: 90%] [G loss: 1.153313]\n",
      "[Epoch 1/200] [Batch 329/938] [D loss: 1.104710, acc: 91%] [G loss: 1.187880]\n",
      "[Epoch 1/200] [Batch 330/938] [D loss: 1.055664, acc: 93%] [G loss: 1.211578]\n",
      "[Epoch 1/200] [Batch 331/938] [D loss: 1.129701, acc: 87%] [G loss: 1.189942]\n",
      "[Epoch 1/200] [Batch 332/938] [D loss: 1.086614, acc: 89%] [G loss: 1.282065]\n",
      "[Epoch 1/200] [Batch 333/938] [D loss: 1.080157, acc: 91%] [G loss: 1.257958]\n",
      "[Epoch 1/200] [Batch 334/938] [D loss: 1.117336, acc: 91%] [G loss: 1.172695]\n",
      "[Epoch 1/200] [Batch 335/938] [D loss: 1.113982, acc: 89%] [G loss: 1.220885]\n",
      "[Epoch 1/200] [Batch 336/938] [D loss: 1.081506, acc: 90%] [G loss: 1.166716]\n",
      "[Epoch 1/200] [Batch 337/938] [D loss: 1.107456, acc: 89%] [G loss: 1.241693]\n",
      "[Epoch 1/200] [Batch 338/938] [D loss: 1.103753, acc: 97%] [G loss: 1.154267]\n",
      "[Epoch 1/200] [Batch 339/938] [D loss: 1.101849, acc: 89%] [G loss: 1.233714]\n",
      "[Epoch 1/200] [Batch 340/938] [D loss: 1.087711, acc: 91%] [G loss: 1.138731]\n",
      "[Epoch 1/200] [Batch 341/938] [D loss: 1.113366, acc: 90%] [G loss: 1.167496]\n",
      "[Epoch 1/200] [Batch 342/938] [D loss: 1.134756, acc: 84%] [G loss: 1.172916]\n",
      "[Epoch 1/200] [Batch 343/938] [D loss: 1.068753, acc: 94%] [G loss: 1.202397]\n",
      "[Epoch 1/200] [Batch 344/938] [D loss: 1.079325, acc: 91%] [G loss: 1.209894]\n",
      "[Epoch 1/200] [Batch 345/938] [D loss: 1.078829, acc: 89%] [G loss: 1.222401]\n",
      "[Epoch 1/200] [Batch 346/938] [D loss: 1.106327, acc: 93%] [G loss: 1.161232]\n",
      "[Epoch 1/200] [Batch 347/938] [D loss: 1.082448, acc: 88%] [G loss: 1.180893]\n",
      "[Epoch 1/200] [Batch 348/938] [D loss: 1.095067, acc: 89%] [G loss: 1.202825]\n",
      "[Epoch 1/200] [Batch 349/938] [D loss: 1.112075, acc: 92%] [G loss: 1.229061]\n",
      "[Epoch 1/200] [Batch 350/938] [D loss: 1.099072, acc: 88%] [G loss: 1.214173]\n",
      "[Epoch 1/200] [Batch 351/938] [D loss: 1.109188, acc: 89%] [G loss: 1.181774]\n",
      "[Epoch 1/200] [Batch 352/938] [D loss: 1.086540, acc: 92%] [G loss: 1.201176]\n",
      "[Epoch 1/200] [Batch 353/938] [D loss: 1.075239, acc: 92%] [G loss: 1.185838]\n",
      "[Epoch 1/200] [Batch 354/938] [D loss: 1.074202, acc: 90%] [G loss: 1.225375]\n",
      "[Epoch 1/200] [Batch 355/938] [D loss: 1.098869, acc: 90%] [G loss: 1.156972]\n",
      "[Epoch 1/200] [Batch 356/938] [D loss: 1.088943, acc: 92%] [G loss: 1.150637]\n",
      "[Epoch 1/200] [Batch 357/938] [D loss: 1.122330, acc: 92%] [G loss: 1.171138]\n",
      "[Epoch 1/200] [Batch 358/938] [D loss: 1.111587, acc: 89%] [G loss: 1.195764]\n",
      "[Epoch 1/200] [Batch 359/938] [D loss: 1.079445, acc: 92%] [G loss: 1.176912]\n",
      "[Epoch 1/200] [Batch 360/938] [D loss: 1.107560, acc: 90%] [G loss: 1.164280]\n",
      "[Epoch 1/200] [Batch 361/938] [D loss: 1.068976, acc: 91%] [G loss: 1.236612]\n",
      "[Epoch 1/200] [Batch 362/938] [D loss: 1.102588, acc: 90%] [G loss: 1.177511]\n",
      "[Epoch 1/200] [Batch 363/938] [D loss: 1.127822, acc: 92%] [G loss: 1.199725]\n",
      "[Epoch 1/200] [Batch 364/938] [D loss: 1.082870, acc: 91%] [G loss: 1.236672]\n",
      "[Epoch 1/200] [Batch 365/938] [D loss: 1.097538, acc: 89%] [G loss: 1.153585]\n",
      "[Epoch 1/200] [Batch 366/938] [D loss: 1.095060, acc: 89%] [G loss: 1.139012]\n",
      "[Epoch 1/200] [Batch 367/938] [D loss: 1.120381, acc: 90%] [G loss: 1.168924]\n",
      "[Epoch 1/200] [Batch 368/938] [D loss: 1.106551, acc: 91%] [G loss: 1.180411]\n",
      "[Epoch 1/200] [Batch 369/938] [D loss: 1.113015, acc: 89%] [G loss: 1.238748]\n",
      "[Epoch 1/200] [Batch 370/938] [D loss: 1.091374, acc: 89%] [G loss: 1.213415]\n",
      "[Epoch 1/200] [Batch 371/938] [D loss: 1.102505, acc: 86%] [G loss: 1.138205]\n",
      "[Epoch 1/200] [Batch 372/938] [D loss: 1.106047, acc: 86%] [G loss: 1.194711]\n",
      "[Epoch 1/200] [Batch 373/938] [D loss: 1.076221, acc: 94%] [G loss: 1.180554]\n",
      "[Epoch 1/200] [Batch 374/938] [D loss: 1.100865, acc: 93%] [G loss: 1.152525]\n",
      "[Epoch 1/200] [Batch 375/938] [D loss: 1.073587, acc: 93%] [G loss: 1.172904]\n",
      "[Epoch 1/200] [Batch 376/938] [D loss: 1.103575, acc: 88%] [G loss: 1.178431]\n",
      "[Epoch 1/200] [Batch 377/938] [D loss: 1.090548, acc: 89%] [G loss: 1.235523]\n",
      "[Epoch 1/200] [Batch 378/938] [D loss: 1.088460, acc: 92%] [G loss: 1.158491]\n",
      "[Epoch 1/200] [Batch 379/938] [D loss: 1.076759, acc: 91%] [G loss: 1.210817]\n",
      "[Epoch 1/200] [Batch 380/938] [D loss: 1.098303, acc: 89%] [G loss: 1.225310]\n",
      "[Epoch 1/200] [Batch 381/938] [D loss: 1.111051, acc: 90%] [G loss: 1.262220]\n",
      "[Epoch 1/200] [Batch 382/938] [D loss: 1.103383, acc: 92%] [G loss: 1.138079]\n",
      "[Epoch 1/200] [Batch 383/938] [D loss: 1.068954, acc: 93%] [G loss: 1.186035]\n",
      "[Epoch 1/200] [Batch 384/938] [D loss: 1.078825, acc: 91%] [G loss: 1.167477]\n",
      "[Epoch 1/200] [Batch 385/938] [D loss: 1.096468, acc: 93%] [G loss: 1.170266]\n",
      "[Epoch 1/200] [Batch 386/938] [D loss: 1.076169, acc: 89%] [G loss: 1.182640]\n",
      "[Epoch 1/200] [Batch 387/938] [D loss: 1.105671, acc: 89%] [G loss: 1.196023]\n",
      "[Epoch 1/200] [Batch 388/938] [D loss: 1.093034, acc: 85%] [G loss: 1.280842]\n",
      "[Epoch 1/200] [Batch 389/938] [D loss: 1.111288, acc: 89%] [G loss: 1.224724]\n",
      "[Epoch 1/200] [Batch 390/938] [D loss: 1.107448, acc: 89%] [G loss: 1.178723]\n",
      "[Epoch 1/200] [Batch 391/938] [D loss: 1.125829, acc: 90%] [G loss: 1.185984]\n",
      "[Epoch 1/200] [Batch 392/938] [D loss: 1.117264, acc: 89%] [G loss: 1.257011]\n",
      "[Epoch 1/200] [Batch 393/938] [D loss: 1.102710, acc: 92%] [G loss: 1.259520]\n",
      "[Epoch 1/200] [Batch 394/938] [D loss: 1.070736, acc: 94%] [G loss: 1.216907]\n",
      "[Epoch 1/200] [Batch 395/938] [D loss: 1.100151, acc: 91%] [G loss: 1.268716]\n",
      "[Epoch 1/200] [Batch 396/938] [D loss: 1.079917, acc: 92%] [G loss: 1.224585]\n",
      "[Epoch 1/200] [Batch 397/938] [D loss: 1.116691, acc: 87%] [G loss: 1.151629]\n",
      "[Epoch 1/200] [Batch 398/938] [D loss: 1.075222, acc: 95%] [G loss: 1.146124]\n",
      "[Epoch 1/200] [Batch 399/938] [D loss: 1.070505, acc: 94%] [G loss: 1.190278]\n",
      "[Epoch 1/200] [Batch 400/938] [D loss: 1.083692, acc: 92%] [G loss: 1.218078]\n",
      "[Epoch 1/200] [Batch 401/938] [D loss: 1.086368, acc: 90%] [G loss: 1.191340]\n",
      "[Epoch 1/200] [Batch 402/938] [D loss: 1.078692, acc: 92%] [G loss: 1.231776]\n",
      "[Epoch 1/200] [Batch 403/938] [D loss: 1.096116, acc: 89%] [G loss: 1.134342]\n",
      "[Epoch 1/200] [Batch 404/938] [D loss: 1.109868, acc: 92%] [G loss: 1.188728]\n",
      "[Epoch 1/200] [Batch 405/938] [D loss: 1.087858, acc: 94%] [G loss: 1.152108]\n",
      "[Epoch 1/200] [Batch 406/938] [D loss: 1.091859, acc: 92%] [G loss: 1.131644]\n",
      "[Epoch 1/200] [Batch 407/938] [D loss: 1.114357, acc: 87%] [G loss: 1.233513]\n",
      "[Epoch 1/200] [Batch 408/938] [D loss: 1.124514, acc: 88%] [G loss: 1.183083]\n",
      "[Epoch 1/200] [Batch 409/938] [D loss: 1.128094, acc: 86%] [G loss: 1.195055]\n",
      "[Epoch 1/200] [Batch 410/938] [D loss: 1.099660, acc: 89%] [G loss: 1.234261]\n",
      "[Epoch 1/200] [Batch 411/938] [D loss: 1.119869, acc: 88%] [G loss: 1.140062]\n",
      "[Epoch 1/200] [Batch 412/938] [D loss: 1.096252, acc: 90%] [G loss: 1.180488]\n",
      "[Epoch 1/200] [Batch 413/938] [D loss: 1.109473, acc: 93%] [G loss: 1.186607]\n",
      "[Epoch 1/200] [Batch 414/938] [D loss: 1.066363, acc: 91%] [G loss: 1.157483]\n",
      "[Epoch 1/200] [Batch 415/938] [D loss: 1.112404, acc: 92%] [G loss: 1.142067]\n",
      "[Epoch 1/200] [Batch 416/938] [D loss: 1.089718, acc: 95%] [G loss: 1.133616]\n",
      "[Epoch 1/200] [Batch 417/938] [D loss: 1.075593, acc: 90%] [G loss: 1.169608]\n",
      "[Epoch 1/200] [Batch 418/938] [D loss: 1.107213, acc: 92%] [G loss: 1.264940]\n",
      "[Epoch 1/200] [Batch 419/938] [D loss: 1.082339, acc: 95%] [G loss: 1.196881]\n",
      "[Epoch 1/200] [Batch 420/938] [D loss: 1.083321, acc: 92%] [G loss: 1.180670]\n",
      "[Epoch 1/200] [Batch 421/938] [D loss: 1.085046, acc: 89%] [G loss: 1.199128]\n",
      "[Epoch 1/200] [Batch 422/938] [D loss: 1.061015, acc: 92%] [G loss: 1.162331]\n",
      "[Epoch 1/200] [Batch 423/938] [D loss: 1.083343, acc: 95%] [G loss: 1.143462]\n",
      "[Epoch 1/200] [Batch 424/938] [D loss: 1.072033, acc: 94%] [G loss: 1.233368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 425/938] [D loss: 1.097249, acc: 93%] [G loss: 1.156172]\n",
      "[Epoch 1/200] [Batch 426/938] [D loss: 1.061542, acc: 94%] [G loss: 1.236007]\n",
      "[Epoch 1/200] [Batch 427/938] [D loss: 1.095869, acc: 91%] [G loss: 1.183015]\n",
      "[Epoch 1/200] [Batch 428/938] [D loss: 1.076193, acc: 92%] [G loss: 1.210893]\n",
      "[Epoch 1/200] [Batch 429/938] [D loss: 1.087466, acc: 89%] [G loss: 1.174688]\n",
      "[Epoch 1/200] [Batch 430/938] [D loss: 1.103849, acc: 91%] [G loss: 1.152931]\n",
      "[Epoch 1/200] [Batch 431/938] [D loss: 1.110655, acc: 87%] [G loss: 1.235700]\n",
      "[Epoch 1/200] [Batch 432/938] [D loss: 1.118982, acc: 89%] [G loss: 1.199499]\n",
      "[Epoch 1/200] [Batch 433/938] [D loss: 1.089636, acc: 89%] [G loss: 1.228027]\n",
      "[Epoch 1/200] [Batch 434/938] [D loss: 1.067730, acc: 92%] [G loss: 1.205976]\n",
      "[Epoch 1/200] [Batch 435/938] [D loss: 1.117380, acc: 92%] [G loss: 1.206519]\n",
      "[Epoch 1/200] [Batch 436/938] [D loss: 1.087621, acc: 89%] [G loss: 1.181275]\n",
      "[Epoch 1/200] [Batch 437/938] [D loss: 1.070878, acc: 90%] [G loss: 1.153285]\n",
      "[Epoch 1/200] [Batch 438/938] [D loss: 1.067376, acc: 93%] [G loss: 1.228545]\n",
      "[Epoch 1/200] [Batch 439/938] [D loss: 1.065716, acc: 91%] [G loss: 1.162320]\n",
      "[Epoch 1/200] [Batch 440/938] [D loss: 1.116340, acc: 85%] [G loss: 1.178655]\n",
      "[Epoch 1/200] [Batch 441/938] [D loss: 1.093525, acc: 90%] [G loss: 1.182298]\n",
      "[Epoch 1/200] [Batch 442/938] [D loss: 1.117784, acc: 87%] [G loss: 1.186585]\n",
      "[Epoch 1/200] [Batch 443/938] [D loss: 1.091907, acc: 92%] [G loss: 1.210563]\n",
      "[Epoch 1/200] [Batch 444/938] [D loss: 1.081551, acc: 90%] [G loss: 1.228110]\n",
      "[Epoch 1/200] [Batch 445/938] [D loss: 1.074047, acc: 90%] [G loss: 1.205840]\n",
      "[Epoch 1/200] [Batch 446/938] [D loss: 1.105707, acc: 92%] [G loss: 1.171004]\n",
      "[Epoch 1/200] [Batch 447/938] [D loss: 1.095186, acc: 92%] [G loss: 1.112905]\n",
      "[Epoch 1/200] [Batch 448/938] [D loss: 1.072107, acc: 92%] [G loss: 1.210120]\n",
      "[Epoch 1/200] [Batch 449/938] [D loss: 1.115384, acc: 88%] [G loss: 1.265658]\n",
      "[Epoch 1/200] [Batch 450/938] [D loss: 1.130923, acc: 92%] [G loss: 1.217312]\n",
      "[Epoch 1/200] [Batch 451/938] [D loss: 1.073065, acc: 88%] [G loss: 1.278998]\n",
      "[Epoch 1/200] [Batch 452/938] [D loss: 1.129501, acc: 88%] [G loss: 1.179254]\n",
      "[Epoch 1/200] [Batch 453/938] [D loss: 1.066437, acc: 89%] [G loss: 1.157091]\n",
      "[Epoch 1/200] [Batch 454/938] [D loss: 1.107519, acc: 91%] [G loss: 1.217070]\n",
      "[Epoch 1/200] [Batch 455/938] [D loss: 1.111908, acc: 89%] [G loss: 1.207109]\n",
      "[Epoch 1/200] [Batch 456/938] [D loss: 1.082766, acc: 92%] [G loss: 1.213328]\n",
      "[Epoch 1/200] [Batch 457/938] [D loss: 1.080200, acc: 91%] [G loss: 1.233955]\n",
      "[Epoch 1/200] [Batch 458/938] [D loss: 1.138106, acc: 89%] [G loss: 1.176805]\n",
      "[Epoch 1/200] [Batch 459/938] [D loss: 1.080697, acc: 91%] [G loss: 1.188366]\n",
      "[Epoch 1/200] [Batch 460/938] [D loss: 1.063728, acc: 96%] [G loss: 1.186298]\n",
      "[Epoch 1/200] [Batch 461/938] [D loss: 1.097338, acc: 90%] [G loss: 1.160825]\n",
      "[Epoch 1/200] [Batch 462/938] [D loss: 1.104651, acc: 92%] [G loss: 1.195351]\n",
      "[Epoch 1/200] [Batch 463/938] [D loss: 1.059270, acc: 91%] [G loss: 1.283538]\n",
      "[Epoch 1/200] [Batch 464/938] [D loss: 1.093330, acc: 92%] [G loss: 1.189435]\n",
      "[Epoch 1/200] [Batch 465/938] [D loss: 1.063013, acc: 96%] [G loss: 1.198755]\n",
      "[Epoch 1/200] [Batch 466/938] [D loss: 1.073321, acc: 89%] [G loss: 1.237265]\n",
      "[Epoch 1/200] [Batch 467/938] [D loss: 1.100685, acc: 88%] [G loss: 1.206063]\n",
      "[Epoch 1/200] [Batch 468/938] [D loss: 1.115314, acc: 86%] [G loss: 1.177558]\n",
      "[Epoch 1/200] [Batch 469/938] [D loss: 1.074985, acc: 91%] [G loss: 1.162791]\n",
      "[Epoch 1/200] [Batch 470/938] [D loss: 1.064242, acc: 96%] [G loss: 1.204782]\n",
      "[Epoch 1/200] [Batch 471/938] [D loss: 1.063375, acc: 93%] [G loss: 1.166749]\n",
      "[Epoch 1/200] [Batch 472/938] [D loss: 1.082728, acc: 93%] [G loss: 1.222059]\n",
      "[Epoch 1/200] [Batch 473/938] [D loss: 1.086624, acc: 92%] [G loss: 1.183443]\n",
      "[Epoch 1/200] [Batch 474/938] [D loss: 1.085751, acc: 89%] [G loss: 1.155605]\n",
      "[Epoch 1/200] [Batch 475/938] [D loss: 1.082751, acc: 91%] [G loss: 1.149103]\n",
      "[Epoch 1/200] [Batch 476/938] [D loss: 1.065591, acc: 92%] [G loss: 1.224193]\n",
      "[Epoch 1/200] [Batch 477/938] [D loss: 1.097485, acc: 90%] [G loss: 1.240838]\n",
      "[Epoch 1/200] [Batch 478/938] [D loss: 1.092594, acc: 90%] [G loss: 1.269727]\n",
      "[Epoch 1/200] [Batch 479/938] [D loss: 1.107947, acc: 91%] [G loss: 1.115004]\n",
      "[Epoch 1/200] [Batch 480/938] [D loss: 1.079953, acc: 92%] [G loss: 1.155516]\n",
      "[Epoch 1/200] [Batch 481/938] [D loss: 1.115597, acc: 91%] [G loss: 1.167332]\n",
      "[Epoch 1/200] [Batch 482/938] [D loss: 1.088048, acc: 96%] [G loss: 1.182716]\n",
      "[Epoch 1/200] [Batch 483/938] [D loss: 1.068747, acc: 90%] [G loss: 1.196113]\n",
      "[Epoch 1/200] [Batch 484/938] [D loss: 1.121221, acc: 86%] [G loss: 1.262735]\n",
      "[Epoch 1/200] [Batch 485/938] [D loss: 1.107485, acc: 89%] [G loss: 1.206756]\n",
      "[Epoch 1/200] [Batch 486/938] [D loss: 1.065836, acc: 94%] [G loss: 1.202246]\n",
      "[Epoch 1/200] [Batch 487/938] [D loss: 1.120798, acc: 85%] [G loss: 1.171903]\n",
      "[Epoch 1/200] [Batch 488/938] [D loss: 1.110577, acc: 89%] [G loss: 1.180307]\n",
      "[Epoch 1/200] [Batch 489/938] [D loss: 1.091928, acc: 96%] [G loss: 1.119360]\n",
      "[Epoch 1/200] [Batch 490/938] [D loss: 1.125645, acc: 86%] [G loss: 1.153972]\n",
      "[Epoch 1/200] [Batch 491/938] [D loss: 1.110782, acc: 87%] [G loss: 1.187346]\n",
      "[Epoch 1/200] [Batch 492/938] [D loss: 1.108537, acc: 91%] [G loss: 1.228024]\n",
      "[Epoch 1/200] [Batch 493/938] [D loss: 1.101618, acc: 90%] [G loss: 1.179161]\n",
      "[Epoch 1/200] [Batch 494/938] [D loss: 1.077399, acc: 92%] [G loss: 1.176209]\n",
      "[Epoch 1/200] [Batch 495/938] [D loss: 1.068429, acc: 92%] [G loss: 1.195141]\n",
      "[Epoch 1/200] [Batch 496/938] [D loss: 1.118286, acc: 95%] [G loss: 1.137509]\n",
      "[Epoch 1/200] [Batch 497/938] [D loss: 1.074604, acc: 92%] [G loss: 1.151299]\n",
      "[Epoch 1/200] [Batch 498/938] [D loss: 1.075881, acc: 92%] [G loss: 1.198216]\n",
      "[Epoch 1/200] [Batch 499/938] [D loss: 1.090444, acc: 90%] [G loss: 1.172427]\n",
      "[Epoch 1/200] [Batch 500/938] [D loss: 1.089448, acc: 95%] [G loss: 1.228407]\n",
      "[Epoch 1/200] [Batch 501/938] [D loss: 1.078321, acc: 93%] [G loss: 1.233669]\n",
      "[Epoch 1/200] [Batch 502/938] [D loss: 1.089870, acc: 93%] [G loss: 1.163234]\n",
      "[Epoch 1/200] [Batch 503/938] [D loss: 1.088001, acc: 88%] [G loss: 1.232383]\n",
      "[Epoch 1/200] [Batch 504/938] [D loss: 1.083918, acc: 91%] [G loss: 1.225370]\n",
      "[Epoch 1/200] [Batch 505/938] [D loss: 1.119538, acc: 89%] [G loss: 1.215785]\n",
      "[Epoch 1/200] [Batch 506/938] [D loss: 1.086797, acc: 90%] [G loss: 1.243906]\n",
      "[Epoch 1/200] [Batch 507/938] [D loss: 1.076127, acc: 90%] [G loss: 1.189937]\n",
      "[Epoch 1/200] [Batch 508/938] [D loss: 1.074019, acc: 91%] [G loss: 1.185960]\n",
      "[Epoch 1/200] [Batch 509/938] [D loss: 1.097015, acc: 90%] [G loss: 1.212769]\n",
      "[Epoch 1/200] [Batch 510/938] [D loss: 1.081622, acc: 89%] [G loss: 1.159353]\n",
      "[Epoch 1/200] [Batch 511/938] [D loss: 1.114238, acc: 88%] [G loss: 1.132927]\n",
      "[Epoch 1/200] [Batch 512/938] [D loss: 1.087865, acc: 90%] [G loss: 1.238499]\n",
      "[Epoch 1/200] [Batch 513/938] [D loss: 1.094013, acc: 92%] [G loss: 1.248900]\n",
      "[Epoch 1/200] [Batch 514/938] [D loss: 1.084725, acc: 90%] [G loss: 1.224645]\n",
      "[Epoch 1/200] [Batch 515/938] [D loss: 1.138713, acc: 86%] [G loss: 1.201736]\n",
      "[Epoch 1/200] [Batch 516/938] [D loss: 1.127951, acc: 88%] [G loss: 1.147665]\n",
      "[Epoch 1/200] [Batch 517/938] [D loss: 1.079652, acc: 89%] [G loss: 1.138756]\n",
      "[Epoch 1/200] [Batch 518/938] [D loss: 1.095765, acc: 92%] [G loss: 1.157273]\n",
      "[Epoch 1/200] [Batch 519/938] [D loss: 1.090783, acc: 92%] [G loss: 1.207233]\n",
      "[Epoch 1/200] [Batch 520/938] [D loss: 1.103264, acc: 92%] [G loss: 1.195651]\n",
      "[Epoch 1/200] [Batch 521/938] [D loss: 1.093186, acc: 92%] [G loss: 1.118434]\n",
      "[Epoch 1/200] [Batch 522/938] [D loss: 1.099415, acc: 95%] [G loss: 1.108005]\n",
      "[Epoch 1/200] [Batch 523/938] [D loss: 1.086344, acc: 90%] [G loss: 1.265625]\n",
      "[Epoch 1/200] [Batch 524/938] [D loss: 1.075844, acc: 91%] [G loss: 1.217085]\n",
      "[Epoch 1/200] [Batch 525/938] [D loss: 1.103477, acc: 90%] [G loss: 1.189398]\n",
      "[Epoch 1/200] [Batch 526/938] [D loss: 1.101416, acc: 92%] [G loss: 1.146264]\n",
      "[Epoch 1/200] [Batch 527/938] [D loss: 1.102043, acc: 89%] [G loss: 1.131179]\n",
      "[Epoch 1/200] [Batch 528/938] [D loss: 1.052186, acc: 93%] [G loss: 1.184361]\n",
      "[Epoch 1/200] [Batch 529/938] [D loss: 1.080960, acc: 89%] [G loss: 1.203390]\n",
      "[Epoch 1/200] [Batch 530/938] [D loss: 1.072625, acc: 96%] [G loss: 1.192722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 531/938] [D loss: 1.075918, acc: 93%] [G loss: 1.191671]\n",
      "[Epoch 1/200] [Batch 532/938] [D loss: 1.101358, acc: 92%] [G loss: 1.158855]\n",
      "[Epoch 1/200] [Batch 533/938] [D loss: 1.107424, acc: 88%] [G loss: 1.182745]\n",
      "[Epoch 1/200] [Batch 534/938] [D loss: 1.088523, acc: 96%] [G loss: 1.154766]\n",
      "[Epoch 1/200] [Batch 535/938] [D loss: 1.137906, acc: 90%] [G loss: 1.164162]\n",
      "[Epoch 1/200] [Batch 536/938] [D loss: 1.085142, acc: 92%] [G loss: 1.153821]\n",
      "[Epoch 1/200] [Batch 537/938] [D loss: 1.091232, acc: 92%] [G loss: 1.204264]\n",
      "[Epoch 1/200] [Batch 538/938] [D loss: 1.084476, acc: 94%] [G loss: 1.107040]\n",
      "[Epoch 1/200] [Batch 539/938] [D loss: 1.072077, acc: 95%] [G loss: 1.155661]\n",
      "[Epoch 1/200] [Batch 540/938] [D loss: 1.112795, acc: 85%] [G loss: 1.184283]\n",
      "[Epoch 1/200] [Batch 541/938] [D loss: 1.080496, acc: 94%] [G loss: 1.188885]\n",
      "[Epoch 1/200] [Batch 542/938] [D loss: 1.089247, acc: 92%] [G loss: 1.258955]\n",
      "[Epoch 1/200] [Batch 543/938] [D loss: 1.053336, acc: 91%] [G loss: 1.168185]\n",
      "[Epoch 1/200] [Batch 544/938] [D loss: 1.080539, acc: 90%] [G loss: 1.208174]\n",
      "[Epoch 1/200] [Batch 545/938] [D loss: 1.101839, acc: 92%] [G loss: 1.113511]\n",
      "[Epoch 1/200] [Batch 546/938] [D loss: 1.068662, acc: 90%] [G loss: 1.210538]\n",
      "[Epoch 1/200] [Batch 547/938] [D loss: 1.078246, acc: 92%] [G loss: 1.169621]\n",
      "[Epoch 1/200] [Batch 548/938] [D loss: 1.096186, acc: 86%] [G loss: 1.163499]\n",
      "[Epoch 1/200] [Batch 549/938] [D loss: 1.093376, acc: 94%] [G loss: 1.190205]\n",
      "[Epoch 1/200] [Batch 550/938] [D loss: 1.076100, acc: 90%] [G loss: 1.169231]\n",
      "[Epoch 1/200] [Batch 551/938] [D loss: 1.098058, acc: 93%] [G loss: 1.190045]\n",
      "[Epoch 1/200] [Batch 552/938] [D loss: 1.071635, acc: 94%] [G loss: 1.195007]\n",
      "[Epoch 1/200] [Batch 553/938] [D loss: 1.072290, acc: 92%] [G loss: 1.158540]\n",
      "[Epoch 1/200] [Batch 554/938] [D loss: 1.088679, acc: 92%] [G loss: 1.135156]\n",
      "[Epoch 1/200] [Batch 555/938] [D loss: 1.095984, acc: 88%] [G loss: 1.177446]\n",
      "[Epoch 1/200] [Batch 556/938] [D loss: 1.091725, acc: 91%] [G loss: 1.166720]\n",
      "[Epoch 1/200] [Batch 557/938] [D loss: 1.086062, acc: 92%] [G loss: 1.152953]\n",
      "[Epoch 1/200] [Batch 558/938] [D loss: 1.141720, acc: 87%] [G loss: 1.132129]\n",
      "[Epoch 1/200] [Batch 559/938] [D loss: 1.095482, acc: 89%] [G loss: 1.188540]\n",
      "[Epoch 1/200] [Batch 560/938] [D loss: 1.093288, acc: 88%] [G loss: 1.190559]\n",
      "[Epoch 1/200] [Batch 561/938] [D loss: 1.085475, acc: 94%] [G loss: 1.128388]\n",
      "[Epoch 1/200] [Batch 562/938] [D loss: 1.073148, acc: 92%] [G loss: 1.170494]\n",
      "[Epoch 1/200] [Batch 563/938] [D loss: 1.110116, acc: 92%] [G loss: 1.248613]\n",
      "[Epoch 1/200] [Batch 564/938] [D loss: 1.094078, acc: 89%] [G loss: 1.169155]\n",
      "[Epoch 1/200] [Batch 565/938] [D loss: 1.091798, acc: 91%] [G loss: 1.093263]\n",
      "[Epoch 1/200] [Batch 566/938] [D loss: 1.090358, acc: 91%] [G loss: 1.137483]\n",
      "[Epoch 1/200] [Batch 567/938] [D loss: 1.063180, acc: 92%] [G loss: 1.223222]\n",
      "[Epoch 1/200] [Batch 568/938] [D loss: 1.091817, acc: 91%] [G loss: 1.184902]\n",
      "[Epoch 1/200] [Batch 569/938] [D loss: 1.083321, acc: 89%] [G loss: 1.190846]\n",
      "[Epoch 1/200] [Batch 570/938] [D loss: 1.069063, acc: 90%] [G loss: 1.181981]\n",
      "[Epoch 1/200] [Batch 571/938] [D loss: 1.118685, acc: 92%] [G loss: 1.120362]\n",
      "[Epoch 1/200] [Batch 572/938] [D loss: 1.058085, acc: 94%] [G loss: 1.200440]\n",
      "[Epoch 1/200] [Batch 573/938] [D loss: 1.082953, acc: 90%] [G loss: 1.210757]\n",
      "[Epoch 1/200] [Batch 574/938] [D loss: 1.061327, acc: 93%] [G loss: 1.243729]\n",
      "[Epoch 1/200] [Batch 575/938] [D loss: 1.069899, acc: 92%] [G loss: 1.189967]\n",
      "[Epoch 1/200] [Batch 576/938] [D loss: 1.110079, acc: 95%] [G loss: 1.222932]\n",
      "[Epoch 1/200] [Batch 577/938] [D loss: 1.062780, acc: 94%] [G loss: 1.181727]\n",
      "[Epoch 1/200] [Batch 578/938] [D loss: 1.126719, acc: 89%] [G loss: 1.230446]\n",
      "[Epoch 1/200] [Batch 579/938] [D loss: 1.081182, acc: 92%] [G loss: 1.202131]\n",
      "[Epoch 1/200] [Batch 580/938] [D loss: 1.100861, acc: 94%] [G loss: 1.164353]\n",
      "[Epoch 1/200] [Batch 581/938] [D loss: 1.077863, acc: 94%] [G loss: 1.167152]\n",
      "[Epoch 1/200] [Batch 582/938] [D loss: 1.112346, acc: 89%] [G loss: 1.205141]\n",
      "[Epoch 1/200] [Batch 583/938] [D loss: 1.044444, acc: 93%] [G loss: 1.164139]\n",
      "[Epoch 1/200] [Batch 584/938] [D loss: 1.105002, acc: 89%] [G loss: 1.131559]\n",
      "[Epoch 1/200] [Batch 585/938] [D loss: 1.059492, acc: 94%] [G loss: 1.188849]\n",
      "[Epoch 1/200] [Batch 586/938] [D loss: 1.068658, acc: 94%] [G loss: 1.201795]\n",
      "[Epoch 1/200] [Batch 587/938] [D loss: 1.077505, acc: 92%] [G loss: 1.144694]\n",
      "[Epoch 1/200] [Batch 588/938] [D loss: 1.060879, acc: 92%] [G loss: 1.195594]\n",
      "[Epoch 1/200] [Batch 589/938] [D loss: 1.104564, acc: 92%] [G loss: 1.163713]\n",
      "[Epoch 1/200] [Batch 590/938] [D loss: 1.077541, acc: 92%] [G loss: 1.218057]\n",
      "[Epoch 1/200] [Batch 591/938] [D loss: 1.080806, acc: 94%] [G loss: 1.248665]\n",
      "[Epoch 1/200] [Batch 592/938] [D loss: 1.071039, acc: 89%] [G loss: 1.322332]\n",
      "[Epoch 1/200] [Batch 593/938] [D loss: 1.090706, acc: 92%] [G loss: 1.236903]\n",
      "[Epoch 1/200] [Batch 594/938] [D loss: 1.096067, acc: 89%] [G loss: 1.232471]\n",
      "[Epoch 1/200] [Batch 595/938] [D loss: 1.128222, acc: 87%] [G loss: 1.242995]\n",
      "[Epoch 1/200] [Batch 596/938] [D loss: 1.063865, acc: 91%] [G loss: 1.135732]\n",
      "[Epoch 1/200] [Batch 597/938] [D loss: 1.109650, acc: 89%] [G loss: 1.179744]\n",
      "[Epoch 1/200] [Batch 598/938] [D loss: 1.105133, acc: 91%] [G loss: 1.128993]\n",
      "[Epoch 1/200] [Batch 599/938] [D loss: 1.099264, acc: 89%] [G loss: 1.191368]\n",
      "[Epoch 1/200] [Batch 600/938] [D loss: 1.089238, acc: 94%] [G loss: 1.207347]\n",
      "[Epoch 1/200] [Batch 601/938] [D loss: 1.065964, acc: 94%] [G loss: 1.182412]\n",
      "[Epoch 1/200] [Batch 602/938] [D loss: 1.071476, acc: 95%] [G loss: 1.213268]\n",
      "[Epoch 1/200] [Batch 603/938] [D loss: 1.076515, acc: 89%] [G loss: 1.200850]\n",
      "[Epoch 1/200] [Batch 604/938] [D loss: 1.078464, acc: 93%] [G loss: 1.220522]\n",
      "[Epoch 1/200] [Batch 605/938] [D loss: 1.090148, acc: 92%] [G loss: 1.212134]\n",
      "[Epoch 1/200] [Batch 606/938] [D loss: 1.073680, acc: 94%] [G loss: 1.181358]\n",
      "[Epoch 1/200] [Batch 607/938] [D loss: 1.098919, acc: 88%] [G loss: 1.245960]\n",
      "[Epoch 1/200] [Batch 608/938] [D loss: 1.079271, acc: 94%] [G loss: 1.145794]\n",
      "[Epoch 1/200] [Batch 609/938] [D loss: 1.040873, acc: 95%] [G loss: 1.225582]\n",
      "[Epoch 1/200] [Batch 610/938] [D loss: 1.090254, acc: 92%] [G loss: 1.212617]\n",
      "[Epoch 1/200] [Batch 611/938] [D loss: 1.078380, acc: 89%] [G loss: 1.235463]\n",
      "[Epoch 1/200] [Batch 612/938] [D loss: 1.101822, acc: 89%] [G loss: 1.210454]\n",
      "[Epoch 1/200] [Batch 613/938] [D loss: 1.093997, acc: 91%] [G loss: 1.160759]\n",
      "[Epoch 1/200] [Batch 614/938] [D loss: 1.096798, acc: 92%] [G loss: 1.162455]\n",
      "[Epoch 1/200] [Batch 615/938] [D loss: 1.077081, acc: 92%] [G loss: 1.202719]\n",
      "[Epoch 1/200] [Batch 616/938] [D loss: 1.075640, acc: 93%] [G loss: 1.202549]\n",
      "[Epoch 1/200] [Batch 617/938] [D loss: 1.068428, acc: 92%] [G loss: 1.220135]\n",
      "[Epoch 1/200] [Batch 618/938] [D loss: 1.127738, acc: 89%] [G loss: 1.220268]\n",
      "[Epoch 1/200] [Batch 619/938] [D loss: 1.091576, acc: 92%] [G loss: 1.182240]\n",
      "[Epoch 1/200] [Batch 620/938] [D loss: 1.088685, acc: 92%] [G loss: 1.194019]\n",
      "[Epoch 1/200] [Batch 621/938] [D loss: 1.084894, acc: 92%] [G loss: 1.127214]\n",
      "[Epoch 1/200] [Batch 622/938] [D loss: 1.083531, acc: 91%] [G loss: 1.119698]\n",
      "[Epoch 1/200] [Batch 623/938] [D loss: 1.086941, acc: 92%] [G loss: 1.173796]\n",
      "[Epoch 1/200] [Batch 624/938] [D loss: 1.088728, acc: 87%] [G loss: 1.264415]\n",
      "[Epoch 1/200] [Batch 625/938] [D loss: 1.091551, acc: 88%] [G loss: 1.250718]\n",
      "[Epoch 1/200] [Batch 626/938] [D loss: 1.078815, acc: 94%] [G loss: 1.135437]\n",
      "[Epoch 1/200] [Batch 627/938] [D loss: 1.120201, acc: 90%] [G loss: 1.189509]\n",
      "[Epoch 1/200] [Batch 628/938] [D loss: 1.088295, acc: 89%] [G loss: 1.231985]\n",
      "[Epoch 1/200] [Batch 629/938] [D loss: 1.082848, acc: 88%] [G loss: 1.192870]\n",
      "[Epoch 1/200] [Batch 630/938] [D loss: 1.090054, acc: 87%] [G loss: 1.155027]\n",
      "[Epoch 1/200] [Batch 631/938] [D loss: 1.076044, acc: 87%] [G loss: 1.166152]\n",
      "[Epoch 1/200] [Batch 632/938] [D loss: 1.100355, acc: 92%] [G loss: 1.272155]\n",
      "[Epoch 1/200] [Batch 633/938] [D loss: 1.065556, acc: 92%] [G loss: 1.204833]\n",
      "[Epoch 1/200] [Batch 634/938] [D loss: 1.066074, acc: 92%] [G loss: 1.173059]\n",
      "[Epoch 1/200] [Batch 635/938] [D loss: 1.082303, acc: 95%] [G loss: 1.130658]\n",
      "[Epoch 1/200] [Batch 636/938] [D loss: 1.085457, acc: 90%] [G loss: 1.192417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 637/938] [D loss: 1.068944, acc: 90%] [G loss: 1.199515]\n",
      "[Epoch 1/200] [Batch 638/938] [D loss: 1.083447, acc: 92%] [G loss: 1.223046]\n",
      "[Epoch 1/200] [Batch 639/938] [D loss: 1.089920, acc: 94%] [G loss: 1.198047]\n",
      "[Epoch 1/200] [Batch 640/938] [D loss: 1.084281, acc: 93%] [G loss: 1.184832]\n",
      "[Epoch 1/200] [Batch 641/938] [D loss: 1.050967, acc: 94%] [G loss: 1.163769]\n",
      "[Epoch 1/200] [Batch 642/938] [D loss: 1.065936, acc: 96%] [G loss: 1.159084]\n",
      "[Epoch 1/200] [Batch 643/938] [D loss: 1.070483, acc: 93%] [G loss: 1.215001]\n",
      "[Epoch 1/200] [Batch 644/938] [D loss: 1.104798, acc: 91%] [G loss: 1.204957]\n",
      "[Epoch 1/200] [Batch 645/938] [D loss: 1.088383, acc: 91%] [G loss: 1.231192]\n",
      "[Epoch 1/200] [Batch 646/938] [D loss: 1.086635, acc: 92%] [G loss: 1.174118]\n",
      "[Epoch 1/200] [Batch 647/938] [D loss: 1.074904, acc: 90%] [G loss: 1.163746]\n",
      "[Epoch 1/200] [Batch 648/938] [D loss: 1.107291, acc: 87%] [G loss: 1.194669]\n",
      "[Epoch 1/200] [Batch 649/938] [D loss: 1.089782, acc: 89%] [G loss: 1.187782]\n",
      "[Epoch 1/200] [Batch 650/938] [D loss: 1.115930, acc: 88%] [G loss: 1.177311]\n",
      "[Epoch 1/200] [Batch 651/938] [D loss: 1.113110, acc: 92%] [G loss: 1.182992]\n",
      "[Epoch 1/200] [Batch 652/938] [D loss: 1.082457, acc: 90%] [G loss: 1.190988]\n",
      "[Epoch 1/200] [Batch 653/938] [D loss: 1.111849, acc: 88%] [G loss: 1.232037]\n",
      "[Epoch 1/200] [Batch 654/938] [D loss: 1.042122, acc: 93%] [G loss: 1.191275]\n",
      "[Epoch 1/200] [Batch 655/938] [D loss: 1.087107, acc: 92%] [G loss: 1.127191]\n",
      "[Epoch 1/200] [Batch 656/938] [D loss: 1.105926, acc: 90%] [G loss: 1.185687]\n",
      "[Epoch 1/200] [Batch 657/938] [D loss: 1.069376, acc: 95%] [G loss: 1.252527]\n",
      "[Epoch 1/200] [Batch 658/938] [D loss: 1.105526, acc: 91%] [G loss: 1.283139]\n",
      "[Epoch 1/200] [Batch 659/938] [D loss: 1.075588, acc: 95%] [G loss: 1.201179]\n",
      "[Epoch 1/200] [Batch 660/938] [D loss: 1.090551, acc: 93%] [G loss: 1.151806]\n",
      "[Epoch 1/200] [Batch 661/938] [D loss: 1.094279, acc: 89%] [G loss: 1.220689]\n",
      "[Epoch 1/200] [Batch 662/938] [D loss: 1.074630, acc: 94%] [G loss: 1.198225]\n",
      "[Epoch 1/200] [Batch 663/938] [D loss: 1.067447, acc: 93%] [G loss: 1.293362]\n",
      "[Epoch 1/200] [Batch 664/938] [D loss: 1.103521, acc: 90%] [G loss: 1.173401]\n",
      "[Epoch 1/200] [Batch 665/938] [D loss: 1.070513, acc: 91%] [G loss: 1.132657]\n",
      "[Epoch 1/200] [Batch 666/938] [D loss: 1.111589, acc: 91%] [G loss: 1.189264]\n",
      "[Epoch 1/200] [Batch 667/938] [D loss: 1.077478, acc: 92%] [G loss: 1.154997]\n",
      "[Epoch 1/200] [Batch 668/938] [D loss: 1.074602, acc: 92%] [G loss: 1.180034]\n",
      "[Epoch 1/200] [Batch 669/938] [D loss: 1.091005, acc: 93%] [G loss: 1.244188]\n",
      "[Epoch 1/200] [Batch 670/938] [D loss: 1.080764, acc: 92%] [G loss: 1.093549]\n",
      "[Epoch 1/200] [Batch 671/938] [D loss: 1.081675, acc: 93%] [G loss: 1.199590]\n",
      "[Epoch 1/200] [Batch 672/938] [D loss: 1.102907, acc: 92%] [G loss: 1.203014]\n",
      "[Epoch 1/200] [Batch 673/938] [D loss: 1.056859, acc: 93%] [G loss: 1.216703]\n",
      "[Epoch 1/200] [Batch 674/938] [D loss: 1.060506, acc: 91%] [G loss: 1.224342]\n",
      "[Epoch 1/200] [Batch 675/938] [D loss: 1.083115, acc: 96%] [G loss: 1.106797]\n",
      "[Epoch 1/200] [Batch 676/938] [D loss: 1.070360, acc: 90%] [G loss: 1.150780]\n",
      "[Epoch 1/200] [Batch 677/938] [D loss: 1.080472, acc: 95%] [G loss: 1.214798]\n",
      "[Epoch 1/200] [Batch 678/938] [D loss: 1.069627, acc: 89%] [G loss: 1.221219]\n",
      "[Epoch 1/200] [Batch 679/938] [D loss: 1.057796, acc: 91%] [G loss: 1.214134]\n",
      "[Epoch 1/200] [Batch 680/938] [D loss: 1.080249, acc: 92%] [G loss: 1.170655]\n",
      "[Epoch 1/200] [Batch 681/938] [D loss: 1.072387, acc: 92%] [G loss: 1.209182]\n",
      "[Epoch 1/200] [Batch 682/938] [D loss: 1.102011, acc: 92%] [G loss: 1.225121]\n",
      "[Epoch 1/200] [Batch 683/938] [D loss: 1.058743, acc: 96%] [G loss: 1.147816]\n",
      "[Epoch 1/200] [Batch 684/938] [D loss: 1.085620, acc: 92%] [G loss: 1.257853]\n",
      "[Epoch 1/200] [Batch 685/938] [D loss: 1.091244, acc: 91%] [G loss: 1.168770]\n",
      "[Epoch 1/200] [Batch 686/938] [D loss: 1.118878, acc: 89%] [G loss: 1.133469]\n",
      "[Epoch 1/200] [Batch 687/938] [D loss: 1.091460, acc: 92%] [G loss: 1.187473]\n",
      "[Epoch 1/200] [Batch 688/938] [D loss: 1.079510, acc: 92%] [G loss: 1.188691]\n",
      "[Epoch 1/200] [Batch 689/938] [D loss: 1.110400, acc: 85%] [G loss: 1.230407]\n",
      "[Epoch 1/200] [Batch 690/938] [D loss: 1.112707, acc: 91%] [G loss: 1.180374]\n",
      "[Epoch 1/200] [Batch 691/938] [D loss: 1.100917, acc: 90%] [G loss: 1.111039]\n",
      "[Epoch 1/200] [Batch 692/938] [D loss: 1.137949, acc: 91%] [G loss: 1.218719]\n",
      "[Epoch 1/200] [Batch 693/938] [D loss: 1.072828, acc: 89%] [G loss: 1.171008]\n",
      "[Epoch 1/200] [Batch 694/938] [D loss: 1.094714, acc: 91%] [G loss: 1.211534]\n",
      "[Epoch 1/200] [Batch 695/938] [D loss: 1.065727, acc: 96%] [G loss: 1.165882]\n",
      "[Epoch 1/200] [Batch 696/938] [D loss: 1.058465, acc: 92%] [G loss: 1.231547]\n",
      "[Epoch 1/200] [Batch 697/938] [D loss: 1.064306, acc: 90%] [G loss: 1.216300]\n",
      "[Epoch 1/200] [Batch 698/938] [D loss: 1.107135, acc: 92%] [G loss: 1.228283]\n",
      "[Epoch 1/200] [Batch 699/938] [D loss: 1.064504, acc: 94%] [G loss: 1.222169]\n",
      "[Epoch 1/200] [Batch 700/938] [D loss: 1.077122, acc: 92%] [G loss: 1.279973]\n",
      "[Epoch 1/200] [Batch 701/938] [D loss: 1.065592, acc: 93%] [G loss: 1.236319]\n",
      "[Epoch 1/200] [Batch 702/938] [D loss: 1.093236, acc: 91%] [G loss: 1.172783]\n",
      "[Epoch 1/200] [Batch 703/938] [D loss: 1.104203, acc: 92%] [G loss: 1.175460]\n",
      "[Epoch 1/200] [Batch 704/938] [D loss: 1.075929, acc: 93%] [G loss: 1.193972]\n",
      "[Epoch 1/200] [Batch 705/938] [D loss: 1.091226, acc: 91%] [G loss: 1.193305]\n",
      "[Epoch 1/200] [Batch 706/938] [D loss: 1.097013, acc: 89%] [G loss: 1.173753]\n",
      "[Epoch 1/200] [Batch 707/938] [D loss: 1.067210, acc: 91%] [G loss: 1.209443]\n",
      "[Epoch 1/200] [Batch 708/938] [D loss: 1.048983, acc: 92%] [G loss: 1.177944]\n",
      "[Epoch 1/200] [Batch 709/938] [D loss: 1.067933, acc: 93%] [G loss: 1.225969]\n",
      "[Epoch 1/200] [Batch 710/938] [D loss: 1.069016, acc: 96%] [G loss: 1.201407]\n",
      "[Epoch 1/200] [Batch 711/938] [D loss: 1.076585, acc: 89%] [G loss: 1.152813]\n",
      "[Epoch 1/200] [Batch 712/938] [D loss: 1.070555, acc: 93%] [G loss: 1.266314]\n",
      "[Epoch 1/200] [Batch 713/938] [D loss: 1.109556, acc: 92%] [G loss: 1.233600]\n",
      "[Epoch 1/200] [Batch 714/938] [D loss: 1.076338, acc: 97%] [G loss: 1.175588]\n",
      "[Epoch 1/200] [Batch 715/938] [D loss: 1.093672, acc: 92%] [G loss: 1.143046]\n",
      "[Epoch 1/200] [Batch 716/938] [D loss: 1.081287, acc: 92%] [G loss: 1.200908]\n",
      "[Epoch 1/200] [Batch 717/938] [D loss: 1.129609, acc: 90%] [G loss: 1.219414]\n",
      "[Epoch 1/200] [Batch 718/938] [D loss: 1.083155, acc: 92%] [G loss: 1.165110]\n",
      "[Epoch 1/200] [Batch 719/938] [D loss: 1.085425, acc: 94%] [G loss: 1.227328]\n",
      "[Epoch 1/200] [Batch 720/938] [D loss: 1.067726, acc: 92%] [G loss: 1.207998]\n",
      "[Epoch 1/200] [Batch 721/938] [D loss: 1.048325, acc: 92%] [G loss: 1.239617]\n",
      "[Epoch 1/200] [Batch 722/938] [D loss: 1.066602, acc: 90%] [G loss: 1.170747]\n",
      "[Epoch 1/200] [Batch 723/938] [D loss: 1.100544, acc: 90%] [G loss: 1.218561]\n",
      "[Epoch 1/200] [Batch 724/938] [D loss: 1.082391, acc: 94%] [G loss: 1.186963]\n",
      "[Epoch 1/200] [Batch 725/938] [D loss: 1.026769, acc: 94%] [G loss: 1.278963]\n",
      "[Epoch 1/200] [Batch 726/938] [D loss: 1.108935, acc: 92%] [G loss: 1.206882]\n",
      "[Epoch 1/200] [Batch 727/938] [D loss: 1.045264, acc: 96%] [G loss: 1.170626]\n",
      "[Epoch 1/200] [Batch 728/938] [D loss: 1.108465, acc: 90%] [G loss: 1.234324]\n",
      "[Epoch 1/200] [Batch 729/938] [D loss: 1.116057, acc: 92%] [G loss: 1.170592]\n",
      "[Epoch 1/200] [Batch 730/938] [D loss: 1.108101, acc: 92%] [G loss: 1.168615]\n",
      "[Epoch 1/200] [Batch 731/938] [D loss: 1.100034, acc: 92%] [G loss: 1.192914]\n",
      "[Epoch 1/200] [Batch 732/938] [D loss: 1.093529, acc: 94%] [G loss: 1.153838]\n",
      "[Epoch 1/200] [Batch 733/938] [D loss: 1.116622, acc: 91%] [G loss: 1.156533]\n",
      "[Epoch 1/200] [Batch 734/938] [D loss: 1.076076, acc: 92%] [G loss: 1.135296]\n",
      "[Epoch 1/200] [Batch 735/938] [D loss: 1.071022, acc: 96%] [G loss: 1.196863]\n",
      "[Epoch 1/200] [Batch 736/938] [D loss: 1.082457, acc: 92%] [G loss: 1.199266]\n",
      "[Epoch 1/200] [Batch 737/938] [D loss: 1.078035, acc: 91%] [G loss: 1.209635]\n",
      "[Epoch 1/200] [Batch 738/938] [D loss: 1.115622, acc: 92%] [G loss: 1.195474]\n",
      "[Epoch 1/200] [Batch 739/938] [D loss: 1.119809, acc: 96%] [G loss: 1.153751]\n",
      "[Epoch 1/200] [Batch 740/938] [D loss: 1.061765, acc: 95%] [G loss: 1.178001]\n",
      "[Epoch 1/200] [Batch 741/938] [D loss: 1.100705, acc: 94%] [G loss: 1.201407]\n",
      "[Epoch 1/200] [Batch 742/938] [D loss: 1.096641, acc: 90%] [G loss: 1.209564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 743/938] [D loss: 1.108984, acc: 91%] [G loss: 1.221130]\n",
      "[Epoch 1/200] [Batch 744/938] [D loss: 1.066954, acc: 93%] [G loss: 1.228231]\n",
      "[Epoch 1/200] [Batch 745/938] [D loss: 1.017540, acc: 95%] [G loss: 1.242556]\n",
      "[Epoch 1/200] [Batch 746/938] [D loss: 1.117129, acc: 92%] [G loss: 1.164706]\n",
      "[Epoch 1/200] [Batch 747/938] [D loss: 1.084373, acc: 92%] [G loss: 1.099607]\n",
      "[Epoch 1/200] [Batch 748/938] [D loss: 1.060150, acc: 95%] [G loss: 1.199050]\n",
      "[Epoch 1/200] [Batch 749/938] [D loss: 1.111169, acc: 93%] [G loss: 1.252996]\n",
      "[Epoch 1/200] [Batch 750/938] [D loss: 1.044379, acc: 97%] [G loss: 1.231704]\n",
      "[Epoch 1/200] [Batch 751/938] [D loss: 1.063958, acc: 96%] [G loss: 1.136227]\n",
      "[Epoch 1/200] [Batch 752/938] [D loss: 1.129031, acc: 89%] [G loss: 1.187241]\n",
      "[Epoch 1/200] [Batch 753/938] [D loss: 1.081709, acc: 90%] [G loss: 1.259707]\n",
      "[Epoch 1/200] [Batch 754/938] [D loss: 1.081619, acc: 90%] [G loss: 1.271804]\n",
      "[Epoch 1/200] [Batch 755/938] [D loss: 1.119216, acc: 88%] [G loss: 1.192171]\n",
      "[Epoch 1/200] [Batch 756/938] [D loss: 1.094915, acc: 93%] [G loss: 1.139575]\n",
      "[Epoch 1/200] [Batch 757/938] [D loss: 1.087153, acc: 91%] [G loss: 1.213605]\n",
      "[Epoch 1/200] [Batch 758/938] [D loss: 1.081823, acc: 92%] [G loss: 1.202779]\n",
      "[Epoch 1/200] [Batch 759/938] [D loss: 1.060814, acc: 96%] [G loss: 1.183776]\n",
      "[Epoch 1/200] [Batch 760/938] [D loss: 1.051892, acc: 95%] [G loss: 1.222593]\n",
      "[Epoch 1/200] [Batch 761/938] [D loss: 1.095046, acc: 92%] [G loss: 1.121916]\n",
      "[Epoch 1/200] [Batch 762/938] [D loss: 1.068074, acc: 94%] [G loss: 1.176743]\n",
      "[Epoch 1/200] [Batch 763/938] [D loss: 1.077647, acc: 92%] [G loss: 1.115906]\n",
      "[Epoch 1/200] [Batch 764/938] [D loss: 1.090474, acc: 95%] [G loss: 1.136075]\n",
      "[Epoch 1/200] [Batch 765/938] [D loss: 1.079874, acc: 92%] [G loss: 1.161421]\n",
      "[Epoch 1/200] [Batch 766/938] [D loss: 1.072861, acc: 96%] [G loss: 1.174152]\n",
      "[Epoch 1/200] [Batch 767/938] [D loss: 1.062852, acc: 92%] [G loss: 1.223928]\n",
      "[Epoch 1/200] [Batch 768/938] [D loss: 1.091228, acc: 90%] [G loss: 1.219890]\n",
      "[Epoch 1/200] [Batch 769/938] [D loss: 1.088655, acc: 88%] [G loss: 1.184842]\n",
      "[Epoch 1/200] [Batch 770/938] [D loss: 1.092272, acc: 94%] [G loss: 1.147803]\n",
      "[Epoch 1/200] [Batch 771/938] [D loss: 1.064059, acc: 91%] [G loss: 1.230566]\n",
      "[Epoch 1/200] [Batch 772/938] [D loss: 1.087183, acc: 92%] [G loss: 1.210464]\n",
      "[Epoch 1/200] [Batch 773/938] [D loss: 1.086403, acc: 92%] [G loss: 1.233395]\n",
      "[Epoch 1/200] [Batch 774/938] [D loss: 1.063430, acc: 94%] [G loss: 1.208953]\n",
      "[Epoch 1/200] [Batch 775/938] [D loss: 1.075100, acc: 94%] [G loss: 1.190621]\n",
      "[Epoch 1/200] [Batch 776/938] [D loss: 1.089788, acc: 89%] [G loss: 1.146038]\n",
      "[Epoch 1/200] [Batch 777/938] [D loss: 1.052753, acc: 95%] [G loss: 1.130924]\n",
      "[Epoch 1/200] [Batch 778/938] [D loss: 1.048208, acc: 96%] [G loss: 1.252973]\n",
      "[Epoch 1/200] [Batch 779/938] [D loss: 1.073123, acc: 93%] [G loss: 1.193646]\n",
      "[Epoch 1/200] [Batch 780/938] [D loss: 1.083247, acc: 94%] [G loss: 1.288460]\n",
      "[Epoch 1/200] [Batch 781/938] [D loss: 1.053739, acc: 94%] [G loss: 1.276879]\n",
      "[Epoch 1/200] [Batch 782/938] [D loss: 1.061424, acc: 93%] [G loss: 1.123950]\n",
      "[Epoch 1/200] [Batch 783/938] [D loss: 1.086607, acc: 93%] [G loss: 1.177696]\n",
      "[Epoch 1/200] [Batch 784/938] [D loss: 1.077354, acc: 96%] [G loss: 1.157333]\n",
      "[Epoch 1/200] [Batch 785/938] [D loss: 1.047839, acc: 97%] [G loss: 1.219718]\n",
      "[Epoch 1/200] [Batch 786/938] [D loss: 1.050027, acc: 95%] [G loss: 1.179511]\n",
      "[Epoch 1/200] [Batch 787/938] [D loss: 1.053124, acc: 89%] [G loss: 1.279715]\n",
      "[Epoch 1/200] [Batch 788/938] [D loss: 1.093428, acc: 94%] [G loss: 1.168654]\n",
      "[Epoch 1/200] [Batch 789/938] [D loss: 1.072616, acc: 95%] [G loss: 1.161766]\n",
      "[Epoch 1/200] [Batch 790/938] [D loss: 1.084009, acc: 89%] [G loss: 1.178346]\n",
      "[Epoch 1/200] [Batch 791/938] [D loss: 1.125476, acc: 93%] [G loss: 1.141802]\n",
      "[Epoch 1/200] [Batch 792/938] [D loss: 1.114253, acc: 92%] [G loss: 1.206352]\n",
      "[Epoch 1/200] [Batch 793/938] [D loss: 1.098999, acc: 94%] [G loss: 1.176617]\n",
      "[Epoch 1/200] [Batch 794/938] [D loss: 1.072730, acc: 93%] [G loss: 1.238561]\n",
      "[Epoch 1/200] [Batch 795/938] [D loss: 1.080038, acc: 92%] [G loss: 1.077644]\n",
      "[Epoch 1/200] [Batch 796/938] [D loss: 1.037776, acc: 97%] [G loss: 1.148956]\n",
      "[Epoch 1/200] [Batch 797/938] [D loss: 1.060174, acc: 95%] [G loss: 1.196067]\n",
      "[Epoch 1/200] [Batch 798/938] [D loss: 1.092582, acc: 89%] [G loss: 1.207654]\n",
      "[Epoch 1/200] [Batch 799/938] [D loss: 1.095668, acc: 95%] [G loss: 1.173484]\n",
      "[Epoch 1/200] [Batch 800/938] [D loss: 1.092371, acc: 89%] [G loss: 1.216939]\n",
      "[Epoch 1/200] [Batch 801/938] [D loss: 1.106042, acc: 92%] [G loss: 1.272219]\n",
      "[Epoch 1/200] [Batch 802/938] [D loss: 1.095717, acc: 93%] [G loss: 1.164157]\n",
      "[Epoch 1/200] [Batch 803/938] [D loss: 1.108551, acc: 86%] [G loss: 1.118872]\n",
      "[Epoch 1/200] [Batch 804/938] [D loss: 1.087942, acc: 92%] [G loss: 1.159070]\n",
      "[Epoch 1/200] [Batch 805/938] [D loss: 1.109241, acc: 92%] [G loss: 1.205078]\n",
      "[Epoch 1/200] [Batch 806/938] [D loss: 1.085243, acc: 94%] [G loss: 1.174196]\n",
      "[Epoch 1/200] [Batch 807/938] [D loss: 1.086902, acc: 93%] [G loss: 1.130749]\n",
      "[Epoch 1/200] [Batch 808/938] [D loss: 1.051384, acc: 94%] [G loss: 1.139996]\n",
      "[Epoch 1/200] [Batch 809/938] [D loss: 1.075777, acc: 92%] [G loss: 1.205045]\n",
      "[Epoch 1/200] [Batch 810/938] [D loss: 1.075387, acc: 95%] [G loss: 1.130421]\n",
      "[Epoch 1/200] [Batch 811/938] [D loss: 1.119018, acc: 88%] [G loss: 1.270797]\n",
      "[Epoch 1/200] [Batch 812/938] [D loss: 1.079472, acc: 90%] [G loss: 1.228191]\n",
      "[Epoch 1/200] [Batch 813/938] [D loss: 1.110007, acc: 87%] [G loss: 1.203015]\n",
      "[Epoch 1/200] [Batch 814/938] [D loss: 1.086860, acc: 93%] [G loss: 1.161747]\n",
      "[Epoch 1/200] [Batch 815/938] [D loss: 1.096446, acc: 93%] [G loss: 1.136426]\n",
      "[Epoch 1/200] [Batch 816/938] [D loss: 1.066255, acc: 93%] [G loss: 1.206751]\n",
      "[Epoch 1/200] [Batch 817/938] [D loss: 1.076419, acc: 92%] [G loss: 1.170769]\n",
      "[Epoch 1/200] [Batch 818/938] [D loss: 1.107055, acc: 91%] [G loss: 1.195820]\n",
      "[Epoch 1/200] [Batch 819/938] [D loss: 1.078456, acc: 96%] [G loss: 1.162398]\n",
      "[Epoch 1/200] [Batch 820/938] [D loss: 1.105311, acc: 92%] [G loss: 1.155242]\n",
      "[Epoch 1/200] [Batch 821/938] [D loss: 1.057144, acc: 92%] [G loss: 1.202530]\n",
      "[Epoch 1/200] [Batch 822/938] [D loss: 1.090681, acc: 91%] [G loss: 1.183272]\n",
      "[Epoch 1/200] [Batch 823/938] [D loss: 1.093822, acc: 89%] [G loss: 1.185992]\n",
      "[Epoch 1/200] [Batch 824/938] [D loss: 1.108977, acc: 92%] [G loss: 1.210578]\n",
      "[Epoch 1/200] [Batch 825/938] [D loss: 1.074530, acc: 94%] [G loss: 1.160172]\n",
      "[Epoch 1/200] [Batch 826/938] [D loss: 1.081509, acc: 97%] [G loss: 1.197855]\n",
      "[Epoch 1/200] [Batch 827/938] [D loss: 1.102762, acc: 89%] [G loss: 1.218532]\n",
      "[Epoch 1/200] [Batch 828/938] [D loss: 1.103558, acc: 93%] [G loss: 1.170835]\n",
      "[Epoch 1/200] [Batch 829/938] [D loss: 1.111238, acc: 92%] [G loss: 1.158333]\n",
      "[Epoch 1/200] [Batch 830/938] [D loss: 1.110052, acc: 87%] [G loss: 1.142403]\n",
      "[Epoch 1/200] [Batch 831/938] [D loss: 1.048496, acc: 96%] [G loss: 1.211861]\n",
      "[Epoch 1/200] [Batch 832/938] [D loss: 1.084540, acc: 91%] [G loss: 1.177555]\n",
      "[Epoch 1/200] [Batch 833/938] [D loss: 1.106554, acc: 95%] [G loss: 1.209187]\n",
      "[Epoch 1/200] [Batch 834/938] [D loss: 1.074343, acc: 94%] [G loss: 1.298207]\n",
      "[Epoch 1/200] [Batch 835/938] [D loss: 1.120937, acc: 90%] [G loss: 1.214237]\n",
      "[Epoch 1/200] [Batch 836/938] [D loss: 1.094431, acc: 90%] [G loss: 1.190701]\n",
      "[Epoch 1/200] [Batch 837/938] [D loss: 1.057133, acc: 96%] [G loss: 1.255117]\n",
      "[Epoch 1/200] [Batch 838/938] [D loss: 1.064711, acc: 96%] [G loss: 1.226169]\n",
      "[Epoch 1/200] [Batch 839/938] [D loss: 1.105075, acc: 89%] [G loss: 1.231966]\n",
      "[Epoch 1/200] [Batch 840/938] [D loss: 1.063767, acc: 95%] [G loss: 1.268504]\n",
      "[Epoch 1/200] [Batch 841/938] [D loss: 1.094525, acc: 93%] [G loss: 1.209548]\n",
      "[Epoch 1/200] [Batch 842/938] [D loss: 1.055332, acc: 94%] [G loss: 1.209561]\n",
      "[Epoch 1/200] [Batch 843/938] [D loss: 1.068727, acc: 96%] [G loss: 1.158196]\n",
      "[Epoch 1/200] [Batch 844/938] [D loss: 1.084340, acc: 93%] [G loss: 1.219050]\n",
      "[Epoch 1/200] [Batch 845/938] [D loss: 1.064673, acc: 91%] [G loss: 1.149299]\n",
      "[Epoch 1/200] [Batch 846/938] [D loss: 1.117907, acc: 90%] [G loss: 1.139235]\n",
      "[Epoch 1/200] [Batch 847/938] [D loss: 1.119024, acc: 94%] [G loss: 1.199778]\n",
      "[Epoch 1/200] [Batch 848/938] [D loss: 1.074950, acc: 92%] [G loss: 1.146809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 849/938] [D loss: 1.060737, acc: 92%] [G loss: 1.188333]\n",
      "[Epoch 1/200] [Batch 850/938] [D loss: 1.090889, acc: 87%] [G loss: 1.187841]\n",
      "[Epoch 1/200] [Batch 851/938] [D loss: 1.077773, acc: 87%] [G loss: 1.227162]\n",
      "[Epoch 1/200] [Batch 852/938] [D loss: 1.090778, acc: 96%] [G loss: 1.115005]\n",
      "[Epoch 1/200] [Batch 853/938] [D loss: 1.107210, acc: 92%] [G loss: 1.201614]\n",
      "[Epoch 1/200] [Batch 854/938] [D loss: 1.074748, acc: 92%] [G loss: 1.214687]\n",
      "[Epoch 1/200] [Batch 855/938] [D loss: 1.071019, acc: 96%] [G loss: 1.158263]\n",
      "[Epoch 1/200] [Batch 856/938] [D loss: 1.080194, acc: 95%] [G loss: 1.197005]\n",
      "[Epoch 1/200] [Batch 857/938] [D loss: 1.072439, acc: 94%] [G loss: 1.086423]\n",
      "[Epoch 1/200] [Batch 858/938] [D loss: 1.075168, acc: 90%] [G loss: 1.233711]\n",
      "[Epoch 1/200] [Batch 859/938] [D loss: 1.082556, acc: 90%] [G loss: 1.255558]\n",
      "[Epoch 1/200] [Batch 860/938] [D loss: 1.077277, acc: 89%] [G loss: 1.133445]\n",
      "[Epoch 1/200] [Batch 861/938] [D loss: 1.104765, acc: 89%] [G loss: 1.116478]\n",
      "[Epoch 1/200] [Batch 862/938] [D loss: 1.075704, acc: 92%] [G loss: 1.230824]\n",
      "[Epoch 1/200] [Batch 863/938] [D loss: 1.103585, acc: 95%] [G loss: 1.087923]\n",
      "[Epoch 1/200] [Batch 864/938] [D loss: 1.102246, acc: 92%] [G loss: 1.122290]\n",
      "[Epoch 1/200] [Batch 865/938] [D loss: 1.115897, acc: 92%] [G loss: 1.200446]\n",
      "[Epoch 1/200] [Batch 866/938] [D loss: 1.093289, acc: 92%] [G loss: 1.225626]\n",
      "[Epoch 1/200] [Batch 867/938] [D loss: 1.087175, acc: 93%] [G loss: 1.179033]\n",
      "[Epoch 1/200] [Batch 868/938] [D loss: 1.109207, acc: 88%] [G loss: 1.252073]\n",
      "[Epoch 1/200] [Batch 869/938] [D loss: 1.097327, acc: 91%] [G loss: 1.264287]\n",
      "[Epoch 1/200] [Batch 870/938] [D loss: 1.060385, acc: 92%] [G loss: 1.195469]\n",
      "[Epoch 1/200] [Batch 871/938] [D loss: 1.110344, acc: 91%] [G loss: 1.237106]\n",
      "[Epoch 1/200] [Batch 872/938] [D loss: 1.084766, acc: 95%] [G loss: 1.190326]\n",
      "[Epoch 1/200] [Batch 873/938] [D loss: 1.086791, acc: 94%] [G loss: 1.138200]\n",
      "[Epoch 1/200] [Batch 874/938] [D loss: 1.100709, acc: 90%] [G loss: 1.285507]\n",
      "[Epoch 1/200] [Batch 875/938] [D loss: 1.053735, acc: 96%] [G loss: 1.199033]\n",
      "[Epoch 1/200] [Batch 876/938] [D loss: 1.092760, acc: 94%] [G loss: 1.115191]\n",
      "[Epoch 1/200] [Batch 877/938] [D loss: 1.086406, acc: 91%] [G loss: 1.242108]\n",
      "[Epoch 1/200] [Batch 878/938] [D loss: 1.067527, acc: 96%] [G loss: 1.213519]\n",
      "[Epoch 1/200] [Batch 879/938] [D loss: 1.048403, acc: 93%] [G loss: 1.239978]\n",
      "[Epoch 1/200] [Batch 880/938] [D loss: 1.068220, acc: 92%] [G loss: 1.200704]\n",
      "[Epoch 1/200] [Batch 881/938] [D loss: 1.046414, acc: 92%] [G loss: 1.140536]\n",
      "[Epoch 1/200] [Batch 882/938] [D loss: 1.083737, acc: 88%] [G loss: 1.261067]\n",
      "[Epoch 1/200] [Batch 883/938] [D loss: 1.094191, acc: 89%] [G loss: 1.206428]\n",
      "[Epoch 1/200] [Batch 884/938] [D loss: 1.111011, acc: 91%] [G loss: 1.241699]\n",
      "[Epoch 1/200] [Batch 885/938] [D loss: 1.094327, acc: 91%] [G loss: 1.190885]\n",
      "[Epoch 1/200] [Batch 886/938] [D loss: 1.093087, acc: 96%] [G loss: 1.145990]\n",
      "[Epoch 1/200] [Batch 887/938] [D loss: 1.046367, acc: 91%] [G loss: 1.193089]\n",
      "[Epoch 1/200] [Batch 888/938] [D loss: 1.086149, acc: 92%] [G loss: 1.197735]\n",
      "[Epoch 1/200] [Batch 889/938] [D loss: 1.120700, acc: 94%] [G loss: 1.133648]\n",
      "[Epoch 1/200] [Batch 890/938] [D loss: 1.137695, acc: 92%] [G loss: 1.129924]\n",
      "[Epoch 1/200] [Batch 891/938] [D loss: 1.033901, acc: 94%] [G loss: 1.135376]\n",
      "[Epoch 1/200] [Batch 892/938] [D loss: 1.079772, acc: 92%] [G loss: 1.207205]\n",
      "[Epoch 1/200] [Batch 893/938] [D loss: 1.055683, acc: 98%] [G loss: 1.254615]\n",
      "[Epoch 1/200] [Batch 894/938] [D loss: 1.073426, acc: 96%] [G loss: 1.256976]\n",
      "[Epoch 1/200] [Batch 895/938] [D loss: 1.088113, acc: 93%] [G loss: 1.224301]\n",
      "[Epoch 1/200] [Batch 896/938] [D loss: 1.115215, acc: 89%] [G loss: 1.171769]\n",
      "[Epoch 1/200] [Batch 897/938] [D loss: 1.077949, acc: 92%] [G loss: 1.216139]\n",
      "[Epoch 1/200] [Batch 898/938] [D loss: 1.102918, acc: 89%] [G loss: 1.129277]\n",
      "[Epoch 1/200] [Batch 899/938] [D loss: 1.101207, acc: 92%] [G loss: 1.149626]\n",
      "[Epoch 1/200] [Batch 900/938] [D loss: 1.089676, acc: 88%] [G loss: 1.202063]\n",
      "[Epoch 1/200] [Batch 901/938] [D loss: 1.042992, acc: 95%] [G loss: 1.261403]\n",
      "[Epoch 1/200] [Batch 902/938] [D loss: 1.075500, acc: 96%] [G loss: 1.201309]\n",
      "[Epoch 1/200] [Batch 903/938] [D loss: 1.114991, acc: 92%] [G loss: 1.168228]\n",
      "[Epoch 1/200] [Batch 904/938] [D loss: 1.141660, acc: 94%] [G loss: 1.151924]\n",
      "[Epoch 1/200] [Batch 905/938] [D loss: 1.112579, acc: 93%] [G loss: 1.175679]\n",
      "[Epoch 1/200] [Batch 906/938] [D loss: 1.084455, acc: 90%] [G loss: 1.262662]\n",
      "[Epoch 1/200] [Batch 907/938] [D loss: 1.109303, acc: 88%] [G loss: 1.194506]\n",
      "[Epoch 1/200] [Batch 908/938] [D loss: 1.112054, acc: 92%] [G loss: 1.169686]\n",
      "[Epoch 1/200] [Batch 909/938] [D loss: 1.069439, acc: 91%] [G loss: 1.173031]\n",
      "[Epoch 1/200] [Batch 910/938] [D loss: 1.096426, acc: 91%] [G loss: 1.144544]\n",
      "[Epoch 1/200] [Batch 911/938] [D loss: 1.090543, acc: 92%] [G loss: 1.126045]\n",
      "[Epoch 1/200] [Batch 912/938] [D loss: 1.074684, acc: 91%] [G loss: 1.230486]\n",
      "[Epoch 1/200] [Batch 913/938] [D loss: 1.067861, acc: 97%] [G loss: 1.189325]\n",
      "[Epoch 1/200] [Batch 914/938] [D loss: 1.098477, acc: 91%] [G loss: 1.163386]\n",
      "[Epoch 1/200] [Batch 915/938] [D loss: 1.082328, acc: 94%] [G loss: 1.205631]\n",
      "[Epoch 1/200] [Batch 916/938] [D loss: 1.085805, acc: 95%] [G loss: 1.151127]\n",
      "[Epoch 1/200] [Batch 917/938] [D loss: 1.078146, acc: 94%] [G loss: 1.145414]\n",
      "[Epoch 1/200] [Batch 918/938] [D loss: 1.086259, acc: 94%] [G loss: 1.171952]\n",
      "[Epoch 1/200] [Batch 919/938] [D loss: 1.073689, acc: 90%] [G loss: 1.223490]\n",
      "[Epoch 1/200] [Batch 920/938] [D loss: 1.054680, acc: 93%] [G loss: 1.184710]\n",
      "[Epoch 1/200] [Batch 921/938] [D loss: 1.069249, acc: 94%] [G loss: 1.165850]\n",
      "[Epoch 1/200] [Batch 922/938] [D loss: 1.085173, acc: 90%] [G loss: 1.264427]\n",
      "[Epoch 1/200] [Batch 923/938] [D loss: 1.088395, acc: 93%] [G loss: 1.229047]\n",
      "[Epoch 1/200] [Batch 924/938] [D loss: 1.081479, acc: 96%] [G loss: 1.128404]\n",
      "[Epoch 1/200] [Batch 925/938] [D loss: 1.070584, acc: 96%] [G loss: 1.237789]\n",
      "[Epoch 1/200] [Batch 926/938] [D loss: 1.043670, acc: 96%] [G loss: 1.211932]\n",
      "[Epoch 1/200] [Batch 927/938] [D loss: 1.109458, acc: 92%] [G loss: 1.154347]\n",
      "[Epoch 1/200] [Batch 928/938] [D loss: 1.076161, acc: 92%] [G loss: 1.242760]\n",
      "[Epoch 1/200] [Batch 929/938] [D loss: 1.086168, acc: 96%] [G loss: 1.212259]\n",
      "[Epoch 1/200] [Batch 930/938] [D loss: 1.052043, acc: 94%] [G loss: 1.231446]\n",
      "[Epoch 1/200] [Batch 931/938] [D loss: 1.075558, acc: 91%] [G loss: 1.219403]\n",
      "[Epoch 1/200] [Batch 932/938] [D loss: 1.108094, acc: 92%] [G loss: 1.199578]\n",
      "[Epoch 1/200] [Batch 933/938] [D loss: 1.097549, acc: 94%] [G loss: 1.194164]\n",
      "[Epoch 1/200] [Batch 934/938] [D loss: 1.115694, acc: 91%] [G loss: 1.207767]\n",
      "[Epoch 1/200] [Batch 935/938] [D loss: 1.054975, acc: 92%] [G loss: 1.193970]\n",
      "[Epoch 1/200] [Batch 936/938] [D loss: 1.057240, acc: 92%] [G loss: 1.145737]\n",
      "[Epoch 1/200] [Batch 937/938] [D loss: 1.006132, acc: 95%] [G loss: 1.194134]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0975eb1e142e41cb90437758100cf6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 0/938] [D loss: 1.102890, acc: 89%] [G loss: 1.195687]\n",
      "[Epoch 2/200] [Batch 1/938] [D loss: 1.093609, acc: 92%] [G loss: 1.211217]\n",
      "[Epoch 2/200] [Batch 2/938] [D loss: 1.097213, acc: 91%] [G loss: 1.268608]\n",
      "[Epoch 2/200] [Batch 3/938] [D loss: 1.090481, acc: 97%] [G loss: 1.151255]\n",
      "[Epoch 2/200] [Batch 4/938] [D loss: 1.085281, acc: 88%] [G loss: 1.166504]\n",
      "[Epoch 2/200] [Batch 5/938] [D loss: 1.061405, acc: 95%] [G loss: 1.182537]\n",
      "[Epoch 2/200] [Batch 6/938] [D loss: 1.069397, acc: 93%] [G loss: 1.247150]\n",
      "[Epoch 2/200] [Batch 7/938] [D loss: 1.064615, acc: 93%] [G loss: 1.180156]\n",
      "[Epoch 2/200] [Batch 8/938] [D loss: 1.060749, acc: 89%] [G loss: 1.190495]\n",
      "[Epoch 2/200] [Batch 9/938] [D loss: 1.105645, acc: 92%] [G loss: 1.192605]\n",
      "[Epoch 2/200] [Batch 10/938] [D loss: 1.108043, acc: 90%] [G loss: 1.196802]\n",
      "[Epoch 2/200] [Batch 11/938] [D loss: 1.108950, acc: 91%] [G loss: 1.120402]\n",
      "[Epoch 2/200] [Batch 12/938] [D loss: 1.079841, acc: 95%] [G loss: 1.163673]\n",
      "[Epoch 2/200] [Batch 13/938] [D loss: 1.101323, acc: 91%] [G loss: 1.222492]\n",
      "[Epoch 2/200] [Batch 14/938] [D loss: 1.061676, acc: 91%] [G loss: 1.205540]\n",
      "[Epoch 2/200] [Batch 15/938] [D loss: 1.101013, acc: 91%] [G loss: 1.173093]\n",
      "[Epoch 2/200] [Batch 16/938] [D loss: 1.063806, acc: 95%] [G loss: 1.286338]\n",
      "[Epoch 2/200] [Batch 17/938] [D loss: 1.083912, acc: 92%] [G loss: 1.215232]\n",
      "[Epoch 2/200] [Batch 18/938] [D loss: 1.059957, acc: 92%] [G loss: 1.193923]\n",
      "[Epoch 2/200] [Batch 19/938] [D loss: 1.095094, acc: 91%] [G loss: 1.113660]\n",
      "[Epoch 2/200] [Batch 20/938] [D loss: 1.058083, acc: 93%] [G loss: 1.198515]\n",
      "[Epoch 2/200] [Batch 21/938] [D loss: 1.089161, acc: 92%] [G loss: 1.175076]\n",
      "[Epoch 2/200] [Batch 22/938] [D loss: 1.071569, acc: 94%] [G loss: 1.233842]\n",
      "[Epoch 2/200] [Batch 23/938] [D loss: 1.103319, acc: 93%] [G loss: 1.231761]\n",
      "[Epoch 2/200] [Batch 24/938] [D loss: 1.095308, acc: 92%] [G loss: 1.118131]\n",
      "[Epoch 2/200] [Batch 25/938] [D loss: 1.053205, acc: 91%] [G loss: 1.173437]\n",
      "[Epoch 2/200] [Batch 26/938] [D loss: 1.058302, acc: 92%] [G loss: 1.242951]\n",
      "[Epoch 2/200] [Batch 27/938] [D loss: 1.076802, acc: 93%] [G loss: 1.158897]\n",
      "[Epoch 2/200] [Batch 28/938] [D loss: 1.073010, acc: 89%] [G loss: 1.226300]\n",
      "[Epoch 2/200] [Batch 29/938] [D loss: 1.101776, acc: 91%] [G loss: 1.128690]\n",
      "[Epoch 2/200] [Batch 30/938] [D loss: 1.080430, acc: 90%] [G loss: 1.218326]\n",
      "[Epoch 2/200] [Batch 31/938] [D loss: 1.105869, acc: 88%] [G loss: 1.179165]\n",
      "[Epoch 2/200] [Batch 32/938] [D loss: 1.092213, acc: 92%] [G loss: 1.177846]\n",
      "[Epoch 2/200] [Batch 33/938] [D loss: 1.070221, acc: 95%] [G loss: 1.201345]\n",
      "[Epoch 2/200] [Batch 34/938] [D loss: 1.100253, acc: 90%] [G loss: 1.225557]\n",
      "[Epoch 2/200] [Batch 35/938] [D loss: 1.119103, acc: 92%] [G loss: 1.164057]\n",
      "[Epoch 2/200] [Batch 36/938] [D loss: 1.082819, acc: 94%] [G loss: 1.260940]\n",
      "[Epoch 2/200] [Batch 37/938] [D loss: 1.074055, acc: 91%] [G loss: 1.162558]\n",
      "[Epoch 2/200] [Batch 38/938] [D loss: 1.085889, acc: 89%] [G loss: 1.154197]\n",
      "[Epoch 2/200] [Batch 39/938] [D loss: 1.082484, acc: 93%] [G loss: 1.209941]\n",
      "[Epoch 2/200] [Batch 40/938] [D loss: 1.065582, acc: 95%] [G loss: 1.256732]\n",
      "[Epoch 2/200] [Batch 41/938] [D loss: 1.132222, acc: 92%] [G loss: 1.178653]\n",
      "[Epoch 2/200] [Batch 42/938] [D loss: 1.092997, acc: 92%] [G loss: 1.158176]\n",
      "[Epoch 2/200] [Batch 43/938] [D loss: 1.114186, acc: 88%] [G loss: 1.200137]\n",
      "[Epoch 2/200] [Batch 44/938] [D loss: 1.085912, acc: 94%] [G loss: 1.216319]\n",
      "[Epoch 2/200] [Batch 45/938] [D loss: 1.108401, acc: 93%] [G loss: 1.121840]\n",
      "[Epoch 2/200] [Batch 46/938] [D loss: 1.068961, acc: 93%] [G loss: 1.192209]\n",
      "[Epoch 2/200] [Batch 47/938] [D loss: 1.125516, acc: 92%] [G loss: 1.135317]\n",
      "[Epoch 2/200] [Batch 48/938] [D loss: 1.105467, acc: 92%] [G loss: 1.198330]\n",
      "[Epoch 2/200] [Batch 49/938] [D loss: 1.091341, acc: 96%] [G loss: 1.100389]\n",
      "[Epoch 2/200] [Batch 50/938] [D loss: 1.083670, acc: 97%] [G loss: 1.113463]\n",
      "[Epoch 2/200] [Batch 51/938] [D loss: 1.090206, acc: 93%] [G loss: 1.187161]\n",
      "[Epoch 2/200] [Batch 52/938] [D loss: 1.097512, acc: 92%] [G loss: 1.192508]\n",
      "[Epoch 2/200] [Batch 53/938] [D loss: 1.056278, acc: 94%] [G loss: 1.276743]\n",
      "[Epoch 2/200] [Batch 54/938] [D loss: 1.060082, acc: 94%] [G loss: 1.207451]\n",
      "[Epoch 2/200] [Batch 55/938] [D loss: 1.077148, acc: 89%] [G loss: 1.205285]\n",
      "[Epoch 2/200] [Batch 56/938] [D loss: 1.112418, acc: 94%] [G loss: 1.146002]\n",
      "[Epoch 2/200] [Batch 57/938] [D loss: 1.054190, acc: 90%] [G loss: 1.249969]\n",
      "[Epoch 2/200] [Batch 58/938] [D loss: 1.125132, acc: 92%] [G loss: 1.266529]\n",
      "[Epoch 2/200] [Batch 59/938] [D loss: 1.078712, acc: 98%] [G loss: 1.172792]\n",
      "[Epoch 2/200] [Batch 60/938] [D loss: 1.075776, acc: 95%] [G loss: 1.216815]\n",
      "[Epoch 2/200] [Batch 61/938] [D loss: 1.122731, acc: 90%] [G loss: 1.212361]\n",
      "[Epoch 2/200] [Batch 62/938] [D loss: 1.049030, acc: 91%] [G loss: 1.332738]\n",
      "[Epoch 2/200] [Batch 63/938] [D loss: 1.066491, acc: 94%] [G loss: 1.214542]\n",
      "[Epoch 2/200] [Batch 64/938] [D loss: 1.111886, acc: 94%] [G loss: 1.224182]\n",
      "[Epoch 2/200] [Batch 65/938] [D loss: 1.063841, acc: 92%] [G loss: 1.141213]\n",
      "[Epoch 2/200] [Batch 66/938] [D loss: 1.091460, acc: 90%] [G loss: 1.161658]\n",
      "[Epoch 2/200] [Batch 67/938] [D loss: 1.091951, acc: 92%] [G loss: 1.180739]\n",
      "[Epoch 2/200] [Batch 68/938] [D loss: 1.055253, acc: 95%] [G loss: 1.238731]\n",
      "[Epoch 2/200] [Batch 69/938] [D loss: 1.069700, acc: 93%] [G loss: 1.254301]\n",
      "[Epoch 2/200] [Batch 70/938] [D loss: 1.081453, acc: 91%] [G loss: 1.141551]\n",
      "[Epoch 2/200] [Batch 71/938] [D loss: 1.079307, acc: 92%] [G loss: 1.096537]\n",
      "[Epoch 2/200] [Batch 72/938] [D loss: 1.130131, acc: 90%] [G loss: 1.102143]\n",
      "[Epoch 2/200] [Batch 73/938] [D loss: 1.108641, acc: 92%] [G loss: 1.230548]\n",
      "[Epoch 2/200] [Batch 74/938] [D loss: 1.058362, acc: 93%] [G loss: 1.268298]\n",
      "[Epoch 2/200] [Batch 75/938] [D loss: 1.041667, acc: 94%] [G loss: 1.280231]\n",
      "[Epoch 2/200] [Batch 76/938] [D loss: 1.071802, acc: 92%] [G loss: 1.120192]\n",
      "[Epoch 2/200] [Batch 77/938] [D loss: 1.089538, acc: 95%] [G loss: 1.140611]\n",
      "[Epoch 2/200] [Batch 78/938] [D loss: 1.060629, acc: 92%] [G loss: 1.151048]\n",
      "[Epoch 2/200] [Batch 79/938] [D loss: 1.067628, acc: 94%] [G loss: 1.216615]\n",
      "[Epoch 2/200] [Batch 80/938] [D loss: 1.116668, acc: 91%] [G loss: 1.121033]\n",
      "[Epoch 2/200] [Batch 81/938] [D loss: 1.054150, acc: 95%] [G loss: 1.208533]\n",
      "[Epoch 2/200] [Batch 82/938] [D loss: 1.100135, acc: 95%] [G loss: 1.200297]\n",
      "[Epoch 2/200] [Batch 83/938] [D loss: 1.087212, acc: 91%] [G loss: 1.140731]\n",
      "[Epoch 2/200] [Batch 84/938] [D loss: 1.097828, acc: 89%] [G loss: 1.198111]\n",
      "[Epoch 2/200] [Batch 85/938] [D loss: 1.110552, acc: 90%] [G loss: 1.220063]\n",
      "[Epoch 2/200] [Batch 86/938] [D loss: 1.049703, acc: 95%] [G loss: 1.143125]\n",
      "[Epoch 2/200] [Batch 87/938] [D loss: 1.073005, acc: 89%] [G loss: 1.232981]\n",
      "[Epoch 2/200] [Batch 88/938] [D loss: 1.079754, acc: 93%] [G loss: 1.206854]\n",
      "[Epoch 2/200] [Batch 89/938] [D loss: 1.109325, acc: 93%] [G loss: 1.176038]\n",
      "[Epoch 2/200] [Batch 90/938] [D loss: 1.073737, acc: 94%] [G loss: 1.140697]\n",
      "[Epoch 2/200] [Batch 91/938] [D loss: 1.056619, acc: 93%] [G loss: 1.176684]\n",
      "[Epoch 2/200] [Batch 92/938] [D loss: 1.103938, acc: 90%] [G loss: 1.183749]\n",
      "[Epoch 2/200] [Batch 93/938] [D loss: 1.088380, acc: 92%] [G loss: 1.179986]\n",
      "[Epoch 2/200] [Batch 94/938] [D loss: 1.100762, acc: 92%] [G loss: 1.110547]\n",
      "[Epoch 2/200] [Batch 95/938] [D loss: 1.091544, acc: 89%] [G loss: 1.175445]\n",
      "[Epoch 2/200] [Batch 96/938] [D loss: 1.070706, acc: 91%] [G loss: 1.215213]\n",
      "[Epoch 2/200] [Batch 97/938] [D loss: 1.088253, acc: 94%] [G loss: 1.109633]\n",
      "[Epoch 2/200] [Batch 98/938] [D loss: 1.048324, acc: 92%] [G loss: 1.250196]\n",
      "[Epoch 2/200] [Batch 99/938] [D loss: 1.090391, acc: 92%] [G loss: 1.234497]\n",
      "[Epoch 2/200] [Batch 100/938] [D loss: 1.042360, acc: 91%] [G loss: 1.162964]\n",
      "[Epoch 2/200] [Batch 101/938] [D loss: 1.058887, acc: 92%] [G loss: 1.258615]\n",
      "[Epoch 2/200] [Batch 102/938] [D loss: 1.111909, acc: 91%] [G loss: 1.200595]\n",
      "[Epoch 2/200] [Batch 103/938] [D loss: 1.038237, acc: 95%] [G loss: 1.162341]\n",
      "[Epoch 2/200] [Batch 104/938] [D loss: 1.064057, acc: 92%] [G loss: 1.216794]\n",
      "[Epoch 2/200] [Batch 105/938] [D loss: 1.063437, acc: 96%] [G loss: 1.114301]\n",
      "[Epoch 2/200] [Batch 106/938] [D loss: 1.044603, acc: 93%] [G loss: 1.208871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 107/938] [D loss: 1.039314, acc: 95%] [G loss: 1.190061]\n",
      "[Epoch 2/200] [Batch 108/938] [D loss: 1.051823, acc: 95%] [G loss: 1.270427]\n",
      "[Epoch 2/200] [Batch 109/938] [D loss: 1.079331, acc: 90%] [G loss: 1.210922]\n",
      "[Epoch 2/200] [Batch 110/938] [D loss: 1.067799, acc: 93%] [G loss: 1.189546]\n",
      "[Epoch 2/200] [Batch 111/938] [D loss: 1.117964, acc: 88%] [G loss: 1.177274]\n",
      "[Epoch 2/200] [Batch 112/938] [D loss: 1.071062, acc: 96%] [G loss: 1.159407]\n",
      "[Epoch 2/200] [Batch 113/938] [D loss: 1.096241, acc: 89%] [G loss: 1.084659]\n",
      "[Epoch 2/200] [Batch 114/938] [D loss: 1.120517, acc: 87%] [G loss: 1.254713]\n",
      "[Epoch 2/200] [Batch 115/938] [D loss: 1.081268, acc: 94%] [G loss: 1.209117]\n",
      "[Epoch 2/200] [Batch 116/938] [D loss: 1.107633, acc: 92%] [G loss: 1.157788]\n",
      "[Epoch 2/200] [Batch 117/938] [D loss: 1.102468, acc: 93%] [G loss: 1.219024]\n",
      "[Epoch 2/200] [Batch 118/938] [D loss: 1.140484, acc: 89%] [G loss: 1.188647]\n",
      "[Epoch 2/200] [Batch 119/938] [D loss: 1.058849, acc: 93%] [G loss: 1.205954]\n",
      "[Epoch 2/200] [Batch 120/938] [D loss: 1.077103, acc: 95%] [G loss: 1.210648]\n",
      "[Epoch 2/200] [Batch 121/938] [D loss: 1.092776, acc: 92%] [G loss: 1.193633]\n",
      "[Epoch 2/200] [Batch 122/938] [D loss: 1.112580, acc: 89%] [G loss: 1.195151]\n",
      "[Epoch 2/200] [Batch 123/938] [D loss: 1.050139, acc: 96%] [G loss: 1.220123]\n",
      "[Epoch 2/200] [Batch 124/938] [D loss: 1.072948, acc: 93%] [G loss: 1.135016]\n",
      "[Epoch 2/200] [Batch 125/938] [D loss: 1.045480, acc: 92%] [G loss: 1.187509]\n",
      "[Epoch 2/200] [Batch 126/938] [D loss: 1.053212, acc: 93%] [G loss: 1.243976]\n",
      "[Epoch 2/200] [Batch 127/938] [D loss: 1.106348, acc: 90%] [G loss: 1.112657]\n",
      "[Epoch 2/200] [Batch 128/938] [D loss: 1.090047, acc: 91%] [G loss: 1.182473]\n",
      "[Epoch 2/200] [Batch 129/938] [D loss: 1.073844, acc: 94%] [G loss: 1.164271]\n",
      "[Epoch 2/200] [Batch 130/938] [D loss: 1.068140, acc: 96%] [G loss: 1.177822]\n",
      "[Epoch 2/200] [Batch 131/938] [D loss: 1.073394, acc: 93%] [G loss: 1.156475]\n",
      "[Epoch 2/200] [Batch 132/938] [D loss: 1.067723, acc: 93%] [G loss: 1.213567]\n",
      "[Epoch 2/200] [Batch 133/938] [D loss: 1.105255, acc: 92%] [G loss: 1.179439]\n",
      "[Epoch 2/200] [Batch 134/938] [D loss: 1.098088, acc: 88%] [G loss: 1.217608]\n",
      "[Epoch 2/200] [Batch 135/938] [D loss: 1.077646, acc: 91%] [G loss: 1.171627]\n",
      "[Epoch 2/200] [Batch 136/938] [D loss: 1.090303, acc: 93%] [G loss: 1.125033]\n",
      "[Epoch 2/200] [Batch 137/938] [D loss: 1.041906, acc: 91%] [G loss: 1.259542]\n",
      "[Epoch 2/200] [Batch 138/938] [D loss: 1.073148, acc: 96%] [G loss: 1.140753]\n",
      "[Epoch 2/200] [Batch 139/938] [D loss: 1.078084, acc: 92%] [G loss: 1.162547]\n",
      "[Epoch 2/200] [Batch 140/938] [D loss: 1.061269, acc: 93%] [G loss: 1.146332]\n",
      "[Epoch 2/200] [Batch 141/938] [D loss: 1.080155, acc: 92%] [G loss: 1.242334]\n",
      "[Epoch 2/200] [Batch 142/938] [D loss: 1.056512, acc: 92%] [G loss: 1.129461]\n",
      "[Epoch 2/200] [Batch 143/938] [D loss: 1.077495, acc: 90%] [G loss: 1.174938]\n",
      "[Epoch 2/200] [Batch 144/938] [D loss: 1.071135, acc: 92%] [G loss: 1.222095]\n",
      "[Epoch 2/200] [Batch 145/938] [D loss: 1.121593, acc: 92%] [G loss: 1.229969]\n",
      "[Epoch 2/200] [Batch 146/938] [D loss: 1.099010, acc: 89%] [G loss: 1.238735]\n",
      "[Epoch 2/200] [Batch 147/938] [D loss: 1.062099, acc: 93%] [G loss: 1.198700]\n",
      "[Epoch 2/200] [Batch 148/938] [D loss: 1.074728, acc: 95%] [G loss: 1.057973]\n",
      "[Epoch 2/200] [Batch 149/938] [D loss: 1.060159, acc: 92%] [G loss: 1.157302]\n",
      "[Epoch 2/200] [Batch 150/938] [D loss: 1.062953, acc: 96%] [G loss: 1.158903]\n",
      "[Epoch 2/200] [Batch 151/938] [D loss: 1.066975, acc: 92%] [G loss: 1.169893]\n",
      "[Epoch 2/200] [Batch 152/938] [D loss: 1.092703, acc: 90%] [G loss: 1.138725]\n",
      "[Epoch 2/200] [Batch 153/938] [D loss: 1.056613, acc: 96%] [G loss: 1.145458]\n",
      "[Epoch 2/200] [Batch 154/938] [D loss: 1.081176, acc: 91%] [G loss: 1.098510]\n",
      "[Epoch 2/200] [Batch 155/938] [D loss: 1.099820, acc: 89%] [G loss: 1.208804]\n",
      "[Epoch 2/200] [Batch 156/938] [D loss: 1.086875, acc: 92%] [G loss: 1.175722]\n",
      "[Epoch 2/200] [Batch 157/938] [D loss: 1.095495, acc: 90%] [G loss: 1.210212]\n",
      "[Epoch 2/200] [Batch 158/938] [D loss: 1.069975, acc: 94%] [G loss: 1.250205]\n",
      "[Epoch 2/200] [Batch 159/938] [D loss: 1.075345, acc: 94%] [G loss: 1.217119]\n",
      "[Epoch 2/200] [Batch 160/938] [D loss: 1.124443, acc: 96%] [G loss: 1.138195]\n",
      "[Epoch 2/200] [Batch 161/938] [D loss: 1.083260, acc: 91%] [G loss: 1.215057]\n",
      "[Epoch 2/200] [Batch 162/938] [D loss: 1.077140, acc: 92%] [G loss: 1.200199]\n",
      "[Epoch 2/200] [Batch 163/938] [D loss: 1.137979, acc: 93%] [G loss: 1.109922]\n",
      "[Epoch 2/200] [Batch 164/938] [D loss: 1.045827, acc: 94%] [G loss: 1.214811]\n",
      "[Epoch 2/200] [Batch 165/938] [D loss: 1.099156, acc: 90%] [G loss: 1.177455]\n",
      "[Epoch 2/200] [Batch 166/938] [D loss: 1.108614, acc: 92%] [G loss: 1.164641]\n",
      "[Epoch 2/200] [Batch 167/938] [D loss: 1.065937, acc: 94%] [G loss: 1.188650]\n",
      "[Epoch 2/200] [Batch 168/938] [D loss: 1.086026, acc: 91%] [G loss: 1.237641]\n",
      "[Epoch 2/200] [Batch 169/938] [D loss: 1.097140, acc: 92%] [G loss: 1.260233]\n",
      "[Epoch 2/200] [Batch 170/938] [D loss: 1.074255, acc: 94%] [G loss: 1.145371]\n",
      "[Epoch 2/200] [Batch 171/938] [D loss: 1.094233, acc: 93%] [G loss: 1.158519]\n",
      "[Epoch 2/200] [Batch 172/938] [D loss: 1.069277, acc: 90%] [G loss: 1.179006]\n",
      "[Epoch 2/200] [Batch 173/938] [D loss: 1.092102, acc: 93%] [G loss: 1.231469]\n",
      "[Epoch 2/200] [Batch 174/938] [D loss: 1.089137, acc: 92%] [G loss: 1.231707]\n",
      "[Epoch 2/200] [Batch 175/938] [D loss: 1.068283, acc: 92%] [G loss: 1.170175]\n",
      "[Epoch 2/200] [Batch 176/938] [D loss: 1.088236, acc: 93%] [G loss: 1.219692]\n",
      "[Epoch 2/200] [Batch 177/938] [D loss: 1.115247, acc: 93%] [G loss: 1.139679]\n",
      "[Epoch 2/200] [Batch 178/938] [D loss: 1.076569, acc: 97%] [G loss: 1.185516]\n",
      "[Epoch 2/200] [Batch 179/938] [D loss: 1.127667, acc: 90%] [G loss: 1.181501]\n",
      "[Epoch 2/200] [Batch 180/938] [D loss: 1.085651, acc: 93%] [G loss: 1.159402]\n",
      "[Epoch 2/200] [Batch 181/938] [D loss: 1.058345, acc: 92%] [G loss: 1.192807]\n",
      "[Epoch 2/200] [Batch 182/938] [D loss: 1.102149, acc: 93%] [G loss: 1.219392]\n",
      "[Epoch 2/200] [Batch 183/938] [D loss: 1.070836, acc: 92%] [G loss: 1.200104]\n",
      "[Epoch 2/200] [Batch 184/938] [D loss: 1.125557, acc: 91%] [G loss: 1.109972]\n",
      "[Epoch 2/200] [Batch 185/938] [D loss: 1.103290, acc: 90%] [G loss: 1.127339]\n",
      "[Epoch 2/200] [Batch 186/938] [D loss: 1.095084, acc: 91%] [G loss: 1.162456]\n",
      "[Epoch 2/200] [Batch 187/938] [D loss: 1.092501, acc: 91%] [G loss: 1.200647]\n",
      "[Epoch 2/200] [Batch 188/938] [D loss: 1.120582, acc: 90%] [G loss: 1.232179]\n",
      "[Epoch 2/200] [Batch 189/938] [D loss: 1.087358, acc: 92%] [G loss: 1.205302]\n",
      "[Epoch 2/200] [Batch 190/938] [D loss: 1.050881, acc: 95%] [G loss: 1.219920]\n",
      "[Epoch 2/200] [Batch 191/938] [D loss: 1.087958, acc: 92%] [G loss: 1.171110]\n",
      "[Epoch 2/200] [Batch 192/938] [D loss: 1.056430, acc: 93%] [G loss: 1.219467]\n",
      "[Epoch 2/200] [Batch 193/938] [D loss: 1.084526, acc: 94%] [G loss: 1.171024]\n",
      "[Epoch 2/200] [Batch 194/938] [D loss: 1.070617, acc: 90%] [G loss: 1.195574]\n",
      "[Epoch 2/200] [Batch 195/938] [D loss: 1.048211, acc: 92%] [G loss: 1.181644]\n",
      "[Epoch 2/200] [Batch 196/938] [D loss: 1.106147, acc: 92%] [G loss: 1.147476]\n",
      "[Epoch 2/200] [Batch 197/938] [D loss: 1.057844, acc: 92%] [G loss: 1.233530]\n",
      "[Epoch 2/200] [Batch 198/938] [D loss: 1.073482, acc: 93%] [G loss: 1.268068]\n",
      "[Epoch 2/200] [Batch 199/938] [D loss: 1.064219, acc: 93%] [G loss: 1.273090]\n",
      "[Epoch 2/200] [Batch 200/938] [D loss: 1.058363, acc: 98%] [G loss: 1.212539]\n",
      "[Epoch 2/200] [Batch 201/938] [D loss: 1.125455, acc: 91%] [G loss: 1.202578]\n",
      "[Epoch 2/200] [Batch 202/938] [D loss: 1.042954, acc: 92%] [G loss: 1.161192]\n",
      "[Epoch 2/200] [Batch 203/938] [D loss: 1.079258, acc: 96%] [G loss: 1.169821]\n",
      "[Epoch 2/200] [Batch 204/938] [D loss: 1.097386, acc: 89%] [G loss: 1.162138]\n",
      "[Epoch 2/200] [Batch 205/938] [D loss: 1.105533, acc: 88%] [G loss: 1.202871]\n",
      "[Epoch 2/200] [Batch 206/938] [D loss: 1.137062, acc: 92%] [G loss: 1.141434]\n",
      "[Epoch 2/200] [Batch 207/938] [D loss: 1.118058, acc: 91%] [G loss: 1.173400]\n",
      "[Epoch 2/200] [Batch 208/938] [D loss: 1.128577, acc: 90%] [G loss: 1.168254]\n",
      "[Epoch 2/200] [Batch 209/938] [D loss: 1.095621, acc: 95%] [G loss: 1.151312]\n",
      "[Epoch 2/200] [Batch 210/938] [D loss: 1.061504, acc: 95%] [G loss: 1.137432]\n",
      "[Epoch 2/200] [Batch 211/938] [D loss: 1.060228, acc: 95%] [G loss: 1.118509]\n",
      "[Epoch 2/200] [Batch 212/938] [D loss: 1.060095, acc: 92%] [G loss: 1.210208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 213/938] [D loss: 1.060479, acc: 91%] [G loss: 1.280282]\n",
      "[Epoch 2/200] [Batch 214/938] [D loss: 1.105238, acc: 90%] [G loss: 1.147521]\n",
      "[Epoch 2/200] [Batch 215/938] [D loss: 1.101566, acc: 92%] [G loss: 1.130339]\n",
      "[Epoch 2/200] [Batch 216/938] [D loss: 1.087903, acc: 90%] [G loss: 1.121422]\n",
      "[Epoch 2/200] [Batch 217/938] [D loss: 1.079043, acc: 95%] [G loss: 1.256165]\n",
      "[Epoch 2/200] [Batch 218/938] [D loss: 1.075976, acc: 92%] [G loss: 1.145417]\n",
      "[Epoch 2/200] [Batch 219/938] [D loss: 1.082336, acc: 94%] [G loss: 1.201916]\n",
      "[Epoch 2/200] [Batch 220/938] [D loss: 1.038555, acc: 96%] [G loss: 1.268795]\n",
      "[Epoch 2/200] [Batch 221/938] [D loss: 1.100896, acc: 91%] [G loss: 1.187418]\n",
      "[Epoch 2/200] [Batch 222/938] [D loss: 1.120338, acc: 94%] [G loss: 1.216826]\n",
      "[Epoch 2/200] [Batch 223/938] [D loss: 1.080180, acc: 94%] [G loss: 1.198048]\n",
      "[Epoch 2/200] [Batch 224/938] [D loss: 1.107908, acc: 91%] [G loss: 1.129198]\n",
      "[Epoch 2/200] [Batch 225/938] [D loss: 1.092399, acc: 93%] [G loss: 1.200347]\n",
      "[Epoch 2/200] [Batch 226/938] [D loss: 1.076021, acc: 91%] [G loss: 1.191036]\n",
      "[Epoch 2/200] [Batch 227/938] [D loss: 1.069892, acc: 93%] [G loss: 1.205860]\n",
      "[Epoch 2/200] [Batch 228/938] [D loss: 1.110708, acc: 92%] [G loss: 1.108521]\n",
      "[Epoch 2/200] [Batch 229/938] [D loss: 1.029011, acc: 96%] [G loss: 1.157311]\n",
      "[Epoch 2/200] [Batch 230/938] [D loss: 1.104763, acc: 92%] [G loss: 1.244727]\n",
      "[Epoch 2/200] [Batch 231/938] [D loss: 1.076696, acc: 91%] [G loss: 1.177126]\n",
      "[Epoch 2/200] [Batch 232/938] [D loss: 1.075865, acc: 94%] [G loss: 1.240785]\n",
      "[Epoch 2/200] [Batch 233/938] [D loss: 1.079880, acc: 93%] [G loss: 1.216633]\n",
      "[Epoch 2/200] [Batch 234/938] [D loss: 1.081105, acc: 92%] [G loss: 1.183347]\n",
      "[Epoch 2/200] [Batch 235/938] [D loss: 1.096800, acc: 91%] [G loss: 1.117377]\n",
      "[Epoch 2/200] [Batch 236/938] [D loss: 1.144345, acc: 90%] [G loss: 1.139717]\n",
      "[Epoch 2/200] [Batch 237/938] [D loss: 1.092442, acc: 92%] [G loss: 1.195456]\n",
      "[Epoch 2/200] [Batch 238/938] [D loss: 1.087045, acc: 94%] [G loss: 1.188980]\n",
      "[Epoch 2/200] [Batch 239/938] [D loss: 1.084680, acc: 92%] [G loss: 1.148394]\n",
      "[Epoch 2/200] [Batch 240/938] [D loss: 1.061003, acc: 94%] [G loss: 1.168998]\n",
      "[Epoch 2/200] [Batch 241/938] [D loss: 1.105258, acc: 96%] [G loss: 1.277149]\n",
      "[Epoch 2/200] [Batch 242/938] [D loss: 1.066266, acc: 93%] [G loss: 1.191662]\n",
      "[Epoch 2/200] [Batch 243/938] [D loss: 1.066123, acc: 94%] [G loss: 1.158946]\n",
      "[Epoch 2/200] [Batch 244/938] [D loss: 1.023920, acc: 96%] [G loss: 1.159533]\n",
      "[Epoch 2/200] [Batch 245/938] [D loss: 1.074860, acc: 93%] [G loss: 1.236461]\n",
      "[Epoch 2/200] [Batch 246/938] [D loss: 1.086444, acc: 92%] [G loss: 1.189430]\n",
      "[Epoch 2/200] [Batch 247/938] [D loss: 1.118471, acc: 88%] [G loss: 1.246801]\n",
      "[Epoch 2/200] [Batch 248/938] [D loss: 1.085839, acc: 91%] [G loss: 1.111234]\n",
      "[Epoch 2/200] [Batch 249/938] [D loss: 1.047717, acc: 92%] [G loss: 1.212993]\n",
      "[Epoch 2/200] [Batch 250/938] [D loss: 1.072121, acc: 92%] [G loss: 1.138595]\n",
      "[Epoch 2/200] [Batch 251/938] [D loss: 1.125308, acc: 87%] [G loss: 1.153225]\n",
      "[Epoch 2/200] [Batch 252/938] [D loss: 1.103062, acc: 92%] [G loss: 1.131001]\n",
      "[Epoch 2/200] [Batch 253/938] [D loss: 1.056002, acc: 97%] [G loss: 1.095885]\n",
      "[Epoch 2/200] [Batch 254/938] [D loss: 1.088964, acc: 90%] [G loss: 1.153584]\n",
      "[Epoch 2/200] [Batch 255/938] [D loss: 1.092901, acc: 93%] [G loss: 1.160956]\n",
      "[Epoch 2/200] [Batch 256/938] [D loss: 1.105340, acc: 88%] [G loss: 1.202382]\n",
      "[Epoch 2/200] [Batch 257/938] [D loss: 1.119079, acc: 96%] [G loss: 1.131204]\n",
      "[Epoch 2/200] [Batch 258/938] [D loss: 1.071375, acc: 91%] [G loss: 1.180035]\n",
      "[Epoch 2/200] [Batch 259/938] [D loss: 1.089334, acc: 92%] [G loss: 1.189860]\n",
      "[Epoch 2/200] [Batch 260/938] [D loss: 1.069201, acc: 91%] [G loss: 1.155090]\n",
      "[Epoch 2/200] [Batch 261/938] [D loss: 1.051678, acc: 94%] [G loss: 1.156877]\n",
      "[Epoch 2/200] [Batch 262/938] [D loss: 1.071238, acc: 92%] [G loss: 1.200126]\n",
      "[Epoch 2/200] [Batch 263/938] [D loss: 1.060055, acc: 90%] [G loss: 1.218352]\n",
      "[Epoch 2/200] [Batch 264/938] [D loss: 1.109295, acc: 86%] [G loss: 1.117762]\n",
      "[Epoch 2/200] [Batch 265/938] [D loss: 1.067770, acc: 95%] [G loss: 1.190178]\n",
      "[Epoch 2/200] [Batch 266/938] [D loss: 1.059342, acc: 94%] [G loss: 1.227374]\n",
      "[Epoch 2/200] [Batch 267/938] [D loss: 1.112543, acc: 94%] [G loss: 1.222302]\n",
      "[Epoch 2/200] [Batch 268/938] [D loss: 1.107134, acc: 91%] [G loss: 1.181655]\n",
      "[Epoch 2/200] [Batch 269/938] [D loss: 1.095815, acc: 92%] [G loss: 1.258450]\n",
      "[Epoch 2/200] [Batch 270/938] [D loss: 1.047338, acc: 93%] [G loss: 1.122363]\n",
      "[Epoch 2/200] [Batch 271/938] [D loss: 1.097652, acc: 92%] [G loss: 1.155218]\n",
      "[Epoch 2/200] [Batch 272/938] [D loss: 1.089067, acc: 91%] [G loss: 1.148706]\n",
      "[Epoch 2/200] [Batch 273/938] [D loss: 1.048129, acc: 95%] [G loss: 1.209597]\n",
      "[Epoch 2/200] [Batch 274/938] [D loss: 1.101022, acc: 91%] [G loss: 1.247463]\n",
      "[Epoch 2/200] [Batch 275/938] [D loss: 1.089321, acc: 94%] [G loss: 1.228793]\n",
      "[Epoch 2/200] [Batch 276/938] [D loss: 1.090614, acc: 93%] [G loss: 1.199884]\n",
      "[Epoch 2/200] [Batch 277/938] [D loss: 1.111802, acc: 94%] [G loss: 1.144000]\n",
      "[Epoch 2/200] [Batch 278/938] [D loss: 1.050939, acc: 95%] [G loss: 1.218911]\n",
      "[Epoch 2/200] [Batch 279/938] [D loss: 1.094474, acc: 94%] [G loss: 1.212220]\n",
      "[Epoch 2/200] [Batch 280/938] [D loss: 1.064093, acc: 94%] [G loss: 1.190545]\n",
      "[Epoch 2/200] [Batch 281/938] [D loss: 1.085731, acc: 93%] [G loss: 1.139746]\n",
      "[Epoch 2/200] [Batch 282/938] [D loss: 1.030960, acc: 94%] [G loss: 1.131741]\n",
      "[Epoch 2/200] [Batch 283/938] [D loss: 1.072073, acc: 96%] [G loss: 1.174203]\n",
      "[Epoch 2/200] [Batch 284/938] [D loss: 1.053217, acc: 97%] [G loss: 1.196306]\n",
      "[Epoch 2/200] [Batch 285/938] [D loss: 1.102435, acc: 94%] [G loss: 1.205884]\n",
      "[Epoch 2/200] [Batch 286/938] [D loss: 1.089517, acc: 93%] [G loss: 1.229068]\n",
      "[Epoch 2/200] [Batch 287/938] [D loss: 1.072134, acc: 92%] [G loss: 1.121961]\n",
      "[Epoch 2/200] [Batch 288/938] [D loss: 1.055956, acc: 94%] [G loss: 1.090899]\n",
      "[Epoch 2/200] [Batch 289/938] [D loss: 1.090131, acc: 93%] [G loss: 1.213060]\n",
      "[Epoch 2/200] [Batch 290/938] [D loss: 1.079083, acc: 92%] [G loss: 1.246474]\n",
      "[Epoch 2/200] [Batch 291/938] [D loss: 1.060997, acc: 94%] [G loss: 1.209618]\n",
      "[Epoch 2/200] [Batch 292/938] [D loss: 1.087143, acc: 92%] [G loss: 1.186278]\n",
      "[Epoch 2/200] [Batch 293/938] [D loss: 1.107064, acc: 92%] [G loss: 1.162940]\n",
      "[Epoch 2/200] [Batch 294/938] [D loss: 1.112515, acc: 90%] [G loss: 1.134164]\n",
      "[Epoch 2/200] [Batch 295/938] [D loss: 1.092401, acc: 93%] [G loss: 1.222510]\n",
      "[Epoch 2/200] [Batch 296/938] [D loss: 1.048132, acc: 93%] [G loss: 1.167712]\n",
      "[Epoch 2/200] [Batch 297/938] [D loss: 1.073286, acc: 93%] [G loss: 1.180029]\n",
      "[Epoch 2/200] [Batch 298/938] [D loss: 1.089530, acc: 95%] [G loss: 1.207396]\n",
      "[Epoch 2/200] [Batch 299/938] [D loss: 1.086067, acc: 90%] [G loss: 1.209122]\n",
      "[Epoch 2/200] [Batch 300/938] [D loss: 1.121056, acc: 91%] [G loss: 1.131550]\n",
      "[Epoch 2/200] [Batch 301/938] [D loss: 1.099036, acc: 93%] [G loss: 1.111707]\n",
      "[Epoch 2/200] [Batch 302/938] [D loss: 1.103657, acc: 91%] [G loss: 1.202180]\n",
      "[Epoch 2/200] [Batch 303/938] [D loss: 1.078346, acc: 88%] [G loss: 1.120105]\n",
      "[Epoch 2/200] [Batch 304/938] [D loss: 1.069359, acc: 89%] [G loss: 1.185407]\n",
      "[Epoch 2/200] [Batch 305/938] [D loss: 1.089862, acc: 93%] [G loss: 1.217532]\n",
      "[Epoch 2/200] [Batch 306/938] [D loss: 1.082891, acc: 94%] [G loss: 1.182567]\n",
      "[Epoch 2/200] [Batch 307/938] [D loss: 1.080575, acc: 94%] [G loss: 1.161467]\n",
      "[Epoch 2/200] [Batch 308/938] [D loss: 1.072282, acc: 96%] [G loss: 1.108127]\n",
      "[Epoch 2/200] [Batch 309/938] [D loss: 1.056115, acc: 94%] [G loss: 1.259837]\n",
      "[Epoch 2/200] [Batch 310/938] [D loss: 1.112819, acc: 92%] [G loss: 1.204730]\n",
      "[Epoch 2/200] [Batch 311/938] [D loss: 1.071289, acc: 90%] [G loss: 1.207471]\n",
      "[Epoch 2/200] [Batch 312/938] [D loss: 1.071085, acc: 96%] [G loss: 1.122070]\n",
      "[Epoch 2/200] [Batch 313/938] [D loss: 1.097036, acc: 92%] [G loss: 1.147979]\n",
      "[Epoch 2/200] [Batch 314/938] [D loss: 1.076647, acc: 90%] [G loss: 1.225036]\n",
      "[Epoch 2/200] [Batch 315/938] [D loss: 1.113067, acc: 92%] [G loss: 1.130435]\n",
      "[Epoch 2/200] [Batch 316/938] [D loss: 1.050630, acc: 96%] [G loss: 1.195816]\n",
      "[Epoch 2/200] [Batch 317/938] [D loss: 1.045767, acc: 95%] [G loss: 1.155858]\n",
      "[Epoch 2/200] [Batch 318/938] [D loss: 1.083494, acc: 94%] [G loss: 1.213221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 319/938] [D loss: 1.102903, acc: 89%] [G loss: 1.271832]\n",
      "[Epoch 2/200] [Batch 320/938] [D loss: 1.049786, acc: 89%] [G loss: 1.249318]\n",
      "[Epoch 2/200] [Batch 321/938] [D loss: 1.054102, acc: 95%] [G loss: 1.209340]\n",
      "[Epoch 2/200] [Batch 322/938] [D loss: 1.111116, acc: 89%] [G loss: 1.181624]\n",
      "[Epoch 2/200] [Batch 323/938] [D loss: 1.078224, acc: 91%] [G loss: 1.144606]\n",
      "[Epoch 2/200] [Batch 324/938] [D loss: 1.059552, acc: 89%] [G loss: 1.198100]\n",
      "[Epoch 2/200] [Batch 325/938] [D loss: 1.104737, acc: 92%] [G loss: 1.143250]\n",
      "[Epoch 2/200] [Batch 326/938] [D loss: 1.035690, acc: 92%] [G loss: 1.263182]\n",
      "[Epoch 2/200] [Batch 327/938] [D loss: 1.090132, acc: 92%] [G loss: 1.177783]\n",
      "[Epoch 2/200] [Batch 328/938] [D loss: 1.060285, acc: 95%] [G loss: 1.220115]\n",
      "[Epoch 2/200] [Batch 329/938] [D loss: 1.110611, acc: 92%] [G loss: 1.134801]\n",
      "[Epoch 2/200] [Batch 330/938] [D loss: 1.094442, acc: 93%] [G loss: 1.131503]\n",
      "[Epoch 2/200] [Batch 331/938] [D loss: 1.116585, acc: 95%] [G loss: 1.140475]\n",
      "[Epoch 2/200] [Batch 332/938] [D loss: 1.080785, acc: 92%] [G loss: 1.202647]\n",
      "[Epoch 2/200] [Batch 333/938] [D loss: 1.060645, acc: 89%] [G loss: 1.234654]\n",
      "[Epoch 2/200] [Batch 334/938] [D loss: 1.088915, acc: 94%] [G loss: 1.191690]\n",
      "[Epoch 2/200] [Batch 335/938] [D loss: 1.106825, acc: 92%] [G loss: 1.110217]\n",
      "[Epoch 2/200] [Batch 336/938] [D loss: 1.086873, acc: 95%] [G loss: 1.153826]\n",
      "[Epoch 2/200] [Batch 337/938] [D loss: 1.063906, acc: 90%] [G loss: 1.221810]\n",
      "[Epoch 2/200] [Batch 338/938] [D loss: 1.095069, acc: 95%] [G loss: 1.230419]\n",
      "[Epoch 2/200] [Batch 339/938] [D loss: 1.133184, acc: 92%] [G loss: 1.145039]\n",
      "[Epoch 2/200] [Batch 340/938] [D loss: 1.069450, acc: 94%] [G loss: 1.212357]\n",
      "[Epoch 2/200] [Batch 341/938] [D loss: 1.106158, acc: 91%] [G loss: 1.095058]\n",
      "[Epoch 2/200] [Batch 342/938] [D loss: 1.059479, acc: 96%] [G loss: 1.139280]\n",
      "[Epoch 2/200] [Batch 343/938] [D loss: 1.064794, acc: 95%] [G loss: 1.167024]\n",
      "[Epoch 2/200] [Batch 344/938] [D loss: 1.040664, acc: 96%] [G loss: 1.099781]\n",
      "[Epoch 2/200] [Batch 345/938] [D loss: 1.088775, acc: 92%] [G loss: 1.171761]\n",
      "[Epoch 2/200] [Batch 346/938] [D loss: 1.079635, acc: 94%] [G loss: 1.269508]\n",
      "[Epoch 2/200] [Batch 347/938] [D loss: 1.071715, acc: 93%] [G loss: 1.232160]\n",
      "[Epoch 2/200] [Batch 348/938] [D loss: 1.049481, acc: 97%] [G loss: 1.172277]\n",
      "[Epoch 2/200] [Batch 349/938] [D loss: 1.067229, acc: 94%] [G loss: 1.110911]\n",
      "[Epoch 2/200] [Batch 350/938] [D loss: 1.109185, acc: 94%] [G loss: 1.191446]\n",
      "[Epoch 2/200] [Batch 351/938] [D loss: 1.086972, acc: 94%] [G loss: 1.175089]\n",
      "[Epoch 2/200] [Batch 352/938] [D loss: 1.081996, acc: 96%] [G loss: 1.186509]\n",
      "[Epoch 2/200] [Batch 353/938] [D loss: 1.076544, acc: 96%] [G loss: 1.214812]\n",
      "[Epoch 2/200] [Batch 354/938] [D loss: 1.066690, acc: 96%] [G loss: 1.157531]\n",
      "[Epoch 2/200] [Batch 355/938] [D loss: 1.061976, acc: 89%] [G loss: 1.142817]\n",
      "[Epoch 2/200] [Batch 356/938] [D loss: 1.090136, acc: 93%] [G loss: 1.099433]\n",
      "[Epoch 2/200] [Batch 357/938] [D loss: 1.073914, acc: 95%] [G loss: 1.124098]\n",
      "[Epoch 2/200] [Batch 358/938] [D loss: 1.066129, acc: 97%] [G loss: 1.153784]\n",
      "[Epoch 2/200] [Batch 359/938] [D loss: 1.072410, acc: 94%] [G loss: 1.234904]\n",
      "[Epoch 2/200] [Batch 360/938] [D loss: 1.098716, acc: 93%] [G loss: 1.190182]\n",
      "[Epoch 2/200] [Batch 361/938] [D loss: 1.090393, acc: 93%] [G loss: 1.208812]\n",
      "[Epoch 2/200] [Batch 362/938] [D loss: 1.098419, acc: 93%] [G loss: 1.187286]\n",
      "[Epoch 2/200] [Batch 363/938] [D loss: 1.087066, acc: 92%] [G loss: 1.224754]\n",
      "[Epoch 2/200] [Batch 364/938] [D loss: 1.102798, acc: 92%] [G loss: 1.224401]\n",
      "[Epoch 2/200] [Batch 365/938] [D loss: 1.056073, acc: 93%] [G loss: 1.201473]\n",
      "[Epoch 2/200] [Batch 366/938] [D loss: 1.089618, acc: 92%] [G loss: 1.174608]\n",
      "[Epoch 2/200] [Batch 367/938] [D loss: 1.082743, acc: 92%] [G loss: 1.159743]\n",
      "[Epoch 2/200] [Batch 368/938] [D loss: 1.093760, acc: 92%] [G loss: 1.251876]\n",
      "[Epoch 2/200] [Batch 369/938] [D loss: 1.109420, acc: 92%] [G loss: 1.192712]\n",
      "[Epoch 2/200] [Batch 370/938] [D loss: 1.063809, acc: 95%] [G loss: 1.193819]\n",
      "[Epoch 2/200] [Batch 371/938] [D loss: 1.095973, acc: 94%] [G loss: 1.092728]\n",
      "[Epoch 2/200] [Batch 372/938] [D loss: 1.054342, acc: 94%] [G loss: 1.136870]\n",
      "[Epoch 2/200] [Batch 373/938] [D loss: 1.080203, acc: 96%] [G loss: 1.177026]\n",
      "[Epoch 2/200] [Batch 374/938] [D loss: 1.044592, acc: 96%] [G loss: 1.234179]\n",
      "[Epoch 2/200] [Batch 375/938] [D loss: 1.141754, acc: 93%] [G loss: 1.144376]\n",
      "[Epoch 2/200] [Batch 376/938] [D loss: 1.130829, acc: 92%] [G loss: 1.195828]\n",
      "[Epoch 2/200] [Batch 377/938] [D loss: 1.072453, acc: 95%] [G loss: 1.160888]\n",
      "[Epoch 2/200] [Batch 378/938] [D loss: 1.135739, acc: 91%] [G loss: 1.143959]\n",
      "[Epoch 2/200] [Batch 379/938] [D loss: 1.084920, acc: 95%] [G loss: 1.108284]\n",
      "[Epoch 2/200] [Batch 380/938] [D loss: 1.087664, acc: 92%] [G loss: 1.216449]\n",
      "[Epoch 2/200] [Batch 381/938] [D loss: 1.114207, acc: 94%] [G loss: 1.186019]\n",
      "[Epoch 2/200] [Batch 382/938] [D loss: 1.057342, acc: 92%] [G loss: 1.189170]\n",
      "[Epoch 2/200] [Batch 383/938] [D loss: 1.075680, acc: 93%] [G loss: 1.106344]\n",
      "[Epoch 2/200] [Batch 384/938] [D loss: 1.026102, acc: 96%] [G loss: 1.172923]\n",
      "[Epoch 2/200] [Batch 385/938] [D loss: 1.088868, acc: 94%] [G loss: 1.134950]\n",
      "[Epoch 2/200] [Batch 386/938] [D loss: 1.075931, acc: 92%] [G loss: 1.210831]\n",
      "[Epoch 2/200] [Batch 387/938] [D loss: 1.088203, acc: 95%] [G loss: 1.254642]\n",
      "[Epoch 2/200] [Batch 388/938] [D loss: 1.051139, acc: 92%] [G loss: 1.250162]\n",
      "[Epoch 2/200] [Batch 389/938] [D loss: 1.083111, acc: 96%] [G loss: 1.221654]\n",
      "[Epoch 2/200] [Batch 390/938] [D loss: 1.075609, acc: 94%] [G loss: 1.189273]\n",
      "[Epoch 2/200] [Batch 391/938] [D loss: 1.102202, acc: 94%] [G loss: 1.259696]\n",
      "[Epoch 2/200] [Batch 392/938] [D loss: 1.104017, acc: 96%] [G loss: 1.142323]\n",
      "[Epoch 2/200] [Batch 393/938] [D loss: 1.075932, acc: 95%] [G loss: 1.185042]\n",
      "[Epoch 2/200] [Batch 394/938] [D loss: 1.032543, acc: 95%] [G loss: 1.181203]\n",
      "[Epoch 2/200] [Batch 395/938] [D loss: 1.085340, acc: 93%] [G loss: 1.246508]\n",
      "[Epoch 2/200] [Batch 396/938] [D loss: 1.135394, acc: 90%] [G loss: 1.235592]\n",
      "[Epoch 2/200] [Batch 397/938] [D loss: 1.063010, acc: 92%] [G loss: 1.204101]\n",
      "[Epoch 2/200] [Batch 398/938] [D loss: 1.056283, acc: 95%] [G loss: 1.188505]\n",
      "[Epoch 2/200] [Batch 399/938] [D loss: 1.048777, acc: 96%] [G loss: 1.199028]\n",
      "[Epoch 2/200] [Batch 400/938] [D loss: 1.071475, acc: 90%] [G loss: 1.179762]\n",
      "[Epoch 2/200] [Batch 401/938] [D loss: 1.093466, acc: 92%] [G loss: 1.189950]\n",
      "[Epoch 2/200] [Batch 402/938] [D loss: 1.069782, acc: 93%] [G loss: 1.180694]\n",
      "[Epoch 2/200] [Batch 403/938] [D loss: 1.108603, acc: 89%] [G loss: 1.126892]\n",
      "[Epoch 2/200] [Batch 404/938] [D loss: 1.072927, acc: 94%] [G loss: 1.193709]\n",
      "[Epoch 2/200] [Batch 405/938] [D loss: 1.080036, acc: 90%] [G loss: 1.233233]\n",
      "[Epoch 2/200] [Batch 406/938] [D loss: 1.062383, acc: 96%] [G loss: 1.164944]\n",
      "[Epoch 2/200] [Batch 407/938] [D loss: 1.097266, acc: 94%] [G loss: 1.176648]\n",
      "[Epoch 2/200] [Batch 408/938] [D loss: 1.050868, acc: 96%] [G loss: 1.140246]\n",
      "[Epoch 2/200] [Batch 409/938] [D loss: 1.093040, acc: 93%] [G loss: 1.180545]\n",
      "[Epoch 2/200] [Batch 410/938] [D loss: 1.060697, acc: 93%] [G loss: 1.219903]\n",
      "[Epoch 2/200] [Batch 411/938] [D loss: 1.118793, acc: 94%] [G loss: 1.215409]\n",
      "[Epoch 2/200] [Batch 412/938] [D loss: 1.084778, acc: 90%] [G loss: 1.284648]\n",
      "[Epoch 2/200] [Batch 413/938] [D loss: 1.106405, acc: 88%] [G loss: 1.159721]\n",
      "[Epoch 2/200] [Batch 414/938] [D loss: 1.087140, acc: 92%] [G loss: 1.220386]\n",
      "[Epoch 2/200] [Batch 415/938] [D loss: 1.074849, acc: 96%] [G loss: 1.181824]\n",
      "[Epoch 2/200] [Batch 416/938] [D loss: 1.068143, acc: 96%] [G loss: 1.234501]\n",
      "[Epoch 2/200] [Batch 417/938] [D loss: 1.075394, acc: 94%] [G loss: 1.183148]\n",
      "[Epoch 2/200] [Batch 418/938] [D loss: 1.123759, acc: 90%] [G loss: 1.261175]\n",
      "[Epoch 2/200] [Batch 419/938] [D loss: 1.059274, acc: 93%] [G loss: 1.137476]\n",
      "[Epoch 2/200] [Batch 420/938] [D loss: 1.097690, acc: 90%] [G loss: 1.223231]\n",
      "[Epoch 2/200] [Batch 421/938] [D loss: 1.111766, acc: 92%] [G loss: 1.149373]\n",
      "[Epoch 2/200] [Batch 422/938] [D loss: 1.061393, acc: 92%] [G loss: 1.212767]\n",
      "[Epoch 2/200] [Batch 423/938] [D loss: 1.098271, acc: 91%] [G loss: 1.241009]\n",
      "[Epoch 2/200] [Batch 424/938] [D loss: 1.064011, acc: 94%] [G loss: 1.165884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 425/938] [D loss: 1.093765, acc: 93%] [G loss: 1.134894]\n",
      "[Epoch 2/200] [Batch 426/938] [D loss: 1.091266, acc: 89%] [G loss: 1.106836]\n",
      "[Epoch 2/200] [Batch 427/938] [D loss: 1.084486, acc: 92%] [G loss: 1.179195]\n",
      "[Epoch 2/200] [Batch 428/938] [D loss: 1.065375, acc: 96%] [G loss: 1.163546]\n",
      "[Epoch 2/200] [Batch 429/938] [D loss: 1.052460, acc: 96%] [G loss: 1.175261]\n",
      "[Epoch 2/200] [Batch 430/938] [D loss: 1.093300, acc: 93%] [G loss: 1.112750]\n",
      "[Epoch 2/200] [Batch 431/938] [D loss: 1.054741, acc: 94%] [G loss: 1.194234]\n",
      "[Epoch 2/200] [Batch 432/938] [D loss: 1.119252, acc: 90%] [G loss: 1.218548]\n",
      "[Epoch 2/200] [Batch 433/938] [D loss: 1.063872, acc: 96%] [G loss: 1.133448]\n",
      "[Epoch 2/200] [Batch 434/938] [D loss: 1.061647, acc: 93%] [G loss: 1.211649]\n",
      "[Epoch 2/200] [Batch 435/938] [D loss: 1.074428, acc: 92%] [G loss: 1.162088]\n",
      "[Epoch 2/200] [Batch 436/938] [D loss: 1.097741, acc: 94%] [G loss: 1.185436]\n",
      "[Epoch 2/200] [Batch 437/938] [D loss: 1.102136, acc: 92%] [G loss: 1.136343]\n",
      "[Epoch 2/200] [Batch 438/938] [D loss: 1.084049, acc: 94%] [G loss: 1.199376]\n",
      "[Epoch 2/200] [Batch 439/938] [D loss: 1.073105, acc: 96%] [G loss: 1.199922]\n",
      "[Epoch 2/200] [Batch 440/938] [D loss: 1.108813, acc: 92%] [G loss: 1.174956]\n",
      "[Epoch 2/200] [Batch 441/938] [D loss: 1.081957, acc: 94%] [G loss: 1.186758]\n",
      "[Epoch 2/200] [Batch 442/938] [D loss: 1.070164, acc: 94%] [G loss: 1.163215]\n",
      "[Epoch 2/200] [Batch 443/938] [D loss: 1.073892, acc: 96%] [G loss: 1.133521]\n",
      "[Epoch 2/200] [Batch 444/938] [D loss: 1.036059, acc: 96%] [G loss: 1.252147]\n",
      "[Epoch 2/200] [Batch 445/938] [D loss: 1.055279, acc: 96%] [G loss: 1.246048]\n",
      "[Epoch 2/200] [Batch 446/938] [D loss: 1.053371, acc: 92%] [G loss: 1.270223]\n",
      "[Epoch 2/200] [Batch 447/938] [D loss: 1.090488, acc: 93%] [G loss: 1.143394]\n",
      "[Epoch 2/200] [Batch 448/938] [D loss: 1.136031, acc: 92%] [G loss: 1.168741]\n",
      "[Epoch 2/200] [Batch 449/938] [D loss: 1.084536, acc: 92%] [G loss: 1.150118]\n",
      "[Epoch 2/200] [Batch 450/938] [D loss: 1.059591, acc: 94%] [G loss: 1.223510]\n",
      "[Epoch 2/200] [Batch 451/938] [D loss: 1.126568, acc: 90%] [G loss: 1.177555]\n",
      "[Epoch 2/200] [Batch 452/938] [D loss: 1.076702, acc: 94%] [G loss: 1.170592]\n",
      "[Epoch 2/200] [Batch 453/938] [D loss: 1.058944, acc: 92%] [G loss: 1.131480]\n",
      "[Epoch 2/200] [Batch 454/938] [D loss: 1.082939, acc: 95%] [G loss: 1.177735]\n",
      "[Epoch 2/200] [Batch 455/938] [D loss: 1.071142, acc: 92%] [G loss: 1.171788]\n",
      "[Epoch 2/200] [Batch 456/938] [D loss: 1.125966, acc: 94%] [G loss: 1.237939]\n",
      "[Epoch 2/200] [Batch 457/938] [D loss: 1.085717, acc: 95%] [G loss: 1.081632]\n",
      "[Epoch 2/200] [Batch 458/938] [D loss: 1.060456, acc: 93%] [G loss: 1.146033]\n",
      "[Epoch 2/200] [Batch 459/938] [D loss: 1.063179, acc: 96%] [G loss: 1.154736]\n",
      "[Epoch 2/200] [Batch 460/938] [D loss: 1.044513, acc: 94%] [G loss: 1.271230]\n",
      "[Epoch 2/200] [Batch 461/938] [D loss: 1.111579, acc: 96%] [G loss: 1.184262]\n",
      "[Epoch 2/200] [Batch 462/938] [D loss: 1.074137, acc: 96%] [G loss: 1.191975]\n",
      "[Epoch 2/200] [Batch 463/938] [D loss: 1.093258, acc: 92%] [G loss: 1.152591]\n",
      "[Epoch 2/200] [Batch 464/938] [D loss: 1.070763, acc: 95%] [G loss: 1.162550]\n",
      "[Epoch 2/200] [Batch 465/938] [D loss: 1.102019, acc: 95%] [G loss: 1.165700]\n",
      "[Epoch 2/200] [Batch 466/938] [D loss: 1.097803, acc: 92%] [G loss: 1.171114]\n",
      "[Epoch 2/200] [Batch 467/938] [D loss: 1.069122, acc: 95%] [G loss: 1.123505]\n",
      "[Epoch 2/200] [Batch 468/938] [D loss: 1.113056, acc: 95%] [G loss: 1.178262]\n",
      "[Epoch 2/200] [Batch 469/938] [D loss: 1.096758, acc: 95%] [G loss: 1.166715]\n",
      "[Epoch 2/200] [Batch 470/938] [D loss: 1.063498, acc: 93%] [G loss: 1.148206]\n",
      "[Epoch 2/200] [Batch 471/938] [D loss: 1.068786, acc: 96%] [G loss: 1.113367]\n",
      "[Epoch 2/200] [Batch 472/938] [D loss: 1.044829, acc: 95%] [G loss: 1.145988]\n",
      "[Epoch 2/200] [Batch 473/938] [D loss: 1.036214, acc: 95%] [G loss: 1.213474]\n",
      "[Epoch 2/200] [Batch 474/938] [D loss: 1.060670, acc: 94%] [G loss: 1.209459]\n",
      "[Epoch 2/200] [Batch 475/938] [D loss: 1.096739, acc: 89%] [G loss: 1.120174]\n",
      "[Epoch 2/200] [Batch 476/938] [D loss: 1.063695, acc: 96%] [G loss: 1.176845]\n",
      "[Epoch 2/200] [Batch 477/938] [D loss: 1.018806, acc: 97%] [G loss: 1.169538]\n",
      "[Epoch 2/200] [Batch 478/938] [D loss: 1.076250, acc: 95%] [G loss: 1.155902]\n",
      "[Epoch 2/200] [Batch 479/938] [D loss: 1.088345, acc: 89%] [G loss: 1.267317]\n",
      "[Epoch 2/200] [Batch 480/938] [D loss: 1.084250, acc: 90%] [G loss: 1.213507]\n",
      "[Epoch 2/200] [Batch 481/938] [D loss: 1.081150, acc: 89%] [G loss: 1.129712]\n",
      "[Epoch 2/200] [Batch 482/938] [D loss: 1.041420, acc: 96%] [G loss: 1.215353]\n",
      "[Epoch 2/200] [Batch 483/938] [D loss: 1.074978, acc: 94%] [G loss: 1.168972]\n",
      "[Epoch 2/200] [Batch 484/938] [D loss: 1.075430, acc: 90%] [G loss: 1.191031]\n",
      "[Epoch 2/200] [Batch 485/938] [D loss: 1.084105, acc: 92%] [G loss: 1.126831]\n",
      "[Epoch 2/200] [Batch 486/938] [D loss: 1.072813, acc: 96%] [G loss: 1.132058]\n",
      "[Epoch 2/200] [Batch 487/938] [D loss: 1.077023, acc: 97%] [G loss: 1.128207]\n",
      "[Epoch 2/200] [Batch 488/938] [D loss: 1.125016, acc: 91%] [G loss: 1.154442]\n",
      "[Epoch 2/200] [Batch 489/938] [D loss: 1.049192, acc: 96%] [G loss: 1.261640]\n",
      "[Epoch 2/200] [Batch 490/938] [D loss: 1.098251, acc: 95%] [G loss: 1.177751]\n",
      "[Epoch 2/200] [Batch 491/938] [D loss: 1.104422, acc: 93%] [G loss: 1.168142]\n",
      "[Epoch 2/200] [Batch 492/938] [D loss: 1.071628, acc: 95%] [G loss: 1.101522]\n",
      "[Epoch 2/200] [Batch 493/938] [D loss: 1.050362, acc: 92%] [G loss: 1.155545]\n",
      "[Epoch 2/200] [Batch 494/938] [D loss: 1.099976, acc: 92%] [G loss: 1.233679]\n",
      "[Epoch 2/200] [Batch 495/938] [D loss: 1.081510, acc: 92%] [G loss: 1.205266]\n",
      "[Epoch 2/200] [Batch 496/938] [D loss: 1.069645, acc: 93%] [G loss: 1.135047]\n",
      "[Epoch 2/200] [Batch 497/938] [D loss: 1.083728, acc: 95%] [G loss: 1.105161]\n",
      "[Epoch 2/200] [Batch 498/938] [D loss: 1.078458, acc: 89%] [G loss: 1.142743]\n",
      "[Epoch 2/200] [Batch 499/938] [D loss: 1.087457, acc: 90%] [G loss: 1.207515]\n",
      "[Epoch 2/200] [Batch 500/938] [D loss: 1.089439, acc: 92%] [G loss: 1.184617]\n",
      "[Epoch 2/200] [Batch 501/938] [D loss: 1.071725, acc: 90%] [G loss: 1.157825]\n",
      "[Epoch 2/200] [Batch 502/938] [D loss: 1.059302, acc: 92%] [G loss: 1.205806]\n",
      "[Epoch 2/200] [Batch 503/938] [D loss: 1.083801, acc: 95%] [G loss: 1.215581]\n",
      "[Epoch 2/200] [Batch 504/938] [D loss: 1.093509, acc: 92%] [G loss: 1.205992]\n",
      "[Epoch 2/200] [Batch 505/938] [D loss: 1.086020, acc: 92%] [G loss: 1.099151]\n",
      "[Epoch 2/200] [Batch 506/938] [D loss: 1.118547, acc: 96%] [G loss: 1.120590]\n",
      "[Epoch 2/200] [Batch 507/938] [D loss: 1.100126, acc: 89%] [G loss: 1.221091]\n",
      "[Epoch 2/200] [Batch 508/938] [D loss: 1.061824, acc: 95%] [G loss: 1.191218]\n",
      "[Epoch 2/200] [Batch 509/938] [D loss: 1.064612, acc: 92%] [G loss: 1.158715]\n",
      "[Epoch 2/200] [Batch 510/938] [D loss: 1.076830, acc: 91%] [G loss: 1.172489]\n",
      "[Epoch 2/200] [Batch 511/938] [D loss: 1.070081, acc: 91%] [G loss: 1.151182]\n",
      "[Epoch 2/200] [Batch 512/938] [D loss: 1.074524, acc: 92%] [G loss: 1.165165]\n",
      "[Epoch 2/200] [Batch 513/938] [D loss: 1.103993, acc: 95%] [G loss: 1.139478]\n",
      "[Epoch 2/200] [Batch 514/938] [D loss: 1.075209, acc: 96%] [G loss: 1.177142]\n",
      "[Epoch 2/200] [Batch 515/938] [D loss: 1.090908, acc: 94%] [G loss: 1.106952]\n",
      "[Epoch 2/200] [Batch 516/938] [D loss: 1.101426, acc: 92%] [G loss: 1.226242]\n",
      "[Epoch 2/200] [Batch 517/938] [D loss: 1.066860, acc: 95%] [G loss: 1.165974]\n",
      "[Epoch 2/200] [Batch 518/938] [D loss: 1.076037, acc: 93%] [G loss: 1.183660]\n",
      "[Epoch 2/200] [Batch 519/938] [D loss: 1.091448, acc: 90%] [G loss: 1.137958]\n",
      "[Epoch 2/200] [Batch 520/938] [D loss: 1.091284, acc: 94%] [G loss: 1.121481]\n",
      "[Epoch 2/200] [Batch 521/938] [D loss: 1.058936, acc: 93%] [G loss: 1.098664]\n",
      "[Epoch 2/200] [Batch 522/938] [D loss: 1.072176, acc: 96%] [G loss: 1.163606]\n",
      "[Epoch 2/200] [Batch 523/938] [D loss: 1.080865, acc: 96%] [G loss: 1.202713]\n",
      "[Epoch 2/200] [Batch 524/938] [D loss: 1.066719, acc: 94%] [G loss: 1.193778]\n",
      "[Epoch 2/200] [Batch 525/938] [D loss: 1.106246, acc: 96%] [G loss: 1.135707]\n",
      "[Epoch 2/200] [Batch 526/938] [D loss: 1.117762, acc: 94%] [G loss: 1.168059]\n",
      "[Epoch 2/200] [Batch 527/938] [D loss: 1.103066, acc: 89%] [G loss: 1.167436]\n",
      "[Epoch 2/200] [Batch 528/938] [D loss: 1.074332, acc: 93%] [G loss: 1.170594]\n",
      "[Epoch 2/200] [Batch 529/938] [D loss: 1.099925, acc: 90%] [G loss: 1.149172]\n",
      "[Epoch 2/200] [Batch 530/938] [D loss: 1.028640, acc: 96%] [G loss: 1.171243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 531/938] [D loss: 1.072396, acc: 91%] [G loss: 1.305322]\n",
      "[Epoch 2/200] [Batch 532/938] [D loss: 1.091666, acc: 91%] [G loss: 1.238378]\n",
      "[Epoch 2/200] [Batch 533/938] [D loss: 1.062062, acc: 94%] [G loss: 1.108492]\n",
      "[Epoch 2/200] [Batch 534/938] [D loss: 1.099368, acc: 88%] [G loss: 1.156840]\n",
      "[Epoch 2/200] [Batch 535/938] [D loss: 1.046430, acc: 95%] [G loss: 1.155486]\n",
      "[Epoch 2/200] [Batch 536/938] [D loss: 1.088189, acc: 95%] [G loss: 1.126406]\n",
      "[Epoch 2/200] [Batch 537/938] [D loss: 1.060983, acc: 92%] [G loss: 1.230185]\n",
      "[Epoch 2/200] [Batch 538/938] [D loss: 1.120088, acc: 89%] [G loss: 1.200217]\n",
      "[Epoch 2/200] [Batch 539/938] [D loss: 1.105849, acc: 93%] [G loss: 1.084908]\n",
      "[Epoch 2/200] [Batch 540/938] [D loss: 1.065550, acc: 92%] [G loss: 1.117292]\n",
      "[Epoch 2/200] [Batch 541/938] [D loss: 1.042525, acc: 92%] [G loss: 1.202010]\n",
      "[Epoch 2/200] [Batch 542/938] [D loss: 1.096759, acc: 91%] [G loss: 1.229614]\n",
      "[Epoch 2/200] [Batch 543/938] [D loss: 1.060249, acc: 97%] [G loss: 1.133955]\n",
      "[Epoch 2/200] [Batch 544/938] [D loss: 1.098083, acc: 92%] [G loss: 1.218635]\n",
      "[Epoch 2/200] [Batch 545/938] [D loss: 1.089413, acc: 93%] [G loss: 1.160594]\n",
      "[Epoch 2/200] [Batch 546/938] [D loss: 1.125902, acc: 92%] [G loss: 1.203723]\n",
      "[Epoch 2/200] [Batch 547/938] [D loss: 1.076341, acc: 93%] [G loss: 1.134629]\n",
      "[Epoch 2/200] [Batch 548/938] [D loss: 1.094024, acc: 96%] [G loss: 1.149049]\n",
      "[Epoch 2/200] [Batch 549/938] [D loss: 1.066129, acc: 93%] [G loss: 1.258713]\n",
      "[Epoch 2/200] [Batch 550/938] [D loss: 1.070994, acc: 96%] [G loss: 1.198403]\n",
      "[Epoch 2/200] [Batch 551/938] [D loss: 1.059981, acc: 96%] [G loss: 1.104247]\n",
      "[Epoch 2/200] [Batch 552/938] [D loss: 1.122826, acc: 92%] [G loss: 1.173185]\n",
      "[Epoch 2/200] [Batch 553/938] [D loss: 1.060619, acc: 94%] [G loss: 1.155719]\n",
      "[Epoch 2/200] [Batch 554/938] [D loss: 1.102588, acc: 93%] [G loss: 1.152076]\n",
      "[Epoch 2/200] [Batch 555/938] [D loss: 1.087635, acc: 95%] [G loss: 1.157504]\n",
      "[Epoch 2/200] [Batch 556/938] [D loss: 1.104162, acc: 91%] [G loss: 1.137732]\n",
      "[Epoch 2/200] [Batch 557/938] [D loss: 1.130957, acc: 89%] [G loss: 1.167102]\n",
      "[Epoch 2/200] [Batch 558/938] [D loss: 1.112101, acc: 92%] [G loss: 1.242902]\n",
      "[Epoch 2/200] [Batch 559/938] [D loss: 1.116579, acc: 95%] [G loss: 1.202792]\n",
      "[Epoch 2/200] [Batch 560/938] [D loss: 1.096132, acc: 92%] [G loss: 1.173287]\n",
      "[Epoch 2/200] [Batch 561/938] [D loss: 1.081861, acc: 93%] [G loss: 1.152976]\n",
      "[Epoch 2/200] [Batch 562/938] [D loss: 1.086474, acc: 89%] [G loss: 1.153954]\n",
      "[Epoch 2/200] [Batch 563/938] [D loss: 1.084533, acc: 96%] [G loss: 1.120023]\n",
      "[Epoch 2/200] [Batch 564/938] [D loss: 1.073870, acc: 95%] [G loss: 1.190985]\n",
      "[Epoch 2/200] [Batch 565/938] [D loss: 1.085700, acc: 94%] [G loss: 1.143024]\n",
      "[Epoch 2/200] [Batch 566/938] [D loss: 1.069057, acc: 90%] [G loss: 1.270037]\n",
      "[Epoch 2/200] [Batch 567/938] [D loss: 1.029607, acc: 96%] [G loss: 1.156322]\n",
      "[Epoch 2/200] [Batch 568/938] [D loss: 1.086375, acc: 94%] [G loss: 1.160111]\n",
      "[Epoch 2/200] [Batch 569/938] [D loss: 1.055376, acc: 92%] [G loss: 1.292802]\n",
      "[Epoch 2/200] [Batch 570/938] [D loss: 1.113814, acc: 92%] [G loss: 1.166069]\n",
      "[Epoch 2/200] [Batch 571/938] [D loss: 1.105877, acc: 92%] [G loss: 1.188865]\n",
      "[Epoch 2/200] [Batch 572/938] [D loss: 1.092484, acc: 95%] [G loss: 1.231202]\n",
      "[Epoch 2/200] [Batch 573/938] [D loss: 1.107913, acc: 93%] [G loss: 1.154518]\n",
      "[Epoch 2/200] [Batch 574/938] [D loss: 1.118362, acc: 91%] [G loss: 1.188973]\n",
      "[Epoch 2/200] [Batch 575/938] [D loss: 1.076567, acc: 96%] [G loss: 1.172801]\n",
      "[Epoch 2/200] [Batch 576/938] [D loss: 1.084346, acc: 96%] [G loss: 1.112822]\n",
      "[Epoch 2/200] [Batch 577/938] [D loss: 1.087662, acc: 94%] [G loss: 1.200677]\n",
      "[Epoch 2/200] [Batch 578/938] [D loss: 1.116278, acc: 92%] [G loss: 1.248997]\n",
      "[Epoch 2/200] [Batch 579/938] [D loss: 1.066915, acc: 95%] [G loss: 1.199897]\n",
      "[Epoch 2/200] [Batch 580/938] [D loss: 1.062263, acc: 96%] [G loss: 1.185199]\n",
      "[Epoch 2/200] [Batch 581/938] [D loss: 1.091833, acc: 92%] [G loss: 1.137485]\n",
      "[Epoch 2/200] [Batch 582/938] [D loss: 1.077956, acc: 96%] [G loss: 1.161870]\n",
      "[Epoch 2/200] [Batch 583/938] [D loss: 1.112907, acc: 91%] [G loss: 1.164577]\n",
      "[Epoch 2/200] [Batch 584/938] [D loss: 1.096360, acc: 94%] [G loss: 1.142862]\n",
      "[Epoch 2/200] [Batch 585/938] [D loss: 1.063312, acc: 94%] [G loss: 1.085565]\n",
      "[Epoch 2/200] [Batch 586/938] [D loss: 1.090630, acc: 91%] [G loss: 1.253989]\n",
      "[Epoch 2/200] [Batch 587/938] [D loss: 1.107971, acc: 92%] [G loss: 1.181799]\n",
      "[Epoch 2/200] [Batch 588/938] [D loss: 1.109565, acc: 93%] [G loss: 1.078951]\n",
      "[Epoch 2/200] [Batch 589/938] [D loss: 1.089379, acc: 92%] [G loss: 1.171771]\n",
      "[Epoch 2/200] [Batch 590/938] [D loss: 1.097297, acc: 93%] [G loss: 1.114969]\n",
      "[Epoch 2/200] [Batch 591/938] [D loss: 1.072599, acc: 92%] [G loss: 1.146655]\n",
      "[Epoch 2/200] [Batch 592/938] [D loss: 1.115122, acc: 89%] [G loss: 1.205498]\n",
      "[Epoch 2/200] [Batch 593/938] [D loss: 1.074806, acc: 94%] [G loss: 1.168438]\n",
      "[Epoch 2/200] [Batch 594/938] [D loss: 1.079182, acc: 89%] [G loss: 1.236229]\n",
      "[Epoch 2/200] [Batch 595/938] [D loss: 1.059146, acc: 96%] [G loss: 1.218539]\n",
      "[Epoch 2/200] [Batch 596/938] [D loss: 1.060499, acc: 94%] [G loss: 1.127774]\n",
      "[Epoch 2/200] [Batch 597/938] [D loss: 1.051448, acc: 93%] [G loss: 1.233160]\n",
      "[Epoch 2/200] [Batch 598/938] [D loss: 1.080130, acc: 96%] [G loss: 1.220176]\n",
      "[Epoch 2/200] [Batch 599/938] [D loss: 1.108316, acc: 91%] [G loss: 1.136379]\n",
      "[Epoch 2/200] [Batch 600/938] [D loss: 1.069298, acc: 91%] [G loss: 1.187817]\n",
      "[Epoch 2/200] [Batch 601/938] [D loss: 1.096746, acc: 92%] [G loss: 1.165343]\n",
      "[Epoch 2/200] [Batch 602/938] [D loss: 1.081883, acc: 93%] [G loss: 1.203217]\n",
      "[Epoch 2/200] [Batch 603/938] [D loss: 1.070861, acc: 92%] [G loss: 1.174432]\n",
      "[Epoch 2/200] [Batch 604/938] [D loss: 1.072775, acc: 96%] [G loss: 1.126350]\n",
      "[Epoch 2/200] [Batch 605/938] [D loss: 1.076784, acc: 92%] [G loss: 1.036678]\n",
      "[Epoch 2/200] [Batch 606/938] [D loss: 1.080827, acc: 96%] [G loss: 1.204447]\n",
      "[Epoch 2/200] [Batch 607/938] [D loss: 1.059656, acc: 95%] [G loss: 1.264018]\n",
      "[Epoch 2/200] [Batch 608/938] [D loss: 1.052885, acc: 95%] [G loss: 1.199353]\n",
      "[Epoch 2/200] [Batch 609/938] [D loss: 1.089098, acc: 92%] [G loss: 1.203648]\n",
      "[Epoch 2/200] [Batch 610/938] [D loss: 1.102413, acc: 92%] [G loss: 1.204767]\n",
      "[Epoch 2/200] [Batch 611/938] [D loss: 1.064993, acc: 97%] [G loss: 1.174713]\n",
      "[Epoch 2/200] [Batch 612/938] [D loss: 1.059321, acc: 93%] [G loss: 1.204272]\n",
      "[Epoch 2/200] [Batch 613/938] [D loss: 1.037846, acc: 96%] [G loss: 1.221519]\n",
      "[Epoch 2/200] [Batch 614/938] [D loss: 1.093569, acc: 95%] [G loss: 1.177911]\n",
      "[Epoch 2/200] [Batch 615/938] [D loss: 1.060588, acc: 93%] [G loss: 1.239516]\n",
      "[Epoch 2/200] [Batch 616/938] [D loss: 1.085730, acc: 94%] [G loss: 1.177162]\n",
      "[Epoch 2/200] [Batch 617/938] [D loss: 1.116444, acc: 93%] [G loss: 1.096187]\n",
      "[Epoch 2/200] [Batch 618/938] [D loss: 1.039803, acc: 93%] [G loss: 1.218463]\n",
      "[Epoch 2/200] [Batch 619/938] [D loss: 1.100703, acc: 93%] [G loss: 1.242300]\n",
      "[Epoch 2/200] [Batch 620/938] [D loss: 1.083903, acc: 96%] [G loss: 1.160786]\n",
      "[Epoch 2/200] [Batch 621/938] [D loss: 1.098332, acc: 96%] [G loss: 1.143390]\n",
      "[Epoch 2/200] [Batch 622/938] [D loss: 1.060338, acc: 95%] [G loss: 1.157404]\n",
      "[Epoch 2/200] [Batch 623/938] [D loss: 1.101816, acc: 92%] [G loss: 1.139284]\n",
      "[Epoch 2/200] [Batch 624/938] [D loss: 1.073887, acc: 95%] [G loss: 1.084499]\n",
      "[Epoch 2/200] [Batch 625/938] [D loss: 1.106652, acc: 92%] [G loss: 1.161672]\n",
      "[Epoch 2/200] [Batch 626/938] [D loss: 1.049289, acc: 93%] [G loss: 1.206643]\n",
      "[Epoch 2/200] [Batch 627/938] [D loss: 1.074517, acc: 91%] [G loss: 1.196630]\n",
      "[Epoch 2/200] [Batch 628/938] [D loss: 1.083224, acc: 96%] [G loss: 1.253474]\n",
      "[Epoch 2/200] [Batch 629/938] [D loss: 1.076808, acc: 89%] [G loss: 1.168669]\n",
      "[Epoch 2/200] [Batch 630/938] [D loss: 1.045146, acc: 96%] [G loss: 1.176081]\n",
      "[Epoch 2/200] [Batch 631/938] [D loss: 1.044521, acc: 94%] [G loss: 1.168398]\n",
      "[Epoch 2/200] [Batch 632/938] [D loss: 1.095052, acc: 92%] [G loss: 1.204434]\n",
      "[Epoch 2/200] [Batch 633/938] [D loss: 1.105652, acc: 94%] [G loss: 1.129064]\n",
      "[Epoch 2/200] [Batch 634/938] [D loss: 1.086144, acc: 91%] [G loss: 1.183418]\n",
      "[Epoch 2/200] [Batch 635/938] [D loss: 1.079954, acc: 94%] [G loss: 1.156650]\n",
      "[Epoch 2/200] [Batch 636/938] [D loss: 1.086699, acc: 94%] [G loss: 1.225663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 637/938] [D loss: 1.079523, acc: 92%] [G loss: 1.205865]\n",
      "[Epoch 2/200] [Batch 638/938] [D loss: 1.107179, acc: 92%] [G loss: 1.185528]\n",
      "[Epoch 2/200] [Batch 639/938] [D loss: 1.096353, acc: 90%] [G loss: 1.208560]\n",
      "[Epoch 2/200] [Batch 640/938] [D loss: 1.074672, acc: 95%] [G loss: 1.081604]\n",
      "[Epoch 2/200] [Batch 641/938] [D loss: 1.094612, acc: 96%] [G loss: 1.154961]\n",
      "[Epoch 2/200] [Batch 642/938] [D loss: 1.062702, acc: 93%] [G loss: 1.210455]\n",
      "[Epoch 2/200] [Batch 643/938] [D loss: 1.074055, acc: 95%] [G loss: 1.218271]\n",
      "[Epoch 2/200] [Batch 644/938] [D loss: 1.091466, acc: 94%] [G loss: 1.226531]\n",
      "[Epoch 2/200] [Batch 645/938] [D loss: 1.111985, acc: 94%] [G loss: 1.164770]\n",
      "[Epoch 2/200] [Batch 646/938] [D loss: 1.100264, acc: 96%] [G loss: 1.129488]\n",
      "[Epoch 2/200] [Batch 647/938] [D loss: 1.096126, acc: 91%] [G loss: 1.114119]\n",
      "[Epoch 2/200] [Batch 648/938] [D loss: 1.059206, acc: 96%] [G loss: 1.217637]\n",
      "[Epoch 2/200] [Batch 649/938] [D loss: 1.076370, acc: 96%] [G loss: 1.260717]\n",
      "[Epoch 2/200] [Batch 650/938] [D loss: 1.115829, acc: 93%] [G loss: 1.182173]\n",
      "[Epoch 2/200] [Batch 651/938] [D loss: 1.094531, acc: 95%] [G loss: 1.191964]\n",
      "[Epoch 2/200] [Batch 652/938] [D loss: 1.084762, acc: 92%] [G loss: 1.180105]\n",
      "[Epoch 2/200] [Batch 653/938] [D loss: 1.070982, acc: 94%] [G loss: 1.163011]\n",
      "[Epoch 2/200] [Batch 654/938] [D loss: 1.104367, acc: 93%] [G loss: 1.095186]\n",
      "[Epoch 2/200] [Batch 655/938] [D loss: 1.100260, acc: 94%] [G loss: 1.132643]\n",
      "[Epoch 2/200] [Batch 656/938] [D loss: 1.079843, acc: 95%] [G loss: 1.163626]\n",
      "[Epoch 2/200] [Batch 657/938] [D loss: 1.085909, acc: 94%] [G loss: 1.183464]\n",
      "[Epoch 2/200] [Batch 658/938] [D loss: 1.059852, acc: 96%] [G loss: 1.134581]\n",
      "[Epoch 2/200] [Batch 659/938] [D loss: 1.113713, acc: 92%] [G loss: 1.158821]\n",
      "[Epoch 2/200] [Batch 660/938] [D loss: 1.088755, acc: 92%] [G loss: 1.147943]\n",
      "[Epoch 2/200] [Batch 661/938] [D loss: 1.102979, acc: 93%] [G loss: 1.208043]\n",
      "[Epoch 2/200] [Batch 662/938] [D loss: 1.093576, acc: 95%] [G loss: 1.114728]\n",
      "[Epoch 2/200] [Batch 663/938] [D loss: 1.040541, acc: 96%] [G loss: 1.148185]\n",
      "[Epoch 2/200] [Batch 664/938] [D loss: 1.104575, acc: 95%] [G loss: 1.147403]\n",
      "[Epoch 2/200] [Batch 665/938] [D loss: 1.057639, acc: 94%] [G loss: 1.189254]\n",
      "[Epoch 2/200] [Batch 666/938] [D loss: 1.092833, acc: 90%] [G loss: 1.166667]\n",
      "[Epoch 2/200] [Batch 667/938] [D loss: 1.066481, acc: 98%] [G loss: 1.186501]\n",
      "[Epoch 2/200] [Batch 668/938] [D loss: 1.100231, acc: 94%] [G loss: 1.178491]\n",
      "[Epoch 2/200] [Batch 669/938] [D loss: 1.133092, acc: 89%] [G loss: 1.135881]\n",
      "[Epoch 2/200] [Batch 670/938] [D loss: 1.105016, acc: 90%] [G loss: 1.168821]\n",
      "[Epoch 2/200] [Batch 671/938] [D loss: 1.125921, acc: 92%] [G loss: 1.228871]\n",
      "[Epoch 2/200] [Batch 672/938] [D loss: 1.062011, acc: 92%] [G loss: 1.264416]\n",
      "[Epoch 2/200] [Batch 673/938] [D loss: 1.042535, acc: 96%] [G loss: 1.105978]\n",
      "[Epoch 2/200] [Batch 674/938] [D loss: 1.102309, acc: 91%] [G loss: 1.123873]\n",
      "[Epoch 2/200] [Batch 675/938] [D loss: 1.146699, acc: 91%] [G loss: 1.092387]\n",
      "[Epoch 2/200] [Batch 676/938] [D loss: 1.079879, acc: 97%] [G loss: 1.119316]\n",
      "[Epoch 2/200] [Batch 677/938] [D loss: 1.095144, acc: 93%] [G loss: 1.233820]\n",
      "[Epoch 2/200] [Batch 678/938] [D loss: 1.053092, acc: 96%] [G loss: 1.205540]\n",
      "[Epoch 2/200] [Batch 679/938] [D loss: 1.083475, acc: 96%] [G loss: 1.198810]\n",
      "[Epoch 2/200] [Batch 680/938] [D loss: 1.029346, acc: 91%] [G loss: 1.180977]\n",
      "[Epoch 2/200] [Batch 681/938] [D loss: 1.129878, acc: 89%] [G loss: 1.292554]\n",
      "[Epoch 2/200] [Batch 682/938] [D loss: 1.083361, acc: 96%] [G loss: 1.159091]\n",
      "[Epoch 2/200] [Batch 683/938] [D loss: 1.122389, acc: 89%] [G loss: 1.157369]\n",
      "[Epoch 2/200] [Batch 684/938] [D loss: 1.081645, acc: 97%] [G loss: 1.146689]\n",
      "[Epoch 2/200] [Batch 685/938] [D loss: 1.126515, acc: 92%] [G loss: 1.134305]\n",
      "[Epoch 2/200] [Batch 686/938] [D loss: 1.076256, acc: 92%] [G loss: 1.277537]\n",
      "[Epoch 2/200] [Batch 687/938] [D loss: 1.060244, acc: 96%] [G loss: 1.194275]\n",
      "[Epoch 2/200] [Batch 688/938] [D loss: 1.026592, acc: 97%] [G loss: 1.205588]\n",
      "[Epoch 2/200] [Batch 689/938] [D loss: 1.110507, acc: 91%] [G loss: 1.212431]\n",
      "[Epoch 2/200] [Batch 690/938] [D loss: 1.081475, acc: 93%] [G loss: 1.146055]\n",
      "[Epoch 2/200] [Batch 691/938] [D loss: 1.063700, acc: 96%] [G loss: 1.217577]\n",
      "[Epoch 2/200] [Batch 692/938] [D loss: 1.108444, acc: 91%] [G loss: 1.154976]\n",
      "[Epoch 2/200] [Batch 693/938] [D loss: 1.050237, acc: 95%] [G loss: 1.165208]\n",
      "[Epoch 2/200] [Batch 694/938] [D loss: 1.099985, acc: 92%] [G loss: 1.171465]\n",
      "[Epoch 2/200] [Batch 695/938] [D loss: 1.077549, acc: 96%] [G loss: 1.080851]\n",
      "[Epoch 2/200] [Batch 696/938] [D loss: 1.107439, acc: 94%] [G loss: 1.178437]\n",
      "[Epoch 2/200] [Batch 697/938] [D loss: 1.057456, acc: 89%] [G loss: 1.249623]\n",
      "[Epoch 2/200] [Batch 698/938] [D loss: 1.082119, acc: 97%] [G loss: 1.181466]\n",
      "[Epoch 2/200] [Batch 699/938] [D loss: 1.081069, acc: 97%] [G loss: 1.162156]\n",
      "[Epoch 2/200] [Batch 700/938] [D loss: 1.084528, acc: 93%] [G loss: 1.193633]\n",
      "[Epoch 2/200] [Batch 701/938] [D loss: 1.087710, acc: 94%] [G loss: 1.159799]\n",
      "[Epoch 2/200] [Batch 702/938] [D loss: 1.092961, acc: 94%] [G loss: 1.168152]\n",
      "[Epoch 2/200] [Batch 703/938] [D loss: 1.069273, acc: 91%] [G loss: 1.158609]\n",
      "[Epoch 2/200] [Batch 704/938] [D loss: 1.106231, acc: 94%] [G loss: 1.171610]\n",
      "[Epoch 2/200] [Batch 705/938] [D loss: 1.114090, acc: 90%] [G loss: 1.105543]\n",
      "[Epoch 2/200] [Batch 706/938] [D loss: 1.049769, acc: 96%] [G loss: 1.095183]\n",
      "[Epoch 2/200] [Batch 707/938] [D loss: 1.066495, acc: 92%] [G loss: 1.100701]\n",
      "[Epoch 2/200] [Batch 708/938] [D loss: 1.085904, acc: 92%] [G loss: 1.146937]\n",
      "[Epoch 2/200] [Batch 709/938] [D loss: 1.069106, acc: 98%] [G loss: 1.143170]\n",
      "[Epoch 2/200] [Batch 710/938] [D loss: 1.108402, acc: 95%] [G loss: 1.199034]\n",
      "[Epoch 2/200] [Batch 711/938] [D loss: 1.091964, acc: 92%] [G loss: 1.185290]\n",
      "[Epoch 2/200] [Batch 712/938] [D loss: 1.092319, acc: 92%] [G loss: 1.193303]\n",
      "[Epoch 2/200] [Batch 713/938] [D loss: 1.097278, acc: 94%] [G loss: 1.168350]\n",
      "[Epoch 2/200] [Batch 714/938] [D loss: 1.048919, acc: 94%] [G loss: 1.191379]\n",
      "[Epoch 2/200] [Batch 715/938] [D loss: 1.052074, acc: 93%] [G loss: 1.208209]\n",
      "[Epoch 2/200] [Batch 716/938] [D loss: 1.104120, acc: 88%] [G loss: 1.109217]\n",
      "[Epoch 2/200] [Batch 717/938] [D loss: 1.076333, acc: 96%] [G loss: 1.108412]\n",
      "[Epoch 2/200] [Batch 718/938] [D loss: 1.105969, acc: 88%] [G loss: 1.168004]\n",
      "[Epoch 2/200] [Batch 719/938] [D loss: 1.070436, acc: 94%] [G loss: 1.208078]\n",
      "[Epoch 2/200] [Batch 720/938] [D loss: 1.086946, acc: 94%] [G loss: 1.190687]\n",
      "[Epoch 2/200] [Batch 721/938] [D loss: 1.108443, acc: 92%] [G loss: 1.145244]\n",
      "[Epoch 2/200] [Batch 722/938] [D loss: 1.046427, acc: 96%] [G loss: 1.212121]\n",
      "[Epoch 2/200] [Batch 723/938] [D loss: 1.078491, acc: 92%] [G loss: 1.211411]\n",
      "[Epoch 2/200] [Batch 724/938] [D loss: 1.064985, acc: 96%] [G loss: 1.195154]\n",
      "[Epoch 2/200] [Batch 725/938] [D loss: 1.068832, acc: 96%] [G loss: 1.181803]\n",
      "[Epoch 2/200] [Batch 726/938] [D loss: 1.058502, acc: 99%] [G loss: 1.179490]\n",
      "[Epoch 2/200] [Batch 727/938] [D loss: 1.140570, acc: 88%] [G loss: 1.089928]\n",
      "[Epoch 2/200] [Batch 728/938] [D loss: 1.127221, acc: 97%] [G loss: 1.112150]\n",
      "[Epoch 2/200] [Batch 729/938] [D loss: 1.067782, acc: 92%] [G loss: 1.239963]\n",
      "[Epoch 2/200] [Batch 730/938] [D loss: 1.106673, acc: 90%] [G loss: 1.146633]\n",
      "[Epoch 2/200] [Batch 731/938] [D loss: 1.096499, acc: 91%] [G loss: 1.140781]\n",
      "[Epoch 2/200] [Batch 732/938] [D loss: 1.106975, acc: 97%] [G loss: 1.236819]\n",
      "[Epoch 2/200] [Batch 733/938] [D loss: 1.074484, acc: 95%] [G loss: 1.151670]\n",
      "[Epoch 2/200] [Batch 734/938] [D loss: 1.059264, acc: 96%] [G loss: 1.212509]\n",
      "[Epoch 2/200] [Batch 735/938] [D loss: 1.055885, acc: 96%] [G loss: 1.115642]\n",
      "[Epoch 2/200] [Batch 736/938] [D loss: 1.074203, acc: 89%] [G loss: 1.142809]\n",
      "[Epoch 2/200] [Batch 737/938] [D loss: 1.088298, acc: 93%] [G loss: 1.197016]\n",
      "[Epoch 2/200] [Batch 738/938] [D loss: 1.077404, acc: 96%] [G loss: 1.153043]\n",
      "[Epoch 2/200] [Batch 739/938] [D loss: 1.086517, acc: 96%] [G loss: 1.113141]\n",
      "[Epoch 2/200] [Batch 740/938] [D loss: 1.075477, acc: 96%] [G loss: 1.169384]\n",
      "[Epoch 2/200] [Batch 741/938] [D loss: 1.072644, acc: 93%] [G loss: 1.168539]\n",
      "[Epoch 2/200] [Batch 742/938] [D loss: 1.097921, acc: 89%] [G loss: 1.128619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 743/938] [D loss: 1.070741, acc: 96%] [G loss: 1.105129]\n",
      "[Epoch 2/200] [Batch 744/938] [D loss: 1.074708, acc: 94%] [G loss: 1.156389]\n",
      "[Epoch 2/200] [Batch 745/938] [D loss: 1.084383, acc: 95%] [G loss: 1.154712]\n",
      "[Epoch 2/200] [Batch 746/938] [D loss: 1.093162, acc: 93%] [G loss: 1.141195]\n",
      "[Epoch 2/200] [Batch 747/938] [D loss: 1.081249, acc: 92%] [G loss: 1.172596]\n",
      "[Epoch 2/200] [Batch 748/938] [D loss: 1.086212, acc: 93%] [G loss: 1.194900]\n",
      "[Epoch 2/200] [Batch 749/938] [D loss: 1.057721, acc: 96%] [G loss: 1.189040]\n",
      "[Epoch 2/200] [Batch 750/938] [D loss: 1.063310, acc: 96%] [G loss: 1.126994]\n",
      "[Epoch 2/200] [Batch 751/938] [D loss: 1.137797, acc: 90%] [G loss: 1.109057]\n",
      "[Epoch 2/200] [Batch 752/938] [D loss: 1.090594, acc: 92%] [G loss: 1.156974]\n",
      "[Epoch 2/200] [Batch 753/938] [D loss: 1.078449, acc: 90%] [G loss: 1.142026]\n",
      "[Epoch 2/200] [Batch 754/938] [D loss: 1.075206, acc: 93%] [G loss: 1.143493]\n",
      "[Epoch 2/200] [Batch 755/938] [D loss: 1.078937, acc: 95%] [G loss: 1.144529]\n",
      "[Epoch 2/200] [Batch 756/938] [D loss: 1.074221, acc: 95%] [G loss: 1.143030]\n",
      "[Epoch 2/200] [Batch 757/938] [D loss: 1.097267, acc: 93%] [G loss: 1.138964]\n",
      "[Epoch 2/200] [Batch 758/938] [D loss: 1.096117, acc: 92%] [G loss: 1.132389]\n",
      "[Epoch 2/200] [Batch 759/938] [D loss: 1.065478, acc: 95%] [G loss: 1.153265]\n",
      "[Epoch 2/200] [Batch 760/938] [D loss: 1.032405, acc: 96%] [G loss: 1.192162]\n",
      "[Epoch 2/200] [Batch 761/938] [D loss: 1.075519, acc: 94%] [G loss: 1.254851]\n",
      "[Epoch 2/200] [Batch 762/938] [D loss: 1.048422, acc: 94%] [G loss: 1.092625]\n",
      "[Epoch 2/200] [Batch 763/938] [D loss: 1.073730, acc: 95%] [G loss: 1.178442]\n",
      "[Epoch 2/200] [Batch 764/938] [D loss: 1.072591, acc: 96%] [G loss: 1.110820]\n",
      "[Epoch 2/200] [Batch 765/938] [D loss: 1.088022, acc: 93%] [G loss: 1.159559]\n",
      "[Epoch 2/200] [Batch 766/938] [D loss: 1.108274, acc: 95%] [G loss: 1.211211]\n",
      "[Epoch 2/200] [Batch 767/938] [D loss: 1.060611, acc: 93%] [G loss: 1.235664]\n",
      "[Epoch 2/200] [Batch 768/938] [D loss: 1.082913, acc: 94%] [G loss: 1.234526]\n",
      "[Epoch 2/200] [Batch 769/938] [D loss: 1.074729, acc: 92%] [G loss: 1.090039]\n",
      "[Epoch 2/200] [Batch 770/938] [D loss: 1.076409, acc: 97%] [G loss: 1.169296]\n",
      "[Epoch 2/200] [Batch 771/938] [D loss: 1.062952, acc: 96%] [G loss: 1.199489]\n",
      "[Epoch 2/200] [Batch 772/938] [D loss: 1.064331, acc: 93%] [G loss: 1.130000]\n",
      "[Epoch 2/200] [Batch 773/938] [D loss: 1.094215, acc: 93%] [G loss: 1.171593]\n",
      "[Epoch 2/200] [Batch 774/938] [D loss: 1.042090, acc: 96%] [G loss: 1.130049]\n",
      "[Epoch 2/200] [Batch 775/938] [D loss: 1.082644, acc: 93%] [G loss: 1.180957]\n",
      "[Epoch 2/200] [Batch 776/938] [D loss: 1.090017, acc: 92%] [G loss: 1.155147]\n",
      "[Epoch 2/200] [Batch 777/938] [D loss: 1.033632, acc: 94%] [G loss: 1.133617]\n",
      "[Epoch 2/200] [Batch 778/938] [D loss: 1.096111, acc: 92%] [G loss: 1.183363]\n",
      "[Epoch 2/200] [Batch 779/938] [D loss: 1.062266, acc: 91%] [G loss: 1.278932]\n",
      "[Epoch 2/200] [Batch 780/938] [D loss: 1.083729, acc: 92%] [G loss: 1.228232]\n",
      "[Epoch 2/200] [Batch 781/938] [D loss: 1.077719, acc: 93%] [G loss: 1.192059]\n",
      "[Epoch 2/200] [Batch 782/938] [D loss: 1.112695, acc: 90%] [G loss: 1.180116]\n",
      "[Epoch 2/200] [Batch 783/938] [D loss: 1.115986, acc: 90%] [G loss: 1.147613]\n",
      "[Epoch 2/200] [Batch 784/938] [D loss: 1.042574, acc: 95%] [G loss: 1.173736]\n",
      "[Epoch 2/200] [Batch 785/938] [D loss: 1.119547, acc: 91%] [G loss: 1.119005]\n",
      "[Epoch 2/200] [Batch 786/938] [D loss: 1.106617, acc: 94%] [G loss: 1.248311]\n",
      "[Epoch 2/200] [Batch 787/938] [D loss: 1.091551, acc: 92%] [G loss: 1.163670]\n",
      "[Epoch 2/200] [Batch 788/938] [D loss: 1.083146, acc: 92%] [G loss: 1.166032]\n",
      "[Epoch 2/200] [Batch 789/938] [D loss: 1.072163, acc: 97%] [G loss: 1.101824]\n",
      "[Epoch 2/200] [Batch 790/938] [D loss: 1.051398, acc: 90%] [G loss: 1.176277]\n",
      "[Epoch 2/200] [Batch 791/938] [D loss: 1.087655, acc: 94%] [G loss: 1.205812]\n",
      "[Epoch 2/200] [Batch 792/938] [D loss: 1.081209, acc: 92%] [G loss: 1.151626]\n",
      "[Epoch 2/200] [Batch 793/938] [D loss: 1.132988, acc: 97%] [G loss: 1.149051]\n",
      "[Epoch 2/200] [Batch 794/938] [D loss: 1.076089, acc: 92%] [G loss: 1.189977]\n",
      "[Epoch 2/200] [Batch 795/938] [D loss: 1.078244, acc: 92%] [G loss: 1.165617]\n",
      "[Epoch 2/200] [Batch 796/938] [D loss: 1.082783, acc: 96%] [G loss: 1.127247]\n",
      "[Epoch 2/200] [Batch 797/938] [D loss: 1.058931, acc: 96%] [G loss: 1.191655]\n",
      "[Epoch 2/200] [Batch 798/938] [D loss: 1.089126, acc: 97%] [G loss: 1.128599]\n",
      "[Epoch 2/200] [Batch 799/938] [D loss: 1.075238, acc: 95%] [G loss: 1.190027]\n",
      "[Epoch 2/200] [Batch 800/938] [D loss: 1.086337, acc: 93%] [G loss: 1.192858]\n",
      "[Epoch 2/200] [Batch 801/938] [D loss: 1.096200, acc: 91%] [G loss: 1.179102]\n",
      "[Epoch 2/200] [Batch 802/938] [D loss: 1.086337, acc: 94%] [G loss: 1.156332]\n",
      "[Epoch 2/200] [Batch 803/938] [D loss: 1.058789, acc: 92%] [G loss: 1.100106]\n",
      "[Epoch 2/200] [Batch 804/938] [D loss: 1.071406, acc: 93%] [G loss: 1.191074]\n",
      "[Epoch 2/200] [Batch 805/938] [D loss: 1.034400, acc: 95%] [G loss: 1.263061]\n",
      "[Epoch 2/200] [Batch 806/938] [D loss: 1.066646, acc: 92%] [G loss: 1.248820]\n",
      "[Epoch 2/200] [Batch 807/938] [D loss: 1.088659, acc: 91%] [G loss: 1.177288]\n",
      "[Epoch 2/200] [Batch 808/938] [D loss: 1.077877, acc: 93%] [G loss: 1.169879]\n",
      "[Epoch 2/200] [Batch 809/938] [D loss: 1.076959, acc: 95%] [G loss: 1.144492]\n",
      "[Epoch 2/200] [Batch 810/938] [D loss: 1.085612, acc: 91%] [G loss: 1.203723]\n",
      "[Epoch 2/200] [Batch 811/938] [D loss: 1.091253, acc: 96%] [G loss: 1.104929]\n",
      "[Epoch 2/200] [Batch 812/938] [D loss: 1.068278, acc: 92%] [G loss: 1.150497]\n",
      "[Epoch 2/200] [Batch 813/938] [D loss: 1.092755, acc: 92%] [G loss: 1.153606]\n",
      "[Epoch 2/200] [Batch 814/938] [D loss: 1.048787, acc: 97%] [G loss: 1.206297]\n",
      "[Epoch 2/200] [Batch 815/938] [D loss: 1.059192, acc: 95%] [G loss: 1.211274]\n",
      "[Epoch 2/200] [Batch 816/938] [D loss: 1.084335, acc: 96%] [G loss: 1.138246]\n",
      "[Epoch 2/200] [Batch 817/938] [D loss: 1.081434, acc: 95%] [G loss: 1.121132]\n",
      "[Epoch 2/200] [Batch 818/938] [D loss: 1.079318, acc: 93%] [G loss: 1.103168]\n",
      "[Epoch 2/200] [Batch 819/938] [D loss: 1.044383, acc: 95%] [G loss: 1.177158]\n",
      "[Epoch 2/200] [Batch 820/938] [D loss: 1.099270, acc: 92%] [G loss: 1.204791]\n",
      "[Epoch 2/200] [Batch 821/938] [D loss: 1.072003, acc: 92%] [G loss: 1.211652]\n",
      "[Epoch 2/200] [Batch 822/938] [D loss: 1.098906, acc: 89%] [G loss: 1.206460]\n",
      "[Epoch 2/200] [Batch 823/938] [D loss: 1.076929, acc: 93%] [G loss: 1.233869]\n",
      "[Epoch 2/200] [Batch 824/938] [D loss: 1.064296, acc: 93%] [G loss: 1.120551]\n",
      "[Epoch 2/200] [Batch 825/938] [D loss: 1.127016, acc: 91%] [G loss: 1.158923]\n",
      "[Epoch 2/200] [Batch 826/938] [D loss: 1.111331, acc: 94%] [G loss: 1.204637]\n",
      "[Epoch 2/200] [Batch 827/938] [D loss: 1.080899, acc: 96%] [G loss: 1.078459]\n",
      "[Epoch 2/200] [Batch 828/938] [D loss: 1.094174, acc: 92%] [G loss: 1.104431]\n",
      "[Epoch 2/200] [Batch 829/938] [D loss: 1.099037, acc: 96%] [G loss: 1.176530]\n",
      "[Epoch 2/200] [Batch 830/938] [D loss: 1.062885, acc: 95%] [G loss: 1.149311]\n",
      "[Epoch 2/200] [Batch 831/938] [D loss: 1.096343, acc: 92%] [G loss: 1.120366]\n",
      "[Epoch 2/200] [Batch 832/938] [D loss: 1.054441, acc: 96%] [G loss: 1.185510]\n",
      "[Epoch 2/200] [Batch 833/938] [D loss: 1.089667, acc: 96%] [G loss: 1.178599]\n",
      "[Epoch 2/200] [Batch 834/938] [D loss: 1.153933, acc: 94%] [G loss: 1.120298]\n",
      "[Epoch 2/200] [Batch 835/938] [D loss: 1.074861, acc: 91%] [G loss: 1.211948]\n",
      "[Epoch 2/200] [Batch 836/938] [D loss: 1.080802, acc: 98%] [G loss: 1.193995]\n",
      "[Epoch 2/200] [Batch 837/938] [D loss: 1.143407, acc: 90%] [G loss: 1.153663]\n",
      "[Epoch 2/200] [Batch 838/938] [D loss: 1.067645, acc: 95%] [G loss: 1.171941]\n",
      "[Epoch 2/200] [Batch 839/938] [D loss: 1.088515, acc: 91%] [G loss: 1.159651]\n",
      "[Epoch 2/200] [Batch 840/938] [D loss: 1.080060, acc: 95%] [G loss: 1.179994]\n",
      "[Epoch 2/200] [Batch 841/938] [D loss: 1.035698, acc: 94%] [G loss: 1.128930]\n",
      "[Epoch 2/200] [Batch 842/938] [D loss: 1.106964, acc: 89%] [G loss: 1.184999]\n",
      "[Epoch 2/200] [Batch 843/938] [D loss: 1.083166, acc: 90%] [G loss: 1.173967]\n",
      "[Epoch 2/200] [Batch 844/938] [D loss: 1.088301, acc: 91%] [G loss: 1.310267]\n",
      "[Epoch 2/200] [Batch 845/938] [D loss: 1.110940, acc: 87%] [G loss: 1.168438]\n",
      "[Epoch 2/200] [Batch 846/938] [D loss: 1.116585, acc: 94%] [G loss: 1.178954]\n",
      "[Epoch 2/200] [Batch 847/938] [D loss: 1.059549, acc: 96%] [G loss: 1.103317]\n",
      "[Epoch 2/200] [Batch 848/938] [D loss: 1.073027, acc: 94%] [G loss: 1.175351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 849/938] [D loss: 1.051160, acc: 92%] [G loss: 1.244222]\n",
      "[Epoch 2/200] [Batch 850/938] [D loss: 1.078624, acc: 90%] [G loss: 1.147295]\n",
      "[Epoch 2/200] [Batch 851/938] [D loss: 1.062066, acc: 92%] [G loss: 1.162366]\n",
      "[Epoch 2/200] [Batch 852/938] [D loss: 1.094773, acc: 95%] [G loss: 1.184221]\n",
      "[Epoch 2/200] [Batch 853/938] [D loss: 1.160106, acc: 91%] [G loss: 1.155677]\n",
      "[Epoch 2/200] [Batch 854/938] [D loss: 1.085941, acc: 97%] [G loss: 1.141422]\n",
      "[Epoch 2/200] [Batch 855/938] [D loss: 1.096327, acc: 89%] [G loss: 1.203769]\n",
      "[Epoch 2/200] [Batch 856/938] [D loss: 1.077375, acc: 97%] [G loss: 1.206538]\n",
      "[Epoch 2/200] [Batch 857/938] [D loss: 1.127210, acc: 89%] [G loss: 1.195626]\n",
      "[Epoch 2/200] [Batch 858/938] [D loss: 1.087838, acc: 91%] [G loss: 1.223405]\n",
      "[Epoch 2/200] [Batch 859/938] [D loss: 1.105182, acc: 90%] [G loss: 1.213907]\n",
      "[Epoch 2/200] [Batch 860/938] [D loss: 1.122059, acc: 92%] [G loss: 1.142308]\n",
      "[Epoch 2/200] [Batch 861/938] [D loss: 1.132360, acc: 94%] [G loss: 1.210479]\n",
      "[Epoch 2/200] [Batch 862/938] [D loss: 1.093354, acc: 93%] [G loss: 1.180542]\n",
      "[Epoch 2/200] [Batch 863/938] [D loss: 1.092760, acc: 90%] [G loss: 1.229623]\n",
      "[Epoch 2/200] [Batch 864/938] [D loss: 1.093477, acc: 92%] [G loss: 1.143076]\n",
      "[Epoch 2/200] [Batch 865/938] [D loss: 1.104349, acc: 91%] [G loss: 1.093458]\n",
      "[Epoch 2/200] [Batch 866/938] [D loss: 1.077862, acc: 96%] [G loss: 1.166273]\n",
      "[Epoch 2/200] [Batch 867/938] [D loss: 1.046340, acc: 92%] [G loss: 1.243816]\n",
      "[Epoch 2/200] [Batch 868/938] [D loss: 1.110744, acc: 92%] [G loss: 1.189873]\n",
      "[Epoch 2/200] [Batch 869/938] [D loss: 1.112633, acc: 89%] [G loss: 1.153648]\n",
      "[Epoch 2/200] [Batch 870/938] [D loss: 1.075684, acc: 93%] [G loss: 1.140065]\n",
      "[Epoch 2/200] [Batch 871/938] [D loss: 1.081959, acc: 93%] [G loss: 1.209043]\n",
      "[Epoch 2/200] [Batch 872/938] [D loss: 1.126571, acc: 92%] [G loss: 1.128786]\n",
      "[Epoch 2/200] [Batch 873/938] [D loss: 1.060651, acc: 96%] [G loss: 1.195478]\n",
      "[Epoch 2/200] [Batch 874/938] [D loss: 1.097328, acc: 89%] [G loss: 1.144724]\n",
      "[Epoch 2/200] [Batch 875/938] [D loss: 1.061169, acc: 96%] [G loss: 1.233491]\n",
      "[Epoch 2/200] [Batch 876/938] [D loss: 1.073523, acc: 92%] [G loss: 1.148831]\n",
      "[Epoch 2/200] [Batch 877/938] [D loss: 1.070835, acc: 96%] [G loss: 1.163985]\n",
      "[Epoch 2/200] [Batch 878/938] [D loss: 1.067711, acc: 95%] [G loss: 1.166816]\n",
      "[Epoch 2/200] [Batch 879/938] [D loss: 1.068598, acc: 96%] [G loss: 1.127593]\n",
      "[Epoch 2/200] [Batch 880/938] [D loss: 1.077756, acc: 93%] [G loss: 1.205569]\n",
      "[Epoch 2/200] [Batch 881/938] [D loss: 1.044155, acc: 94%] [G loss: 1.219254]\n",
      "[Epoch 2/200] [Batch 882/938] [D loss: 1.068800, acc: 94%] [G loss: 1.208728]\n",
      "[Epoch 2/200] [Batch 883/938] [D loss: 1.056629, acc: 96%] [G loss: 1.167952]\n",
      "[Epoch 2/200] [Batch 884/938] [D loss: 1.135809, acc: 90%] [G loss: 1.217539]\n",
      "[Epoch 2/200] [Batch 885/938] [D loss: 1.081649, acc: 96%] [G loss: 1.195245]\n",
      "[Epoch 2/200] [Batch 886/938] [D loss: 1.098109, acc: 92%] [G loss: 1.125861]\n",
      "[Epoch 2/200] [Batch 887/938] [D loss: 1.066457, acc: 92%] [G loss: 1.162033]\n",
      "[Epoch 2/200] [Batch 888/938] [D loss: 1.063661, acc: 96%] [G loss: 1.110877]\n",
      "[Epoch 2/200] [Batch 889/938] [D loss: 1.116056, acc: 95%] [G loss: 1.180680]\n",
      "[Epoch 2/200] [Batch 890/938] [D loss: 1.081834, acc: 94%] [G loss: 1.206299]\n",
      "[Epoch 2/200] [Batch 891/938] [D loss: 1.081958, acc: 96%] [G loss: 1.223818]\n",
      "[Epoch 2/200] [Batch 892/938] [D loss: 1.070269, acc: 90%] [G loss: 1.149449]\n",
      "[Epoch 2/200] [Batch 893/938] [D loss: 1.055821, acc: 96%] [G loss: 1.158136]\n",
      "[Epoch 2/200] [Batch 894/938] [D loss: 1.086266, acc: 93%] [G loss: 1.167389]\n",
      "[Epoch 2/200] [Batch 895/938] [D loss: 1.081299, acc: 95%] [G loss: 1.232006]\n",
      "[Epoch 2/200] [Batch 896/938] [D loss: 1.120488, acc: 92%] [G loss: 1.239842]\n",
      "[Epoch 2/200] [Batch 897/938] [D loss: 1.076052, acc: 93%] [G loss: 1.302287]\n",
      "[Epoch 2/200] [Batch 898/938] [D loss: 1.067311, acc: 94%] [G loss: 1.127302]\n",
      "[Epoch 2/200] [Batch 899/938] [D loss: 1.101004, acc: 93%] [G loss: 1.176590]\n",
      "[Epoch 2/200] [Batch 900/938] [D loss: 1.095230, acc: 93%] [G loss: 1.138812]\n",
      "[Epoch 2/200] [Batch 901/938] [D loss: 1.091192, acc: 94%] [G loss: 1.143975]\n",
      "[Epoch 2/200] [Batch 902/938] [D loss: 1.024567, acc: 94%] [G loss: 1.204666]\n",
      "[Epoch 2/200] [Batch 903/938] [D loss: 1.121046, acc: 96%] [G loss: 1.142942]\n",
      "[Epoch 2/200] [Batch 904/938] [D loss: 1.109981, acc: 92%] [G loss: 1.183584]\n",
      "[Epoch 2/200] [Batch 905/938] [D loss: 1.098256, acc: 96%] [G loss: 1.123729]\n",
      "[Epoch 2/200] [Batch 906/938] [D loss: 1.073607, acc: 95%] [G loss: 1.132267]\n",
      "[Epoch 2/200] [Batch 907/938] [D loss: 1.073807, acc: 95%] [G loss: 1.225718]\n",
      "[Epoch 2/200] [Batch 908/938] [D loss: 1.145115, acc: 93%] [G loss: 1.187653]\n",
      "[Epoch 2/200] [Batch 909/938] [D loss: 1.053393, acc: 95%] [G loss: 1.179677]\n",
      "[Epoch 2/200] [Batch 910/938] [D loss: 1.105911, acc: 95%] [G loss: 1.151983]\n",
      "[Epoch 2/200] [Batch 911/938] [D loss: 1.099924, acc: 92%] [G loss: 1.084189]\n",
      "[Epoch 2/200] [Batch 912/938] [D loss: 1.078361, acc: 94%] [G loss: 1.136690]\n",
      "[Epoch 2/200] [Batch 913/938] [D loss: 1.094856, acc: 92%] [G loss: 1.266811]\n",
      "[Epoch 2/200] [Batch 914/938] [D loss: 1.112294, acc: 95%] [G loss: 1.165328]\n",
      "[Epoch 2/200] [Batch 915/938] [D loss: 1.075805, acc: 94%] [G loss: 1.130343]\n",
      "[Epoch 2/200] [Batch 916/938] [D loss: 1.130272, acc: 90%] [G loss: 1.139330]\n",
      "[Epoch 2/200] [Batch 917/938] [D loss: 1.090584, acc: 92%] [G loss: 1.206708]\n",
      "[Epoch 2/200] [Batch 918/938] [D loss: 1.050087, acc: 96%] [G loss: 1.236175]\n",
      "[Epoch 2/200] [Batch 919/938] [D loss: 1.064067, acc: 98%] [G loss: 1.209057]\n",
      "[Epoch 2/200] [Batch 920/938] [D loss: 1.058604, acc: 92%] [G loss: 1.157975]\n",
      "[Epoch 2/200] [Batch 921/938] [D loss: 1.084409, acc: 91%] [G loss: 1.163429]\n",
      "[Epoch 2/200] [Batch 922/938] [D loss: 1.115421, acc: 94%] [G loss: 1.082114]\n",
      "[Epoch 2/200] [Batch 923/938] [D loss: 1.068783, acc: 91%] [G loss: 1.198270]\n",
      "[Epoch 2/200] [Batch 924/938] [D loss: 1.097668, acc: 96%] [G loss: 1.133420]\n",
      "[Epoch 2/200] [Batch 925/938] [D loss: 1.055853, acc: 91%] [G loss: 1.231759]\n",
      "[Epoch 2/200] [Batch 926/938] [D loss: 1.076348, acc: 93%] [G loss: 1.229290]\n",
      "[Epoch 2/200] [Batch 927/938] [D loss: 1.111402, acc: 92%] [G loss: 1.178294]\n",
      "[Epoch 2/200] [Batch 928/938] [D loss: 1.133155, acc: 87%] [G loss: 1.236408]\n",
      "[Epoch 2/200] [Batch 929/938] [D loss: 1.067297, acc: 96%] [G loss: 1.117344]\n",
      "[Epoch 2/200] [Batch 930/938] [D loss: 1.083311, acc: 96%] [G loss: 1.125214]\n",
      "[Epoch 2/200] [Batch 931/938] [D loss: 1.060513, acc: 92%] [G loss: 1.185889]\n",
      "[Epoch 2/200] [Batch 932/938] [D loss: 1.098543, acc: 90%] [G loss: 1.235162]\n",
      "[Epoch 2/200] [Batch 933/938] [D loss: 1.095726, acc: 92%] [G loss: 1.125541]\n",
      "[Epoch 2/200] [Batch 934/938] [D loss: 1.068837, acc: 96%] [G loss: 1.123857]\n",
      "[Epoch 2/200] [Batch 935/938] [D loss: 1.063236, acc: 95%] [G loss: 1.154831]\n",
      "[Epoch 2/200] [Batch 936/938] [D loss: 1.087827, acc: 93%] [G loss: 1.137213]\n",
      "[Epoch 2/200] [Batch 937/938] [D loss: 1.039840, acc: 95%] [G loss: 1.129378]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2276f5ae03b4046a111e97b8be28022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 0/938] [D loss: 1.072714, acc: 96%] [G loss: 1.138854]\n",
      "[Epoch 3/200] [Batch 1/938] [D loss: 1.079539, acc: 92%] [G loss: 1.145739]\n",
      "[Epoch 3/200] [Batch 2/938] [D loss: 1.075842, acc: 93%] [G loss: 1.163784]\n",
      "[Epoch 3/200] [Batch 3/938] [D loss: 1.050027, acc: 96%] [G loss: 1.186024]\n",
      "[Epoch 3/200] [Batch 4/938] [D loss: 1.083468, acc: 96%] [G loss: 1.241737]\n",
      "[Epoch 3/200] [Batch 5/938] [D loss: 1.066809, acc: 97%] [G loss: 1.141146]\n",
      "[Epoch 3/200] [Batch 6/938] [D loss: 1.138836, acc: 92%] [G loss: 1.112063]\n",
      "[Epoch 3/200] [Batch 7/938] [D loss: 1.079655, acc: 92%] [G loss: 1.206125]\n",
      "[Epoch 3/200] [Batch 8/938] [D loss: 1.084040, acc: 94%] [G loss: 1.197569]\n",
      "[Epoch 3/200] [Batch 9/938] [D loss: 1.101972, acc: 94%] [G loss: 1.150477]\n",
      "[Epoch 3/200] [Batch 10/938] [D loss: 1.100718, acc: 92%] [G loss: 1.099139]\n",
      "[Epoch 3/200] [Batch 11/938] [D loss: 1.063922, acc: 95%] [G loss: 1.209785]\n",
      "[Epoch 3/200] [Batch 12/938] [D loss: 1.112347, acc: 95%] [G loss: 1.141094]\n",
      "[Epoch 3/200] [Batch 13/938] [D loss: 1.045724, acc: 95%] [G loss: 1.188248]\n",
      "[Epoch 3/200] [Batch 14/938] [D loss: 1.106344, acc: 92%] [G loss: 1.214501]\n",
      "[Epoch 3/200] [Batch 15/938] [D loss: 1.059549, acc: 94%] [G loss: 1.125923]\n",
      "[Epoch 3/200] [Batch 16/938] [D loss: 1.100571, acc: 89%] [G loss: 1.159479]\n",
      "[Epoch 3/200] [Batch 17/938] [D loss: 1.053503, acc: 96%] [G loss: 1.169149]\n",
      "[Epoch 3/200] [Batch 18/938] [D loss: 1.090062, acc: 92%] [G loss: 1.182446]\n",
      "[Epoch 3/200] [Batch 19/938] [D loss: 1.066999, acc: 92%] [G loss: 1.203865]\n",
      "[Epoch 3/200] [Batch 20/938] [D loss: 1.065727, acc: 90%] [G loss: 1.202709]\n",
      "[Epoch 3/200] [Batch 21/938] [D loss: 1.128240, acc: 95%] [G loss: 1.076045]\n",
      "[Epoch 3/200] [Batch 22/938] [D loss: 1.062377, acc: 97%] [G loss: 1.150122]\n",
      "[Epoch 3/200] [Batch 23/938] [D loss: 1.058778, acc: 95%] [G loss: 1.108694]\n",
      "[Epoch 3/200] [Batch 24/938] [D loss: 1.086603, acc: 96%] [G loss: 1.215192]\n",
      "[Epoch 3/200] [Batch 25/938] [D loss: 1.066697, acc: 96%] [G loss: 1.208041]\n",
      "[Epoch 3/200] [Batch 26/938] [D loss: 1.080733, acc: 92%] [G loss: 1.227520]\n",
      "[Epoch 3/200] [Batch 27/938] [D loss: 1.075988, acc: 96%] [G loss: 1.190429]\n",
      "[Epoch 3/200] [Batch 28/938] [D loss: 1.095675, acc: 94%] [G loss: 1.181444]\n",
      "[Epoch 3/200] [Batch 29/938] [D loss: 1.113732, acc: 93%] [G loss: 1.154793]\n",
      "[Epoch 3/200] [Batch 30/938] [D loss: 1.070034, acc: 92%] [G loss: 1.176153]\n",
      "[Epoch 3/200] [Batch 31/938] [D loss: 1.079427, acc: 94%] [G loss: 1.141731]\n",
      "[Epoch 3/200] [Batch 32/938] [D loss: 1.060012, acc: 95%] [G loss: 1.153660]\n",
      "[Epoch 3/200] [Batch 33/938] [D loss: 1.086971, acc: 95%] [G loss: 1.175287]\n",
      "[Epoch 3/200] [Batch 34/938] [D loss: 1.065691, acc: 92%] [G loss: 1.187740]\n",
      "[Epoch 3/200] [Batch 35/938] [D loss: 1.077996, acc: 95%] [G loss: 1.261318]\n",
      "[Epoch 3/200] [Batch 36/938] [D loss: 1.030783, acc: 96%] [G loss: 1.151571]\n",
      "[Epoch 3/200] [Batch 37/938] [D loss: 1.080484, acc: 95%] [G loss: 1.178640]\n",
      "[Epoch 3/200] [Batch 38/938] [D loss: 1.075990, acc: 92%] [G loss: 1.129949]\n",
      "[Epoch 3/200] [Batch 39/938] [D loss: 1.082830, acc: 95%] [G loss: 1.132786]\n",
      "[Epoch 3/200] [Batch 40/938] [D loss: 1.121670, acc: 92%] [G loss: 1.110829]\n",
      "[Epoch 3/200] [Batch 41/938] [D loss: 1.118038, acc: 94%] [G loss: 1.255201]\n",
      "[Epoch 3/200] [Batch 42/938] [D loss: 1.097911, acc: 92%] [G loss: 1.203055]\n",
      "[Epoch 3/200] [Batch 43/938] [D loss: 1.098035, acc: 96%] [G loss: 1.115813]\n",
      "[Epoch 3/200] [Batch 44/938] [D loss: 1.099499, acc: 92%] [G loss: 1.107940]\n",
      "[Epoch 3/200] [Batch 45/938] [D loss: 1.078581, acc: 94%] [G loss: 1.156579]\n",
      "[Epoch 3/200] [Batch 46/938] [D loss: 1.067302, acc: 95%] [G loss: 1.146216]\n",
      "[Epoch 3/200] [Batch 47/938] [D loss: 1.091035, acc: 94%] [G loss: 1.157972]\n",
      "[Epoch 3/200] [Batch 48/938] [D loss: 1.107465, acc: 92%] [G loss: 1.230357]\n",
      "[Epoch 3/200] [Batch 49/938] [D loss: 1.118522, acc: 92%] [G loss: 1.142588]\n",
      "[Epoch 3/200] [Batch 50/938] [D loss: 1.074693, acc: 93%] [G loss: 1.146899]\n",
      "[Epoch 3/200] [Batch 51/938] [D loss: 1.099517, acc: 93%] [G loss: 1.187057]\n",
      "[Epoch 3/200] [Batch 52/938] [D loss: 1.081717, acc: 94%] [G loss: 1.224712]\n",
      "[Epoch 3/200] [Batch 53/938] [D loss: 1.070005, acc: 96%] [G loss: 1.107242]\n",
      "[Epoch 3/200] [Batch 54/938] [D loss: 1.101557, acc: 95%] [G loss: 1.127710]\n",
      "[Epoch 3/200] [Batch 55/938] [D loss: 1.120112, acc: 94%] [G loss: 1.114400]\n",
      "[Epoch 3/200] [Batch 56/938] [D loss: 1.109625, acc: 94%] [G loss: 1.102933]\n",
      "[Epoch 3/200] [Batch 57/938] [D loss: 1.106920, acc: 92%] [G loss: 1.135837]\n",
      "[Epoch 3/200] [Batch 58/938] [D loss: 1.099487, acc: 96%] [G loss: 1.108052]\n",
      "[Epoch 3/200] [Batch 59/938] [D loss: 1.096642, acc: 94%] [G loss: 1.144925]\n",
      "[Epoch 3/200] [Batch 60/938] [D loss: 1.077360, acc: 94%] [G loss: 1.168461]\n",
      "[Epoch 3/200] [Batch 61/938] [D loss: 1.101439, acc: 96%] [G loss: 1.086422]\n",
      "[Epoch 3/200] [Batch 62/938] [D loss: 1.059433, acc: 96%] [G loss: 1.118793]\n",
      "[Epoch 3/200] [Batch 63/938] [D loss: 1.035864, acc: 93%] [G loss: 1.175494]\n",
      "[Epoch 3/200] [Batch 64/938] [D loss: 1.092152, acc: 93%] [G loss: 1.184704]\n",
      "[Epoch 3/200] [Batch 65/938] [D loss: 1.106058, acc: 94%] [G loss: 1.147136]\n",
      "[Epoch 3/200] [Batch 66/938] [D loss: 1.087233, acc: 95%] [G loss: 1.133587]\n",
      "[Epoch 3/200] [Batch 67/938] [D loss: 1.096346, acc: 94%] [G loss: 1.146394]\n",
      "[Epoch 3/200] [Batch 68/938] [D loss: 1.084653, acc: 93%] [G loss: 1.259709]\n",
      "[Epoch 3/200] [Batch 69/938] [D loss: 1.087086, acc: 96%] [G loss: 1.192102]\n",
      "[Epoch 3/200] [Batch 70/938] [D loss: 1.079611, acc: 94%] [G loss: 1.157691]\n",
      "[Epoch 3/200] [Batch 71/938] [D loss: 1.096088, acc: 92%] [G loss: 1.148601]\n",
      "[Epoch 3/200] [Batch 72/938] [D loss: 1.083339, acc: 92%] [G loss: 1.220482]\n",
      "[Epoch 3/200] [Batch 73/938] [D loss: 1.099766, acc: 93%] [G loss: 1.125441]\n",
      "[Epoch 3/200] [Batch 74/938] [D loss: 1.067227, acc: 98%] [G loss: 1.103216]\n",
      "[Epoch 3/200] [Batch 75/938] [D loss: 1.110029, acc: 92%] [G loss: 1.144197]\n",
      "[Epoch 3/200] [Batch 76/938] [D loss: 1.104438, acc: 88%] [G loss: 1.165637]\n",
      "[Epoch 3/200] [Batch 77/938] [D loss: 1.050744, acc: 97%] [G loss: 1.218499]\n",
      "[Epoch 3/200] [Batch 78/938] [D loss: 1.087008, acc: 97%] [G loss: 1.228748]\n",
      "[Epoch 3/200] [Batch 79/938] [D loss: 1.110318, acc: 93%] [G loss: 1.219050]\n",
      "[Epoch 3/200] [Batch 80/938] [D loss: 1.060884, acc: 96%] [G loss: 1.144340]\n",
      "[Epoch 3/200] [Batch 81/938] [D loss: 1.077337, acc: 95%] [G loss: 1.092534]\n",
      "[Epoch 3/200] [Batch 82/938] [D loss: 1.058851, acc: 92%] [G loss: 1.148086]\n",
      "[Epoch 3/200] [Batch 83/938] [D loss: 1.064742, acc: 96%] [G loss: 1.155724]\n",
      "[Epoch 3/200] [Batch 84/938] [D loss: 1.042602, acc: 94%] [G loss: 1.192834]\n",
      "[Epoch 3/200] [Batch 85/938] [D loss: 1.101059, acc: 91%] [G loss: 1.135904]\n",
      "[Epoch 3/200] [Batch 86/938] [D loss: 1.105922, acc: 92%] [G loss: 1.132642]\n",
      "[Epoch 3/200] [Batch 87/938] [D loss: 1.094487, acc: 97%] [G loss: 1.148750]\n",
      "[Epoch 3/200] [Batch 88/938] [D loss: 1.135071, acc: 89%] [G loss: 1.173773]\n",
      "[Epoch 3/200] [Batch 89/938] [D loss: 1.074493, acc: 95%] [G loss: 1.160310]\n",
      "[Epoch 3/200] [Batch 90/938] [D loss: 1.109143, acc: 94%] [G loss: 1.156501]\n",
      "[Epoch 3/200] [Batch 91/938] [D loss: 1.085346, acc: 96%] [G loss: 1.128824]\n",
      "[Epoch 3/200] [Batch 92/938] [D loss: 1.111257, acc: 96%] [G loss: 1.131897]\n",
      "[Epoch 3/200] [Batch 93/938] [D loss: 1.090805, acc: 96%] [G loss: 1.169129]\n",
      "[Epoch 3/200] [Batch 94/938] [D loss: 1.114433, acc: 91%] [G loss: 1.186372]\n",
      "[Epoch 3/200] [Batch 95/938] [D loss: 1.059074, acc: 97%] [G loss: 1.129238]\n",
      "[Epoch 3/200] [Batch 96/938] [D loss: 1.080604, acc: 95%] [G loss: 1.173551]\n",
      "[Epoch 3/200] [Batch 97/938] [D loss: 1.101088, acc: 91%] [G loss: 1.232207]\n",
      "[Epoch 3/200] [Batch 98/938] [D loss: 1.070198, acc: 95%] [G loss: 1.125525]\n",
      "[Epoch 3/200] [Batch 99/938] [D loss: 1.071471, acc: 95%] [G loss: 1.137595]\n",
      "[Epoch 3/200] [Batch 100/938] [D loss: 1.088376, acc: 92%] [G loss: 1.109530]\n",
      "[Epoch 3/200] [Batch 101/938] [D loss: 1.099278, acc: 90%] [G loss: 1.198884]\n",
      "[Epoch 3/200] [Batch 102/938] [D loss: 1.073444, acc: 91%] [G loss: 1.203864]\n",
      "[Epoch 3/200] [Batch 103/938] [D loss: 1.088111, acc: 97%] [G loss: 1.190625]\n",
      "[Epoch 3/200] [Batch 104/938] [D loss: 1.105026, acc: 93%] [G loss: 1.223324]\n",
      "[Epoch 3/200] [Batch 105/938] [D loss: 1.080163, acc: 91%] [G loss: 1.230163]\n",
      "[Epoch 3/200] [Batch 106/938] [D loss: 1.081273, acc: 93%] [G loss: 1.177456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 107/938] [D loss: 1.075289, acc: 91%] [G loss: 1.205831]\n",
      "[Epoch 3/200] [Batch 108/938] [D loss: 1.097090, acc: 92%] [G loss: 1.126397]\n",
      "[Epoch 3/200] [Batch 109/938] [D loss: 1.109792, acc: 91%] [G loss: 1.203012]\n",
      "[Epoch 3/200] [Batch 110/938] [D loss: 1.095421, acc: 92%] [G loss: 1.148052]\n",
      "[Epoch 3/200] [Batch 111/938] [D loss: 1.088406, acc: 92%] [G loss: 1.220828]\n",
      "[Epoch 3/200] [Batch 112/938] [D loss: 1.090239, acc: 92%] [G loss: 1.102894]\n",
      "[Epoch 3/200] [Batch 113/938] [D loss: 1.103595, acc: 93%] [G loss: 1.119544]\n",
      "[Epoch 3/200] [Batch 114/938] [D loss: 1.063030, acc: 95%] [G loss: 1.160781]\n",
      "[Epoch 3/200] [Batch 115/938] [D loss: 1.126558, acc: 92%] [G loss: 1.250502]\n",
      "[Epoch 3/200] [Batch 116/938] [D loss: 1.095304, acc: 93%] [G loss: 1.136691]\n",
      "[Epoch 3/200] [Batch 117/938] [D loss: 1.101430, acc: 92%] [G loss: 1.171251]\n",
      "[Epoch 3/200] [Batch 118/938] [D loss: 1.102468, acc: 89%] [G loss: 1.117683]\n",
      "[Epoch 3/200] [Batch 119/938] [D loss: 1.081921, acc: 97%] [G loss: 1.201791]\n",
      "[Epoch 3/200] [Batch 120/938] [D loss: 1.108119, acc: 95%] [G loss: 1.184566]\n",
      "[Epoch 3/200] [Batch 121/938] [D loss: 1.096308, acc: 96%] [G loss: 1.169378]\n",
      "[Epoch 3/200] [Batch 122/938] [D loss: 1.083592, acc: 94%] [G loss: 1.180235]\n",
      "[Epoch 3/200] [Batch 123/938] [D loss: 1.114179, acc: 94%] [G loss: 1.155989]\n",
      "[Epoch 3/200] [Batch 124/938] [D loss: 1.063905, acc: 95%] [G loss: 1.067340]\n",
      "[Epoch 3/200] [Batch 125/938] [D loss: 1.077736, acc: 91%] [G loss: 1.177442]\n",
      "[Epoch 3/200] [Batch 126/938] [D loss: 1.094312, acc: 92%] [G loss: 1.107389]\n",
      "[Epoch 3/200] [Batch 127/938] [D loss: 1.044647, acc: 93%] [G loss: 1.247386]\n",
      "[Epoch 3/200] [Batch 128/938] [D loss: 1.046179, acc: 99%] [G loss: 1.208403]\n",
      "[Epoch 3/200] [Batch 129/938] [D loss: 1.091638, acc: 96%] [G loss: 1.096021]\n",
      "[Epoch 3/200] [Batch 130/938] [D loss: 1.094438, acc: 95%] [G loss: 1.172428]\n",
      "[Epoch 3/200] [Batch 131/938] [D loss: 1.038696, acc: 97%] [G loss: 1.130667]\n",
      "[Epoch 3/200] [Batch 132/938] [D loss: 1.061899, acc: 93%] [G loss: 1.175113]\n",
      "[Epoch 3/200] [Batch 133/938] [D loss: 1.101681, acc: 92%] [G loss: 1.183577]\n",
      "[Epoch 3/200] [Batch 134/938] [D loss: 1.076246, acc: 93%] [G loss: 1.143625]\n",
      "[Epoch 3/200] [Batch 135/938] [D loss: 1.070532, acc: 93%] [G loss: 1.261580]\n",
      "[Epoch 3/200] [Batch 136/938] [D loss: 1.083844, acc: 98%] [G loss: 1.123897]\n",
      "[Epoch 3/200] [Batch 137/938] [D loss: 1.060737, acc: 91%] [G loss: 1.205751]\n",
      "[Epoch 3/200] [Batch 138/938] [D loss: 1.089060, acc: 92%] [G loss: 1.135545]\n",
      "[Epoch 3/200] [Batch 139/938] [D loss: 1.102648, acc: 94%] [G loss: 1.065935]\n",
      "[Epoch 3/200] [Batch 140/938] [D loss: 1.086756, acc: 92%] [G loss: 1.167090]\n",
      "[Epoch 3/200] [Batch 141/938] [D loss: 1.038028, acc: 96%] [G loss: 1.220985]\n",
      "[Epoch 3/200] [Batch 142/938] [D loss: 1.056103, acc: 92%] [G loss: 1.265991]\n",
      "[Epoch 3/200] [Batch 143/938] [D loss: 1.092237, acc: 92%] [G loss: 1.231591]\n",
      "[Epoch 3/200] [Batch 144/938] [D loss: 1.102315, acc: 92%] [G loss: 1.224350]\n",
      "[Epoch 3/200] [Batch 145/938] [D loss: 1.092293, acc: 97%] [G loss: 1.137958]\n",
      "[Epoch 3/200] [Batch 146/938] [D loss: 1.085826, acc: 96%] [G loss: 1.140495]\n",
      "[Epoch 3/200] [Batch 147/938] [D loss: 1.096145, acc: 95%] [G loss: 1.149620]\n",
      "[Epoch 3/200] [Batch 148/938] [D loss: 1.112218, acc: 96%] [G loss: 1.162327]\n",
      "[Epoch 3/200] [Batch 149/938] [D loss: 1.100189, acc: 93%] [G loss: 1.040292]\n",
      "[Epoch 3/200] [Batch 150/938] [D loss: 1.137098, acc: 92%] [G loss: 1.149529]\n",
      "[Epoch 3/200] [Batch 151/938] [D loss: 1.112071, acc: 92%] [G loss: 1.169217]\n",
      "[Epoch 3/200] [Batch 152/938] [D loss: 1.083835, acc: 92%] [G loss: 1.139264]\n",
      "[Epoch 3/200] [Batch 153/938] [D loss: 1.102622, acc: 92%] [G loss: 1.247738]\n",
      "[Epoch 3/200] [Batch 154/938] [D loss: 1.103576, acc: 93%] [G loss: 1.172644]\n",
      "[Epoch 3/200] [Batch 155/938] [D loss: 1.026575, acc: 100%] [G loss: 1.174923]\n",
      "[Epoch 3/200] [Batch 156/938] [D loss: 1.078403, acc: 92%] [G loss: 1.168652]\n",
      "[Epoch 3/200] [Batch 157/938] [D loss: 1.150921, acc: 89%] [G loss: 1.111336]\n",
      "[Epoch 3/200] [Batch 158/938] [D loss: 1.088143, acc: 94%] [G loss: 1.139808]\n",
      "[Epoch 3/200] [Batch 159/938] [D loss: 1.081864, acc: 92%] [G loss: 1.172000]\n",
      "[Epoch 3/200] [Batch 160/938] [D loss: 1.076018, acc: 96%] [G loss: 1.241330]\n",
      "[Epoch 3/200] [Batch 161/938] [D loss: 1.097689, acc: 94%] [G loss: 1.137834]\n",
      "[Epoch 3/200] [Batch 162/938] [D loss: 1.032428, acc: 96%] [G loss: 1.159737]\n",
      "[Epoch 3/200] [Batch 163/938] [D loss: 1.106626, acc: 96%] [G loss: 1.138469]\n",
      "[Epoch 3/200] [Batch 164/938] [D loss: 1.058803, acc: 95%] [G loss: 1.161989]\n",
      "[Epoch 3/200] [Batch 165/938] [D loss: 1.076435, acc: 95%] [G loss: 1.104704]\n",
      "[Epoch 3/200] [Batch 166/938] [D loss: 1.037419, acc: 97%] [G loss: 1.149827]\n",
      "[Epoch 3/200] [Batch 167/938] [D loss: 1.080042, acc: 89%] [G loss: 1.140697]\n",
      "[Epoch 3/200] [Batch 168/938] [D loss: 1.085331, acc: 96%] [G loss: 1.159458]\n",
      "[Epoch 3/200] [Batch 169/938] [D loss: 1.088928, acc: 95%] [G loss: 1.104703]\n",
      "[Epoch 3/200] [Batch 170/938] [D loss: 1.131713, acc: 95%] [G loss: 1.188937]\n",
      "[Epoch 3/200] [Batch 171/938] [D loss: 1.081085, acc: 91%] [G loss: 1.241776]\n",
      "[Epoch 3/200] [Batch 172/938] [D loss: 1.090449, acc: 98%] [G loss: 1.185717]\n",
      "[Epoch 3/200] [Batch 173/938] [D loss: 1.058291, acc: 96%] [G loss: 1.196361]\n",
      "[Epoch 3/200] [Batch 174/938] [D loss: 1.105156, acc: 88%] [G loss: 1.140538]\n",
      "[Epoch 3/200] [Batch 175/938] [D loss: 1.076428, acc: 95%] [G loss: 1.165190]\n",
      "[Epoch 3/200] [Batch 176/938] [D loss: 1.060933, acc: 95%] [G loss: 1.161349]\n",
      "[Epoch 3/200] [Batch 177/938] [D loss: 1.057848, acc: 94%] [G loss: 1.141969]\n",
      "[Epoch 3/200] [Batch 178/938] [D loss: 1.082312, acc: 94%] [G loss: 1.208286]\n",
      "[Epoch 3/200] [Batch 179/938] [D loss: 1.058569, acc: 97%] [G loss: 1.140290]\n",
      "[Epoch 3/200] [Batch 180/938] [D loss: 1.099921, acc: 92%] [G loss: 1.093378]\n",
      "[Epoch 3/200] [Batch 181/938] [D loss: 1.064823, acc: 96%] [G loss: 1.139628]\n",
      "[Epoch 3/200] [Batch 182/938] [D loss: 1.054007, acc: 95%] [G loss: 1.125113]\n",
      "[Epoch 3/200] [Batch 183/938] [D loss: 1.094747, acc: 91%] [G loss: 1.156670]\n",
      "[Epoch 3/200] [Batch 184/938] [D loss: 1.080071, acc: 89%] [G loss: 1.187513]\n",
      "[Epoch 3/200] [Batch 185/938] [D loss: 1.099286, acc: 92%] [G loss: 1.159233]\n",
      "[Epoch 3/200] [Batch 186/938] [D loss: 1.061763, acc: 90%] [G loss: 1.083961]\n",
      "[Epoch 3/200] [Batch 187/938] [D loss: 1.071253, acc: 97%] [G loss: 1.099540]\n",
      "[Epoch 3/200] [Batch 188/938] [D loss: 1.081365, acc: 95%] [G loss: 1.133192]\n",
      "[Epoch 3/200] [Batch 189/938] [D loss: 1.057285, acc: 89%] [G loss: 1.228472]\n",
      "[Epoch 3/200] [Batch 190/938] [D loss: 1.072812, acc: 91%] [G loss: 1.141320]\n",
      "[Epoch 3/200] [Batch 191/938] [D loss: 1.074778, acc: 92%] [G loss: 1.156703]\n",
      "[Epoch 3/200] [Batch 192/938] [D loss: 1.065220, acc: 96%] [G loss: 1.116461]\n",
      "[Epoch 3/200] [Batch 193/938] [D loss: 1.084392, acc: 93%] [G loss: 1.112211]\n",
      "[Epoch 3/200] [Batch 194/938] [D loss: 1.086680, acc: 93%] [G loss: 1.136295]\n",
      "[Epoch 3/200] [Batch 195/938] [D loss: 1.073109, acc: 97%] [G loss: 1.211782]\n",
      "[Epoch 3/200] [Batch 196/938] [D loss: 1.137147, acc: 92%] [G loss: 1.122396]\n",
      "[Epoch 3/200] [Batch 197/938] [D loss: 1.064678, acc: 90%] [G loss: 1.172650]\n",
      "[Epoch 3/200] [Batch 198/938] [D loss: 1.102306, acc: 96%] [G loss: 1.168076]\n",
      "[Epoch 3/200] [Batch 199/938] [D loss: 1.084191, acc: 95%] [G loss: 1.182551]\n",
      "[Epoch 3/200] [Batch 200/938] [D loss: 1.046581, acc: 96%] [G loss: 1.158543]\n",
      "[Epoch 3/200] [Batch 201/938] [D loss: 1.095554, acc: 91%] [G loss: 1.213457]\n",
      "[Epoch 3/200] [Batch 202/938] [D loss: 1.063658, acc: 96%] [G loss: 1.160199]\n",
      "[Epoch 3/200] [Batch 203/938] [D loss: 1.098097, acc: 95%] [G loss: 1.149346]\n",
      "[Epoch 3/200] [Batch 204/938] [D loss: 1.088577, acc: 94%] [G loss: 1.209965]\n",
      "[Epoch 3/200] [Batch 205/938] [D loss: 1.115570, acc: 90%] [G loss: 1.115236]\n",
      "[Epoch 3/200] [Batch 206/938] [D loss: 1.128421, acc: 91%] [G loss: 1.201947]\n",
      "[Epoch 3/200] [Batch 207/938] [D loss: 1.092415, acc: 96%] [G loss: 1.139024]\n",
      "[Epoch 3/200] [Batch 208/938] [D loss: 1.109869, acc: 93%] [G loss: 1.164804]\n",
      "[Epoch 3/200] [Batch 209/938] [D loss: 1.098835, acc: 94%] [G loss: 1.157419]\n",
      "[Epoch 3/200] [Batch 210/938] [D loss: 1.095097, acc: 92%] [G loss: 1.190544]\n",
      "[Epoch 3/200] [Batch 211/938] [D loss: 1.070650, acc: 96%] [G loss: 1.161648]\n",
      "[Epoch 3/200] [Batch 212/938] [D loss: 1.084507, acc: 92%] [G loss: 1.133287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 213/938] [D loss: 1.096954, acc: 93%] [G loss: 1.124199]\n",
      "[Epoch 3/200] [Batch 214/938] [D loss: 1.056244, acc: 97%] [G loss: 1.190107]\n",
      "[Epoch 3/200] [Batch 215/938] [D loss: 1.127695, acc: 90%] [G loss: 1.075478]\n",
      "[Epoch 3/200] [Batch 216/938] [D loss: 1.084071, acc: 92%] [G loss: 1.155825]\n",
      "[Epoch 3/200] [Batch 217/938] [D loss: 1.118470, acc: 90%] [G loss: 1.201910]\n",
      "[Epoch 3/200] [Batch 218/938] [D loss: 1.084362, acc: 96%] [G loss: 1.164955]\n",
      "[Epoch 3/200] [Batch 219/938] [D loss: 1.088748, acc: 91%] [G loss: 1.176022]\n",
      "[Epoch 3/200] [Batch 220/938] [D loss: 1.107702, acc: 94%] [G loss: 1.119261]\n",
      "[Epoch 3/200] [Batch 221/938] [D loss: 1.097995, acc: 94%] [G loss: 1.222329]\n",
      "[Epoch 3/200] [Batch 222/938] [D loss: 1.076491, acc: 93%] [G loss: 1.181401]\n",
      "[Epoch 3/200] [Batch 223/938] [D loss: 1.101024, acc: 94%] [G loss: 1.111465]\n",
      "[Epoch 3/200] [Batch 224/938] [D loss: 1.091599, acc: 92%] [G loss: 1.101797]\n",
      "[Epoch 3/200] [Batch 225/938] [D loss: 1.101717, acc: 91%] [G loss: 1.156170]\n",
      "[Epoch 3/200] [Batch 226/938] [D loss: 1.048155, acc: 95%] [G loss: 1.181940]\n",
      "[Epoch 3/200] [Batch 227/938] [D loss: 1.083632, acc: 97%] [G loss: 1.204235]\n",
      "[Epoch 3/200] [Batch 228/938] [D loss: 1.107996, acc: 94%] [G loss: 1.100025]\n",
      "[Epoch 3/200] [Batch 229/938] [D loss: 1.046454, acc: 96%] [G loss: 1.082913]\n",
      "[Epoch 3/200] [Batch 230/938] [D loss: 1.083310, acc: 96%] [G loss: 1.114960]\n",
      "[Epoch 3/200] [Batch 231/938] [D loss: 1.092068, acc: 92%] [G loss: 1.084852]\n",
      "[Epoch 3/200] [Batch 232/938] [D loss: 1.081294, acc: 96%] [G loss: 1.211879]\n",
      "[Epoch 3/200] [Batch 233/938] [D loss: 1.097114, acc: 96%] [G loss: 1.155065]\n",
      "[Epoch 3/200] [Batch 234/938] [D loss: 1.076685, acc: 93%] [G loss: 1.205907]\n",
      "[Epoch 3/200] [Batch 235/938] [D loss: 1.063360, acc: 94%] [G loss: 1.162203]\n",
      "[Epoch 3/200] [Batch 236/938] [D loss: 1.047344, acc: 96%] [G loss: 1.199181]\n",
      "[Epoch 3/200] [Batch 237/938] [D loss: 1.061786, acc: 96%] [G loss: 1.144897]\n",
      "[Epoch 3/200] [Batch 238/938] [D loss: 1.089912, acc: 95%] [G loss: 1.117630]\n",
      "[Epoch 3/200] [Batch 239/938] [D loss: 1.076632, acc: 95%] [G loss: 1.113245]\n",
      "[Epoch 3/200] [Batch 240/938] [D loss: 1.059394, acc: 92%] [G loss: 1.108322]\n",
      "[Epoch 3/200] [Batch 241/938] [D loss: 1.090135, acc: 92%] [G loss: 1.205874]\n",
      "[Epoch 3/200] [Batch 242/938] [D loss: 1.074704, acc: 92%] [G loss: 1.180095]\n",
      "[Epoch 3/200] [Batch 243/938] [D loss: 1.058761, acc: 94%] [G loss: 1.189115]\n",
      "[Epoch 3/200] [Batch 244/938] [D loss: 1.084726, acc: 95%] [G loss: 1.074105]\n",
      "[Epoch 3/200] [Batch 245/938] [D loss: 1.086920, acc: 92%] [G loss: 1.163024]\n",
      "[Epoch 3/200] [Batch 246/938] [D loss: 1.070692, acc: 96%] [G loss: 1.147415]\n",
      "[Epoch 3/200] [Batch 247/938] [D loss: 1.104136, acc: 94%] [G loss: 1.242260]\n",
      "[Epoch 3/200] [Batch 248/938] [D loss: 1.107178, acc: 92%] [G loss: 1.172761]\n",
      "[Epoch 3/200] [Batch 249/938] [D loss: 1.069353, acc: 95%] [G loss: 1.104366]\n",
      "[Epoch 3/200] [Batch 250/938] [D loss: 1.109534, acc: 96%] [G loss: 1.171917]\n",
      "[Epoch 3/200] [Batch 251/938] [D loss: 1.133001, acc: 95%] [G loss: 1.160689]\n",
      "[Epoch 3/200] [Batch 252/938] [D loss: 1.061054, acc: 96%] [G loss: 1.110915]\n",
      "[Epoch 3/200] [Batch 253/938] [D loss: 1.084445, acc: 96%] [G loss: 1.058274]\n",
      "[Epoch 3/200] [Batch 254/938] [D loss: 1.106693, acc: 95%] [G loss: 1.077917]\n",
      "[Epoch 3/200] [Batch 255/938] [D loss: 1.078640, acc: 97%] [G loss: 1.096547]\n",
      "[Epoch 3/200] [Batch 256/938] [D loss: 1.083041, acc: 96%] [G loss: 1.197055]\n",
      "[Epoch 3/200] [Batch 257/938] [D loss: 1.071323, acc: 97%] [G loss: 1.109690]\n",
      "[Epoch 3/200] [Batch 258/938] [D loss: 1.019603, acc: 98%] [G loss: 1.089691]\n",
      "[Epoch 3/200] [Batch 259/938] [D loss: 1.091356, acc: 96%] [G loss: 1.181071]\n",
      "[Epoch 3/200] [Batch 260/938] [D loss: 1.034694, acc: 97%] [G loss: 1.233612]\n",
      "[Epoch 3/200] [Batch 261/938] [D loss: 1.144145, acc: 92%] [G loss: 1.194329]\n",
      "[Epoch 3/200] [Batch 262/938] [D loss: 1.087879, acc: 92%] [G loss: 1.207643]\n",
      "[Epoch 3/200] [Batch 263/938] [D loss: 1.101352, acc: 95%] [G loss: 1.115199]\n",
      "[Epoch 3/200] [Batch 264/938] [D loss: 1.079779, acc: 95%] [G loss: 1.122781]\n",
      "[Epoch 3/200] [Batch 265/938] [D loss: 1.032991, acc: 95%] [G loss: 1.232444]\n",
      "[Epoch 3/200] [Batch 266/938] [D loss: 1.110546, acc: 92%] [G loss: 1.174860]\n",
      "[Epoch 3/200] [Batch 267/938] [D loss: 1.065041, acc: 98%] [G loss: 1.129258]\n",
      "[Epoch 3/200] [Batch 268/938] [D loss: 1.102339, acc: 94%] [G loss: 1.196285]\n",
      "[Epoch 3/200] [Batch 269/938] [D loss: 1.090811, acc: 94%] [G loss: 1.141472]\n",
      "[Epoch 3/200] [Batch 270/938] [D loss: 1.097124, acc: 92%] [G loss: 1.183375]\n",
      "[Epoch 3/200] [Batch 271/938] [D loss: 1.090277, acc: 94%] [G loss: 1.106566]\n",
      "[Epoch 3/200] [Batch 272/938] [D loss: 1.115441, acc: 94%] [G loss: 1.192418]\n",
      "[Epoch 3/200] [Batch 273/938] [D loss: 1.068468, acc: 94%] [G loss: 1.176687]\n",
      "[Epoch 3/200] [Batch 274/938] [D loss: 1.101321, acc: 94%] [G loss: 1.105982]\n",
      "[Epoch 3/200] [Batch 275/938] [D loss: 1.085428, acc: 94%] [G loss: 1.193199]\n",
      "[Epoch 3/200] [Batch 276/938] [D loss: 1.074738, acc: 97%] [G loss: 1.189142]\n",
      "[Epoch 3/200] [Batch 277/938] [D loss: 1.093979, acc: 96%] [G loss: 1.162129]\n",
      "[Epoch 3/200] [Batch 278/938] [D loss: 1.087036, acc: 92%] [G loss: 1.131645]\n",
      "[Epoch 3/200] [Batch 279/938] [D loss: 1.079776, acc: 96%] [G loss: 1.140970]\n",
      "[Epoch 3/200] [Batch 280/938] [D loss: 1.071707, acc: 95%] [G loss: 1.124300]\n",
      "[Epoch 3/200] [Batch 281/938] [D loss: 1.033811, acc: 96%] [G loss: 1.166803]\n",
      "[Epoch 3/200] [Batch 282/938] [D loss: 1.078752, acc: 95%] [G loss: 1.104987]\n",
      "[Epoch 3/200] [Batch 283/938] [D loss: 1.109335, acc: 92%] [G loss: 1.134729]\n",
      "[Epoch 3/200] [Batch 284/938] [D loss: 1.052870, acc: 95%] [G loss: 1.186305]\n",
      "[Epoch 3/200] [Batch 285/938] [D loss: 1.090844, acc: 94%] [G loss: 1.198236]\n",
      "[Epoch 3/200] [Batch 286/938] [D loss: 1.055655, acc: 96%] [G loss: 1.186597]\n",
      "[Epoch 3/200] [Batch 287/938] [D loss: 1.067134, acc: 95%] [G loss: 1.188494]\n",
      "[Epoch 3/200] [Batch 288/938] [D loss: 1.112109, acc: 88%] [G loss: 1.098694]\n",
      "[Epoch 3/200] [Batch 289/938] [D loss: 1.068433, acc: 98%] [G loss: 1.164981]\n",
      "[Epoch 3/200] [Batch 290/938] [D loss: 1.085348, acc: 93%] [G loss: 1.182296]\n",
      "[Epoch 3/200] [Batch 291/938] [D loss: 1.088815, acc: 95%] [G loss: 1.199152]\n",
      "[Epoch 3/200] [Batch 292/938] [D loss: 1.165923, acc: 89%] [G loss: 1.153820]\n",
      "[Epoch 3/200] [Batch 293/938] [D loss: 1.085930, acc: 94%] [G loss: 1.125940]\n",
      "[Epoch 3/200] [Batch 294/938] [D loss: 1.072096, acc: 92%] [G loss: 1.152764]\n",
      "[Epoch 3/200] [Batch 295/938] [D loss: 1.109001, acc: 94%] [G loss: 1.137481]\n",
      "[Epoch 3/200] [Batch 296/938] [D loss: 1.066426, acc: 96%] [G loss: 1.235401]\n",
      "[Epoch 3/200] [Batch 297/938] [D loss: 1.089828, acc: 94%] [G loss: 1.237883]\n",
      "[Epoch 3/200] [Batch 298/938] [D loss: 1.108152, acc: 91%] [G loss: 1.216162]\n",
      "[Epoch 3/200] [Batch 299/938] [D loss: 1.063080, acc: 93%] [G loss: 1.080940]\n",
      "[Epoch 3/200] [Batch 300/938] [D loss: 1.098227, acc: 91%] [G loss: 1.144202]\n",
      "[Epoch 3/200] [Batch 301/938] [D loss: 1.077641, acc: 93%] [G loss: 1.219403]\n",
      "[Epoch 3/200] [Batch 302/938] [D loss: 1.074775, acc: 94%] [G loss: 1.207100]\n",
      "[Epoch 3/200] [Batch 303/938] [D loss: 1.060707, acc: 96%] [G loss: 1.156701]\n",
      "[Epoch 3/200] [Batch 304/938] [D loss: 1.067975, acc: 96%] [G loss: 1.126959]\n",
      "[Epoch 3/200] [Batch 305/938] [D loss: 1.082853, acc: 92%] [G loss: 1.202302]\n",
      "[Epoch 3/200] [Batch 306/938] [D loss: 1.069042, acc: 95%] [G loss: 1.172918]\n",
      "[Epoch 3/200] [Batch 307/938] [D loss: 1.072637, acc: 96%] [G loss: 1.199240]\n",
      "[Epoch 3/200] [Batch 308/938] [D loss: 1.115049, acc: 92%] [G loss: 1.164888]\n",
      "[Epoch 3/200] [Batch 309/938] [D loss: 1.103469, acc: 89%] [G loss: 1.141918]\n",
      "[Epoch 3/200] [Batch 310/938] [D loss: 1.107531, acc: 93%] [G loss: 1.098418]\n",
      "[Epoch 3/200] [Batch 311/938] [D loss: 1.118430, acc: 95%] [G loss: 1.185381]\n",
      "[Epoch 3/200] [Batch 312/938] [D loss: 1.065357, acc: 96%] [G loss: 1.206441]\n",
      "[Epoch 3/200] [Batch 313/938] [D loss: 1.096444, acc: 92%] [G loss: 1.142305]\n",
      "[Epoch 3/200] [Batch 314/938] [D loss: 1.062282, acc: 96%] [G loss: 1.162868]\n",
      "[Epoch 3/200] [Batch 315/938] [D loss: 1.109472, acc: 94%] [G loss: 1.165697]\n",
      "[Epoch 3/200] [Batch 316/938] [D loss: 1.092950, acc: 95%] [G loss: 1.081330]\n",
      "[Epoch 3/200] [Batch 317/938] [D loss: 1.098524, acc: 95%] [G loss: 1.165916]\n",
      "[Epoch 3/200] [Batch 318/938] [D loss: 1.081430, acc: 95%] [G loss: 1.169108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 319/938] [D loss: 1.098522, acc: 93%] [G loss: 1.145092]\n",
      "[Epoch 3/200] [Batch 320/938] [D loss: 1.065874, acc: 98%] [G loss: 1.217592]\n",
      "[Epoch 3/200] [Batch 321/938] [D loss: 1.119884, acc: 94%] [G loss: 1.067971]\n",
      "[Epoch 3/200] [Batch 322/938] [D loss: 1.124347, acc: 92%] [G loss: 1.125304]\n",
      "[Epoch 3/200] [Batch 323/938] [D loss: 1.048498, acc: 96%] [G loss: 1.188938]\n",
      "[Epoch 3/200] [Batch 324/938] [D loss: 1.073390, acc: 94%] [G loss: 1.103968]\n",
      "[Epoch 3/200] [Batch 325/938] [D loss: 1.091689, acc: 95%] [G loss: 1.172880]\n",
      "[Epoch 3/200] [Batch 326/938] [D loss: 1.061152, acc: 97%] [G loss: 1.166040]\n",
      "[Epoch 3/200] [Batch 327/938] [D loss: 1.068306, acc: 96%] [G loss: 1.171312]\n",
      "[Epoch 3/200] [Batch 328/938] [D loss: 1.067512, acc: 94%] [G loss: 1.221504]\n",
      "[Epoch 3/200] [Batch 329/938] [D loss: 1.096554, acc: 96%] [G loss: 1.155638]\n",
      "[Epoch 3/200] [Batch 330/938] [D loss: 1.117718, acc: 92%] [G loss: 1.172236]\n",
      "[Epoch 3/200] [Batch 331/938] [D loss: 1.070199, acc: 96%] [G loss: 1.078575]\n",
      "[Epoch 3/200] [Batch 332/938] [D loss: 1.085019, acc: 93%] [G loss: 1.125800]\n",
      "[Epoch 3/200] [Batch 333/938] [D loss: 1.123363, acc: 92%] [G loss: 1.133803]\n",
      "[Epoch 3/200] [Batch 334/938] [D loss: 1.057228, acc: 97%] [G loss: 1.136342]\n",
      "[Epoch 3/200] [Batch 335/938] [D loss: 1.024606, acc: 94%] [G loss: 1.273007]\n",
      "[Epoch 3/200] [Batch 336/938] [D loss: 1.043823, acc: 96%] [G loss: 1.179125]\n",
      "[Epoch 3/200] [Batch 337/938] [D loss: 1.093481, acc: 95%] [G loss: 1.164036]\n",
      "[Epoch 3/200] [Batch 338/938] [D loss: 1.091918, acc: 92%] [G loss: 1.175200]\n",
      "[Epoch 3/200] [Batch 339/938] [D loss: 1.079172, acc: 92%] [G loss: 1.178351]\n",
      "[Epoch 3/200] [Batch 340/938] [D loss: 1.046824, acc: 95%] [G loss: 1.164895]\n",
      "[Epoch 3/200] [Batch 341/938] [D loss: 1.104440, acc: 93%] [G loss: 1.189032]\n",
      "[Epoch 3/200] [Batch 342/938] [D loss: 1.099366, acc: 93%] [G loss: 1.157021]\n",
      "[Epoch 3/200] [Batch 343/938] [D loss: 1.073066, acc: 94%] [G loss: 1.138220]\n",
      "[Epoch 3/200] [Batch 344/938] [D loss: 1.089280, acc: 96%] [G loss: 1.163997]\n",
      "[Epoch 3/200] [Batch 345/938] [D loss: 1.098924, acc: 96%] [G loss: 1.145572]\n",
      "[Epoch 3/200] [Batch 346/938] [D loss: 1.122929, acc: 96%] [G loss: 1.118508]\n",
      "[Epoch 3/200] [Batch 347/938] [D loss: 1.054892, acc: 94%] [G loss: 1.143852]\n",
      "[Epoch 3/200] [Batch 348/938] [D loss: 1.090595, acc: 93%] [G loss: 1.097293]\n",
      "[Epoch 3/200] [Batch 349/938] [D loss: 1.147562, acc: 92%] [G loss: 1.188029]\n",
      "[Epoch 3/200] [Batch 350/938] [D loss: 1.097727, acc: 92%] [G loss: 1.124360]\n",
      "[Epoch 3/200] [Batch 351/938] [D loss: 1.092808, acc: 92%] [G loss: 1.159095]\n",
      "[Epoch 3/200] [Batch 352/938] [D loss: 1.074728, acc: 94%] [G loss: 1.180574]\n",
      "[Epoch 3/200] [Batch 353/938] [D loss: 1.106573, acc: 96%] [G loss: 1.107973]\n",
      "[Epoch 3/200] [Batch 354/938] [D loss: 1.085302, acc: 92%] [G loss: 1.087534]\n",
      "[Epoch 3/200] [Batch 355/938] [D loss: 1.099028, acc: 94%] [G loss: 1.108593]\n",
      "[Epoch 3/200] [Batch 356/938] [D loss: 1.124575, acc: 96%] [G loss: 1.064163]\n",
      "[Epoch 3/200] [Batch 357/938] [D loss: 1.086944, acc: 94%] [G loss: 1.148054]\n",
      "[Epoch 3/200] [Batch 358/938] [D loss: 1.116103, acc: 93%] [G loss: 1.120761]\n",
      "[Epoch 3/200] [Batch 359/938] [D loss: 1.068693, acc: 96%] [G loss: 1.109942]\n",
      "[Epoch 3/200] [Batch 360/938] [D loss: 1.104099, acc: 94%] [G loss: 1.192456]\n",
      "[Epoch 3/200] [Batch 361/938] [D loss: 1.126168, acc: 96%] [G loss: 1.131796]\n",
      "[Epoch 3/200] [Batch 362/938] [D loss: 1.061455, acc: 96%] [G loss: 1.176522]\n",
      "[Epoch 3/200] [Batch 363/938] [D loss: 1.081323, acc: 97%] [G loss: 1.146805]\n",
      "[Epoch 3/200] [Batch 364/938] [D loss: 1.073318, acc: 95%] [G loss: 1.106908]\n",
      "[Epoch 3/200] [Batch 365/938] [D loss: 1.101905, acc: 93%] [G loss: 1.223843]\n",
      "[Epoch 3/200] [Batch 366/938] [D loss: 1.077568, acc: 95%] [G loss: 1.169816]\n",
      "[Epoch 3/200] [Batch 367/938] [D loss: 1.139887, acc: 90%] [G loss: 1.155948]\n",
      "[Epoch 3/200] [Batch 368/938] [D loss: 1.080928, acc: 95%] [G loss: 1.152885]\n",
      "[Epoch 3/200] [Batch 369/938] [D loss: 1.102486, acc: 93%] [G loss: 1.095352]\n",
      "[Epoch 3/200] [Batch 370/938] [D loss: 1.097282, acc: 94%] [G loss: 1.166050]\n",
      "[Epoch 3/200] [Batch 371/938] [D loss: 1.076199, acc: 94%] [G loss: 1.149973]\n",
      "[Epoch 3/200] [Batch 372/938] [D loss: 1.106779, acc: 93%] [G loss: 1.141622]\n",
      "[Epoch 3/200] [Batch 373/938] [D loss: 1.079948, acc: 96%] [G loss: 1.136031]\n",
      "[Epoch 3/200] [Batch 374/938] [D loss: 1.092948, acc: 94%] [G loss: 1.173879]\n",
      "[Epoch 3/200] [Batch 375/938] [D loss: 1.106807, acc: 93%] [G loss: 1.223696]\n",
      "[Epoch 3/200] [Batch 376/938] [D loss: 1.061814, acc: 97%] [G loss: 1.099981]\n",
      "[Epoch 3/200] [Batch 377/938] [D loss: 1.095894, acc: 95%] [G loss: 1.023110]\n",
      "[Epoch 3/200] [Batch 378/938] [D loss: 1.097306, acc: 92%] [G loss: 1.155584]\n",
      "[Epoch 3/200] [Batch 379/938] [D loss: 1.091431, acc: 92%] [G loss: 1.189380]\n",
      "[Epoch 3/200] [Batch 380/938] [D loss: 1.129750, acc: 89%] [G loss: 1.254561]\n",
      "[Epoch 3/200] [Batch 381/938] [D loss: 1.072601, acc: 97%] [G loss: 1.235038]\n",
      "[Epoch 3/200] [Batch 382/938] [D loss: 1.075293, acc: 96%] [G loss: 1.132551]\n",
      "[Epoch 3/200] [Batch 383/938] [D loss: 1.063718, acc: 95%] [G loss: 1.155715]\n",
      "[Epoch 3/200] [Batch 384/938] [D loss: 1.079555, acc: 93%] [G loss: 1.204053]\n",
      "[Epoch 3/200] [Batch 385/938] [D loss: 1.087364, acc: 92%] [G loss: 1.190496]\n",
      "[Epoch 3/200] [Batch 386/938] [D loss: 1.056555, acc: 93%] [G loss: 1.198062]\n",
      "[Epoch 3/200] [Batch 387/938] [D loss: 1.090866, acc: 92%] [G loss: 1.114026]\n",
      "[Epoch 3/200] [Batch 388/938] [D loss: 1.069762, acc: 91%] [G loss: 1.137048]\n",
      "[Epoch 3/200] [Batch 389/938] [D loss: 1.110006, acc: 96%] [G loss: 1.147756]\n",
      "[Epoch 3/200] [Batch 390/938] [D loss: 1.088012, acc: 94%] [G loss: 1.165417]\n",
      "[Epoch 3/200] [Batch 391/938] [D loss: 1.107435, acc: 96%] [G loss: 1.212410]\n",
      "[Epoch 3/200] [Batch 392/938] [D loss: 1.090907, acc: 94%] [G loss: 1.092310]\n",
      "[Epoch 3/200] [Batch 393/938] [D loss: 1.070255, acc: 92%] [G loss: 1.139303]\n",
      "[Epoch 3/200] [Batch 394/938] [D loss: 1.089723, acc: 96%] [G loss: 1.165244]\n",
      "[Epoch 3/200] [Batch 395/938] [D loss: 1.097151, acc: 92%] [G loss: 1.078993]\n",
      "[Epoch 3/200] [Batch 396/938] [D loss: 1.087344, acc: 95%] [G loss: 1.193511]\n",
      "[Epoch 3/200] [Batch 397/938] [D loss: 1.120975, acc: 96%] [G loss: 1.112867]\n",
      "[Epoch 3/200] [Batch 398/938] [D loss: 1.079847, acc: 93%] [G loss: 1.118292]\n",
      "[Epoch 3/200] [Batch 399/938] [D loss: 1.055770, acc: 96%] [G loss: 1.105669]\n",
      "[Epoch 3/200] [Batch 400/938] [D loss: 1.079808, acc: 93%] [G loss: 1.150925]\n",
      "[Epoch 3/200] [Batch 401/938] [D loss: 1.085377, acc: 97%] [G loss: 1.182414]\n",
      "[Epoch 3/200] [Batch 402/938] [D loss: 1.099421, acc: 96%] [G loss: 1.099373]\n",
      "[Epoch 3/200] [Batch 403/938] [D loss: 1.078925, acc: 92%] [G loss: 1.146853]\n",
      "[Epoch 3/200] [Batch 404/938] [D loss: 1.096550, acc: 95%] [G loss: 1.106884]\n",
      "[Epoch 3/200] [Batch 405/938] [D loss: 1.119052, acc: 90%] [G loss: 1.169326]\n",
      "[Epoch 3/200] [Batch 406/938] [D loss: 1.123112, acc: 95%] [G loss: 1.143396]\n",
      "[Epoch 3/200] [Batch 407/938] [D loss: 1.109282, acc: 94%] [G loss: 1.226696]\n",
      "[Epoch 3/200] [Batch 408/938] [D loss: 1.055170, acc: 96%] [G loss: 1.157380]\n",
      "[Epoch 3/200] [Batch 409/938] [D loss: 1.079781, acc: 93%] [G loss: 1.180995]\n",
      "[Epoch 3/200] [Batch 410/938] [D loss: 1.092570, acc: 94%] [G loss: 1.168303]\n",
      "[Epoch 3/200] [Batch 411/938] [D loss: 1.081026, acc: 92%] [G loss: 1.160545]\n",
      "[Epoch 3/200] [Batch 412/938] [D loss: 1.124219, acc: 92%] [G loss: 1.118761]\n",
      "[Epoch 3/200] [Batch 413/938] [D loss: 1.088715, acc: 91%] [G loss: 1.144008]\n",
      "[Epoch 3/200] [Batch 414/938] [D loss: 1.066405, acc: 96%] [G loss: 1.152062]\n",
      "[Epoch 3/200] [Batch 415/938] [D loss: 1.058270, acc: 96%] [G loss: 1.224837]\n",
      "[Epoch 3/200] [Batch 416/938] [D loss: 1.069121, acc: 92%] [G loss: 1.154780]\n",
      "[Epoch 3/200] [Batch 417/938] [D loss: 1.081960, acc: 92%] [G loss: 1.088058]\n",
      "[Epoch 3/200] [Batch 418/938] [D loss: 1.054182, acc: 96%] [G loss: 1.156397]\n",
      "[Epoch 3/200] [Batch 419/938] [D loss: 1.080192, acc: 89%] [G loss: 1.196555]\n",
      "[Epoch 3/200] [Batch 420/938] [D loss: 1.083749, acc: 90%] [G loss: 1.113190]\n",
      "[Epoch 3/200] [Batch 421/938] [D loss: 1.093562, acc: 95%] [G loss: 1.168799]\n",
      "[Epoch 3/200] [Batch 422/938] [D loss: 1.078791, acc: 94%] [G loss: 1.172677]\n",
      "[Epoch 3/200] [Batch 423/938] [D loss: 1.095516, acc: 93%] [G loss: 1.149706]\n",
      "[Epoch 3/200] [Batch 424/938] [D loss: 1.083859, acc: 94%] [G loss: 1.206289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 425/938] [D loss: 1.101528, acc: 96%] [G loss: 1.112240]\n",
      "[Epoch 3/200] [Batch 426/938] [D loss: 1.112256, acc: 92%] [G loss: 1.094425]\n",
      "[Epoch 3/200] [Batch 427/938] [D loss: 1.041872, acc: 100%] [G loss: 1.168994]\n",
      "[Epoch 3/200] [Batch 428/938] [D loss: 1.061279, acc: 93%] [G loss: 1.254724]\n",
      "[Epoch 3/200] [Batch 429/938] [D loss: 1.100939, acc: 97%] [G loss: 1.171062]\n",
      "[Epoch 3/200] [Batch 430/938] [D loss: 1.066240, acc: 93%] [G loss: 1.173293]\n",
      "[Epoch 3/200] [Batch 431/938] [D loss: 1.075953, acc: 95%] [G loss: 1.127591]\n",
      "[Epoch 3/200] [Batch 432/938] [D loss: 1.074695, acc: 96%] [G loss: 1.105700]\n",
      "[Epoch 3/200] [Batch 433/938] [D loss: 1.127645, acc: 96%] [G loss: 1.099224]\n",
      "[Epoch 3/200] [Batch 434/938] [D loss: 1.084182, acc: 92%] [G loss: 1.168123]\n",
      "[Epoch 3/200] [Batch 435/938] [D loss: 1.105605, acc: 92%] [G loss: 1.191677]\n",
      "[Epoch 3/200] [Batch 436/938] [D loss: 1.065906, acc: 92%] [G loss: 1.085853]\n",
      "[Epoch 3/200] [Batch 437/938] [D loss: 1.080837, acc: 96%] [G loss: 1.149167]\n",
      "[Epoch 3/200] [Batch 438/938] [D loss: 1.057882, acc: 96%] [G loss: 1.145211]\n",
      "[Epoch 3/200] [Batch 439/938] [D loss: 1.052183, acc: 97%] [G loss: 1.237923]\n",
      "[Epoch 3/200] [Batch 440/938] [D loss: 1.066206, acc: 92%] [G loss: 1.160971]\n",
      "[Epoch 3/200] [Batch 441/938] [D loss: 1.046575, acc: 92%] [G loss: 1.171084]\n",
      "[Epoch 3/200] [Batch 442/938] [D loss: 1.063010, acc: 97%] [G loss: 1.134357]\n",
      "[Epoch 3/200] [Batch 443/938] [D loss: 1.054106, acc: 96%] [G loss: 1.124701]\n",
      "[Epoch 3/200] [Batch 444/938] [D loss: 1.103082, acc: 92%] [G loss: 1.178669]\n",
      "[Epoch 3/200] [Batch 445/938] [D loss: 1.117917, acc: 96%] [G loss: 1.063927]\n",
      "[Epoch 3/200] [Batch 446/938] [D loss: 1.088323, acc: 94%] [G loss: 1.128904]\n",
      "[Epoch 3/200] [Batch 447/938] [D loss: 1.087308, acc: 96%] [G loss: 1.103750]\n",
      "[Epoch 3/200] [Batch 448/938] [D loss: 1.045449, acc: 96%] [G loss: 1.136834]\n",
      "[Epoch 3/200] [Batch 449/938] [D loss: 1.115700, acc: 93%] [G loss: 1.120162]\n",
      "[Epoch 3/200] [Batch 450/938] [D loss: 1.048484, acc: 97%] [G loss: 1.215272]\n",
      "[Epoch 3/200] [Batch 451/938] [D loss: 1.107000, acc: 92%] [G loss: 1.195905]\n",
      "[Epoch 3/200] [Batch 452/938] [D loss: 1.067626, acc: 95%] [G loss: 1.127161]\n",
      "[Epoch 3/200] [Batch 453/938] [D loss: 1.115886, acc: 95%] [G loss: 1.126237]\n",
      "[Epoch 3/200] [Batch 454/938] [D loss: 1.091126, acc: 95%] [G loss: 1.164398]\n",
      "[Epoch 3/200] [Batch 455/938] [D loss: 1.084297, acc: 93%] [G loss: 1.175922]\n",
      "[Epoch 3/200] [Batch 456/938] [D loss: 1.130593, acc: 92%] [G loss: 1.169994]\n",
      "[Epoch 3/200] [Batch 457/938] [D loss: 1.098509, acc: 89%] [G loss: 1.168926]\n",
      "[Epoch 3/200] [Batch 458/938] [D loss: 1.053848, acc: 94%] [G loss: 1.111928]\n",
      "[Epoch 3/200] [Batch 459/938] [D loss: 1.111829, acc: 93%] [G loss: 1.227096]\n",
      "[Epoch 3/200] [Batch 460/938] [D loss: 1.120006, acc: 90%] [G loss: 1.119385]\n",
      "[Epoch 3/200] [Batch 461/938] [D loss: 1.075010, acc: 91%] [G loss: 1.224447]\n",
      "[Epoch 3/200] [Batch 462/938] [D loss: 1.104623, acc: 92%] [G loss: 1.138296]\n",
      "[Epoch 3/200] [Batch 463/938] [D loss: 1.104145, acc: 93%] [G loss: 1.137928]\n",
      "[Epoch 3/200] [Batch 464/938] [D loss: 1.063917, acc: 95%] [G loss: 1.205746]\n",
      "[Epoch 3/200] [Batch 465/938] [D loss: 1.110497, acc: 92%] [G loss: 1.075669]\n",
      "[Epoch 3/200] [Batch 466/938] [D loss: 1.084378, acc: 91%] [G loss: 1.118857]\n",
      "[Epoch 3/200] [Batch 467/938] [D loss: 1.055556, acc: 96%] [G loss: 1.183432]\n",
      "[Epoch 3/200] [Batch 468/938] [D loss: 1.079430, acc: 96%] [G loss: 1.101644]\n",
      "[Epoch 3/200] [Batch 469/938] [D loss: 1.069120, acc: 96%] [G loss: 1.143091]\n",
      "[Epoch 3/200] [Batch 470/938] [D loss: 1.090373, acc: 94%] [G loss: 1.124724]\n",
      "[Epoch 3/200] [Batch 471/938] [D loss: 1.092782, acc: 94%] [G loss: 1.177742]\n",
      "[Epoch 3/200] [Batch 472/938] [D loss: 1.079718, acc: 93%] [G loss: 1.158579]\n",
      "[Epoch 3/200] [Batch 473/938] [D loss: 1.113624, acc: 90%] [G loss: 1.174579]\n",
      "[Epoch 3/200] [Batch 474/938] [D loss: 1.074440, acc: 97%] [G loss: 1.182639]\n",
      "[Epoch 3/200] [Batch 475/938] [D loss: 1.076341, acc: 95%] [G loss: 1.218261]\n",
      "[Epoch 3/200] [Batch 476/938] [D loss: 1.085226, acc: 94%] [G loss: 1.148621]\n",
      "[Epoch 3/200] [Batch 477/938] [D loss: 1.081255, acc: 94%] [G loss: 1.219889]\n",
      "[Epoch 3/200] [Batch 478/938] [D loss: 1.067841, acc: 95%] [G loss: 1.175649]\n",
      "[Epoch 3/200] [Batch 479/938] [D loss: 1.092655, acc: 93%] [G loss: 1.171295]\n",
      "[Epoch 3/200] [Batch 480/938] [D loss: 1.103756, acc: 94%] [G loss: 1.175307]\n",
      "[Epoch 3/200] [Batch 481/938] [D loss: 1.133973, acc: 96%] [G loss: 1.190149]\n",
      "[Epoch 3/200] [Batch 482/938] [D loss: 1.109172, acc: 93%] [G loss: 1.125672]\n",
      "[Epoch 3/200] [Batch 483/938] [D loss: 1.099888, acc: 92%] [G loss: 1.098246]\n",
      "[Epoch 3/200] [Batch 484/938] [D loss: 1.097254, acc: 94%] [G loss: 1.139347]\n",
      "[Epoch 3/200] [Batch 485/938] [D loss: 1.091091, acc: 92%] [G loss: 1.166070]\n",
      "[Epoch 3/200] [Batch 486/938] [D loss: 1.070606, acc: 93%] [G loss: 1.129041]\n",
      "[Epoch 3/200] [Batch 487/938] [D loss: 1.067111, acc: 95%] [G loss: 1.158627]\n",
      "[Epoch 3/200] [Batch 488/938] [D loss: 1.063704, acc: 94%] [G loss: 1.162108]\n",
      "[Epoch 3/200] [Batch 489/938] [D loss: 1.103633, acc: 96%] [G loss: 1.302205]\n",
      "[Epoch 3/200] [Batch 490/938] [D loss: 1.074311, acc: 94%] [G loss: 1.138649]\n",
      "[Epoch 3/200] [Batch 491/938] [D loss: 1.116833, acc: 96%] [G loss: 1.090020]\n",
      "[Epoch 3/200] [Batch 492/938] [D loss: 1.097060, acc: 93%] [G loss: 1.135830]\n",
      "[Epoch 3/200] [Batch 493/938] [D loss: 1.134268, acc: 91%] [G loss: 1.133126]\n",
      "[Epoch 3/200] [Batch 494/938] [D loss: 1.065808, acc: 96%] [G loss: 1.212534]\n",
      "[Epoch 3/200] [Batch 495/938] [D loss: 1.050315, acc: 97%] [G loss: 1.146830]\n",
      "[Epoch 3/200] [Batch 496/938] [D loss: 1.060286, acc: 96%] [G loss: 1.170092]\n",
      "[Epoch 3/200] [Batch 497/938] [D loss: 1.067150, acc: 90%] [G loss: 1.194888]\n",
      "[Epoch 3/200] [Batch 498/938] [D loss: 1.106251, acc: 93%] [G loss: 1.160786]\n",
      "[Epoch 3/200] [Batch 499/938] [D loss: 1.044706, acc: 96%] [G loss: 1.180809]\n",
      "[Epoch 3/200] [Batch 500/938] [D loss: 1.089700, acc: 95%] [G loss: 1.153815]\n",
      "[Epoch 3/200] [Batch 501/938] [D loss: 1.083806, acc: 94%] [G loss: 1.148366]\n",
      "[Epoch 3/200] [Batch 502/938] [D loss: 1.135013, acc: 90%] [G loss: 1.109382]\n",
      "[Epoch 3/200] [Batch 503/938] [D loss: 1.061875, acc: 96%] [G loss: 1.175573]\n",
      "[Epoch 3/200] [Batch 504/938] [D loss: 1.089052, acc: 94%] [G loss: 1.123789]\n",
      "[Epoch 3/200] [Batch 505/938] [D loss: 1.095730, acc: 94%] [G loss: 1.176147]\n",
      "[Epoch 3/200] [Batch 506/938] [D loss: 1.105923, acc: 96%] [G loss: 1.138696]\n",
      "[Epoch 3/200] [Batch 507/938] [D loss: 1.105168, acc: 91%] [G loss: 1.134682]\n",
      "[Epoch 3/200] [Batch 508/938] [D loss: 1.067732, acc: 96%] [G loss: 1.142397]\n",
      "[Epoch 3/200] [Batch 509/938] [D loss: 1.083243, acc: 92%] [G loss: 1.204415]\n",
      "[Epoch 3/200] [Batch 510/938] [D loss: 1.127876, acc: 92%] [G loss: 1.112488]\n",
      "[Epoch 3/200] [Batch 511/938] [D loss: 1.098536, acc: 93%] [G loss: 1.160500]\n",
      "[Epoch 3/200] [Batch 512/938] [D loss: 1.120394, acc: 94%] [G loss: 1.111025]\n",
      "[Epoch 3/200] [Batch 513/938] [D loss: 1.076856, acc: 97%] [G loss: 1.150370]\n",
      "[Epoch 3/200] [Batch 514/938] [D loss: 1.126779, acc: 94%] [G loss: 1.123334]\n",
      "[Epoch 3/200] [Batch 515/938] [D loss: 1.075685, acc: 94%] [G loss: 1.141020]\n",
      "[Epoch 3/200] [Batch 516/938] [D loss: 1.058698, acc: 96%] [G loss: 1.177634]\n",
      "[Epoch 3/200] [Batch 517/938] [D loss: 1.088447, acc: 96%] [G loss: 1.182252]\n",
      "[Epoch 3/200] [Batch 518/938] [D loss: 1.095434, acc: 92%] [G loss: 1.092682]\n",
      "[Epoch 3/200] [Batch 519/938] [D loss: 1.079562, acc: 93%] [G loss: 1.171243]\n",
      "[Epoch 3/200] [Batch 520/938] [D loss: 1.094131, acc: 94%] [G loss: 1.166119]\n",
      "[Epoch 3/200] [Batch 521/938] [D loss: 1.099948, acc: 92%] [G loss: 1.213590]\n",
      "[Epoch 3/200] [Batch 522/938] [D loss: 1.086282, acc: 97%] [G loss: 1.214571]\n",
      "[Epoch 3/200] [Batch 523/938] [D loss: 1.061291, acc: 92%] [G loss: 1.120354]\n",
      "[Epoch 3/200] [Batch 524/938] [D loss: 1.116881, acc: 94%] [G loss: 1.098531]\n",
      "[Epoch 3/200] [Batch 525/938] [D loss: 1.124844, acc: 96%] [G loss: 1.129030]\n",
      "[Epoch 3/200] [Batch 526/938] [D loss: 1.082719, acc: 92%] [G loss: 1.164538]\n",
      "[Epoch 3/200] [Batch 527/938] [D loss: 1.087135, acc: 98%] [G loss: 1.218023]\n",
      "[Epoch 3/200] [Batch 528/938] [D loss: 1.029190, acc: 98%] [G loss: 1.162590]\n",
      "[Epoch 3/200] [Batch 529/938] [D loss: 1.068750, acc: 95%] [G loss: 1.167229]\n",
      "[Epoch 3/200] [Batch 530/938] [D loss: 1.100155, acc: 96%] [G loss: 1.143950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 531/938] [D loss: 1.072509, acc: 95%] [G loss: 1.162668]\n",
      "[Epoch 3/200] [Batch 532/938] [D loss: 1.103678, acc: 92%] [G loss: 1.148137]\n",
      "[Epoch 3/200] [Batch 533/938] [D loss: 1.037467, acc: 95%] [G loss: 1.113885]\n",
      "[Epoch 3/200] [Batch 534/938] [D loss: 1.114935, acc: 95%] [G loss: 1.134087]\n",
      "[Epoch 3/200] [Batch 535/938] [D loss: 1.106712, acc: 92%] [G loss: 1.151785]\n",
      "[Epoch 3/200] [Batch 536/938] [D loss: 1.092661, acc: 94%] [G loss: 1.181486]\n",
      "[Epoch 3/200] [Batch 537/938] [D loss: 1.118370, acc: 92%] [G loss: 1.160153]\n",
      "[Epoch 3/200] [Batch 538/938] [D loss: 1.101177, acc: 94%] [G loss: 1.086377]\n",
      "[Epoch 3/200] [Batch 539/938] [D loss: 1.078859, acc: 94%] [G loss: 1.172698]\n",
      "[Epoch 3/200] [Batch 540/938] [D loss: 1.106228, acc: 96%] [G loss: 1.108218]\n",
      "[Epoch 3/200] [Batch 541/938] [D loss: 1.072647, acc: 96%] [G loss: 1.186214]\n",
      "[Epoch 3/200] [Batch 542/938] [D loss: 1.123639, acc: 96%] [G loss: 1.181654]\n",
      "[Epoch 3/200] [Batch 543/938] [D loss: 1.079182, acc: 97%] [G loss: 1.125815]\n",
      "[Epoch 3/200] [Batch 544/938] [D loss: 1.101982, acc: 94%] [G loss: 1.170722]\n",
      "[Epoch 3/200] [Batch 545/938] [D loss: 1.075046, acc: 92%] [G loss: 1.097277]\n",
      "[Epoch 3/200] [Batch 546/938] [D loss: 1.065067, acc: 96%] [G loss: 1.204054]\n",
      "[Epoch 3/200] [Batch 547/938] [D loss: 1.041438, acc: 96%] [G loss: 1.228445]\n",
      "[Epoch 3/200] [Batch 548/938] [D loss: 1.065164, acc: 96%] [G loss: 1.181425]\n",
      "[Epoch 3/200] [Batch 549/938] [D loss: 1.132886, acc: 93%] [G loss: 1.110468]\n",
      "[Epoch 3/200] [Batch 550/938] [D loss: 1.116526, acc: 90%] [G loss: 1.149049]\n",
      "[Epoch 3/200] [Batch 551/938] [D loss: 1.094570, acc: 93%] [G loss: 1.153156]\n",
      "[Epoch 3/200] [Batch 552/938] [D loss: 1.091848, acc: 95%] [G loss: 1.112829]\n",
      "[Epoch 3/200] [Batch 553/938] [D loss: 1.093843, acc: 94%] [G loss: 1.117834]\n",
      "[Epoch 3/200] [Batch 554/938] [D loss: 1.088656, acc: 96%] [G loss: 1.108913]\n",
      "[Epoch 3/200] [Batch 555/938] [D loss: 1.084485, acc: 97%] [G loss: 1.143853]\n",
      "[Epoch 3/200] [Batch 556/938] [D loss: 1.053214, acc: 91%] [G loss: 1.222958]\n",
      "[Epoch 3/200] [Batch 557/938] [D loss: 1.047573, acc: 96%] [G loss: 1.130148]\n",
      "[Epoch 3/200] [Batch 558/938] [D loss: 1.064863, acc: 95%] [G loss: 1.136452]\n",
      "[Epoch 3/200] [Batch 559/938] [D loss: 1.060964, acc: 93%] [G loss: 1.176534]\n",
      "[Epoch 3/200] [Batch 560/938] [D loss: 1.113535, acc: 96%] [G loss: 1.162909]\n",
      "[Epoch 3/200] [Batch 561/938] [D loss: 1.090206, acc: 92%] [G loss: 1.120727]\n",
      "[Epoch 3/200] [Batch 562/938] [D loss: 1.066135, acc: 98%] [G loss: 1.059063]\n",
      "[Epoch 3/200] [Batch 563/938] [D loss: 1.080834, acc: 95%] [G loss: 1.140218]\n",
      "[Epoch 3/200] [Batch 564/938] [D loss: 1.077913, acc: 94%] [G loss: 1.136912]\n",
      "[Epoch 3/200] [Batch 565/938] [D loss: 1.070318, acc: 95%] [G loss: 1.080886]\n",
      "[Epoch 3/200] [Batch 566/938] [D loss: 1.090918, acc: 93%] [G loss: 1.225945]\n",
      "[Epoch 3/200] [Batch 567/938] [D loss: 1.076540, acc: 96%] [G loss: 1.162958]\n",
      "[Epoch 3/200] [Batch 568/938] [D loss: 1.039684, acc: 96%] [G loss: 1.177487]\n",
      "[Epoch 3/200] [Batch 569/938] [D loss: 1.064670, acc: 96%] [G loss: 1.136331]\n",
      "[Epoch 3/200] [Batch 570/938] [D loss: 1.066431, acc: 96%] [G loss: 1.116868]\n",
      "[Epoch 3/200] [Batch 571/938] [D loss: 1.083726, acc: 91%] [G loss: 1.205790]\n",
      "[Epoch 3/200] [Batch 572/938] [D loss: 1.077118, acc: 95%] [G loss: 1.140855]\n",
      "[Epoch 3/200] [Batch 573/938] [D loss: 1.078970, acc: 97%] [G loss: 1.155992]\n",
      "[Epoch 3/200] [Batch 574/938] [D loss: 1.025484, acc: 95%] [G loss: 1.188194]\n",
      "[Epoch 3/200] [Batch 575/938] [D loss: 1.094382, acc: 92%] [G loss: 1.273040]\n",
      "[Epoch 3/200] [Batch 576/938] [D loss: 1.103263, acc: 96%] [G loss: 1.096357]\n",
      "[Epoch 3/200] [Batch 577/938] [D loss: 1.102500, acc: 96%] [G loss: 1.174902]\n",
      "[Epoch 3/200] [Batch 578/938] [D loss: 1.110168, acc: 93%] [G loss: 1.108098]\n",
      "[Epoch 3/200] [Batch 579/938] [D loss: 1.074479, acc: 94%] [G loss: 1.124940]\n",
      "[Epoch 3/200] [Batch 580/938] [D loss: 1.112237, acc: 95%] [G loss: 1.150267]\n",
      "[Epoch 3/200] [Batch 581/938] [D loss: 1.102749, acc: 95%] [G loss: 1.103139]\n",
      "[Epoch 3/200] [Batch 582/938] [D loss: 1.101376, acc: 96%] [G loss: 1.175142]\n",
      "[Epoch 3/200] [Batch 583/938] [D loss: 1.095785, acc: 95%] [G loss: 1.147300]\n",
      "[Epoch 3/200] [Batch 584/938] [D loss: 1.106533, acc: 89%] [G loss: 1.129714]\n",
      "[Epoch 3/200] [Batch 585/938] [D loss: 1.074508, acc: 96%] [G loss: 1.144897]\n",
      "[Epoch 3/200] [Batch 586/938] [D loss: 1.078601, acc: 91%] [G loss: 1.089087]\n",
      "[Epoch 3/200] [Batch 587/938] [D loss: 1.091557, acc: 93%] [G loss: 1.153582]\n",
      "[Epoch 3/200] [Batch 588/938] [D loss: 1.067752, acc: 96%] [G loss: 1.195121]\n",
      "[Epoch 3/200] [Batch 589/938] [D loss: 1.086886, acc: 94%] [G loss: 1.153311]\n",
      "[Epoch 3/200] [Batch 590/938] [D loss: 1.090878, acc: 96%] [G loss: 1.120974]\n",
      "[Epoch 3/200] [Batch 591/938] [D loss: 1.049061, acc: 97%] [G loss: 1.128474]\n",
      "[Epoch 3/200] [Batch 592/938] [D loss: 1.083499, acc: 96%] [G loss: 1.069152]\n",
      "[Epoch 3/200] [Batch 593/938] [D loss: 1.066626, acc: 94%] [G loss: 1.190142]\n",
      "[Epoch 3/200] [Batch 594/938] [D loss: 1.106245, acc: 90%] [G loss: 1.242612]\n",
      "[Epoch 3/200] [Batch 595/938] [D loss: 1.097311, acc: 97%] [G loss: 1.102745]\n",
      "[Epoch 3/200] [Batch 596/938] [D loss: 1.088710, acc: 93%] [G loss: 1.136134]\n",
      "[Epoch 3/200] [Batch 597/938] [D loss: 1.143585, acc: 93%] [G loss: 1.104191]\n",
      "[Epoch 3/200] [Batch 598/938] [D loss: 1.059459, acc: 94%] [G loss: 1.138021]\n",
      "[Epoch 3/200] [Batch 599/938] [D loss: 1.105768, acc: 95%] [G loss: 1.096568]\n",
      "[Epoch 3/200] [Batch 600/938] [D loss: 1.111809, acc: 92%] [G loss: 1.173958]\n",
      "[Epoch 3/200] [Batch 601/938] [D loss: 1.108203, acc: 94%] [G loss: 1.111507]\n",
      "[Epoch 3/200] [Batch 602/938] [D loss: 1.101177, acc: 92%] [G loss: 1.072833]\n",
      "[Epoch 3/200] [Batch 603/938] [D loss: 1.083672, acc: 94%] [G loss: 1.108169]\n",
      "[Epoch 3/200] [Batch 604/938] [D loss: 1.083994, acc: 89%] [G loss: 1.270447]\n",
      "[Epoch 3/200] [Batch 605/938] [D loss: 1.097682, acc: 95%] [G loss: 1.176438]\n",
      "[Epoch 3/200] [Batch 606/938] [D loss: 1.092978, acc: 94%] [G loss: 1.189347]\n",
      "[Epoch 3/200] [Batch 607/938] [D loss: 1.111171, acc: 92%] [G loss: 1.127928]\n",
      "[Epoch 3/200] [Batch 608/938] [D loss: 1.056690, acc: 93%] [G loss: 1.179107]\n",
      "[Epoch 3/200] [Batch 609/938] [D loss: 1.102928, acc: 94%] [G loss: 1.159379]\n",
      "[Epoch 3/200] [Batch 610/938] [D loss: 1.126074, acc: 92%] [G loss: 1.125782]\n",
      "[Epoch 3/200] [Batch 611/938] [D loss: 1.083034, acc: 93%] [G loss: 1.120389]\n",
      "[Epoch 3/200] [Batch 612/938] [D loss: 1.090455, acc: 93%] [G loss: 1.055892]\n",
      "[Epoch 3/200] [Batch 613/938] [D loss: 1.018540, acc: 96%] [G loss: 1.160091]\n",
      "[Epoch 3/200] [Batch 614/938] [D loss: 1.104736, acc: 91%] [G loss: 1.186351]\n",
      "[Epoch 3/200] [Batch 615/938] [D loss: 1.092424, acc: 94%] [G loss: 1.120289]\n",
      "[Epoch 3/200] [Batch 616/938] [D loss: 1.053810, acc: 93%] [G loss: 1.127340]\n",
      "[Epoch 3/200] [Batch 617/938] [D loss: 1.087007, acc: 96%] [G loss: 1.116674]\n",
      "[Epoch 3/200] [Batch 618/938] [D loss: 1.073357, acc: 92%] [G loss: 1.211499]\n",
      "[Epoch 3/200] [Batch 619/938] [D loss: 1.058388, acc: 97%] [G loss: 1.161430]\n",
      "[Epoch 3/200] [Batch 620/938] [D loss: 1.123169, acc: 94%] [G loss: 1.172149]\n",
      "[Epoch 3/200] [Batch 621/938] [D loss: 1.062805, acc: 96%] [G loss: 1.253366]\n",
      "[Epoch 3/200] [Batch 622/938] [D loss: 1.076702, acc: 96%] [G loss: 1.136906]\n",
      "[Epoch 3/200] [Batch 623/938] [D loss: 1.055281, acc: 96%] [G loss: 1.131277]\n",
      "[Epoch 3/200] [Batch 624/938] [D loss: 1.082305, acc: 94%] [G loss: 1.160345]\n",
      "[Epoch 3/200] [Batch 625/938] [D loss: 1.099258, acc: 92%] [G loss: 1.103518]\n",
      "[Epoch 3/200] [Batch 626/938] [D loss: 1.119195, acc: 96%] [G loss: 1.159229]\n",
      "[Epoch 3/200] [Batch 627/938] [D loss: 1.115875, acc: 92%] [G loss: 1.143744]\n",
      "[Epoch 3/200] [Batch 628/938] [D loss: 1.100225, acc: 95%] [G loss: 1.164227]\n",
      "[Epoch 3/200] [Batch 629/938] [D loss: 1.134243, acc: 94%] [G loss: 1.124560]\n",
      "[Epoch 3/200] [Batch 630/938] [D loss: 1.137315, acc: 92%] [G loss: 1.139231]\n",
      "[Epoch 3/200] [Batch 631/938] [D loss: 1.060845, acc: 94%] [G loss: 1.241696]\n",
      "[Epoch 3/200] [Batch 632/938] [D loss: 1.060407, acc: 96%] [G loss: 1.150777]\n",
      "[Epoch 3/200] [Batch 633/938] [D loss: 1.079317, acc: 95%] [G loss: 1.186007]\n",
      "[Epoch 3/200] [Batch 634/938] [D loss: 1.101533, acc: 95%] [G loss: 1.125519]\n",
      "[Epoch 3/200] [Batch 635/938] [D loss: 1.073998, acc: 93%] [G loss: 1.110183]\n",
      "[Epoch 3/200] [Batch 636/938] [D loss: 1.085958, acc: 95%] [G loss: 1.121488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 637/938] [D loss: 1.100272, acc: 94%] [G loss: 1.084938]\n",
      "[Epoch 3/200] [Batch 638/938] [D loss: 1.079763, acc: 94%] [G loss: 1.141027]\n",
      "[Epoch 3/200] [Batch 639/938] [D loss: 1.055477, acc: 96%] [G loss: 1.118225]\n",
      "[Epoch 3/200] [Batch 640/938] [D loss: 1.107268, acc: 93%] [G loss: 1.135281]\n",
      "[Epoch 3/200] [Batch 641/938] [D loss: 1.107143, acc: 95%] [G loss: 1.099890]\n",
      "[Epoch 3/200] [Batch 642/938] [D loss: 1.091009, acc: 93%] [G loss: 1.247897]\n",
      "[Epoch 3/200] [Batch 643/938] [D loss: 1.062707, acc: 96%] [G loss: 1.218437]\n",
      "[Epoch 3/200] [Batch 644/938] [D loss: 1.078912, acc: 92%] [G loss: 1.234504]\n",
      "[Epoch 3/200] [Batch 645/938] [D loss: 1.100081, acc: 93%] [G loss: 1.186533]\n",
      "[Epoch 3/200] [Batch 646/938] [D loss: 1.139661, acc: 91%] [G loss: 1.203301]\n",
      "[Epoch 3/200] [Batch 647/938] [D loss: 1.081040, acc: 92%] [G loss: 1.183077]\n",
      "[Epoch 3/200] [Batch 648/938] [D loss: 1.104651, acc: 94%] [G loss: 1.150931]\n",
      "[Epoch 3/200] [Batch 649/938] [D loss: 1.124004, acc: 93%] [G loss: 1.175153]\n",
      "[Epoch 3/200] [Batch 650/938] [D loss: 1.069629, acc: 96%] [G loss: 1.153292]\n",
      "[Epoch 3/200] [Batch 651/938] [D loss: 1.075887, acc: 95%] [G loss: 1.178285]\n",
      "[Epoch 3/200] [Batch 652/938] [D loss: 1.086278, acc: 93%] [G loss: 1.108801]\n",
      "[Epoch 3/200] [Batch 653/938] [D loss: 1.114111, acc: 90%] [G loss: 1.128772]\n",
      "[Epoch 3/200] [Batch 654/938] [D loss: 1.103599, acc: 91%] [G loss: 1.169554]\n",
      "[Epoch 3/200] [Batch 655/938] [D loss: 1.087471, acc: 94%] [G loss: 1.187363]\n",
      "[Epoch 3/200] [Batch 656/938] [D loss: 1.066817, acc: 95%] [G loss: 1.250315]\n",
      "[Epoch 3/200] [Batch 657/938] [D loss: 1.078558, acc: 95%] [G loss: 1.176188]\n",
      "[Epoch 3/200] [Batch 658/938] [D loss: 1.119596, acc: 93%] [G loss: 1.145468]\n",
      "[Epoch 3/200] [Batch 659/938] [D loss: 1.137218, acc: 92%] [G loss: 1.065467]\n",
      "[Epoch 3/200] [Batch 660/938] [D loss: 1.093281, acc: 96%] [G loss: 1.043835]\n",
      "[Epoch 3/200] [Batch 661/938] [D loss: 1.080279, acc: 94%] [G loss: 1.127698]\n",
      "[Epoch 3/200] [Batch 662/938] [D loss: 1.086608, acc: 94%] [G loss: 1.178440]\n",
      "[Epoch 3/200] [Batch 663/938] [D loss: 1.088901, acc: 96%] [G loss: 1.191118]\n",
      "[Epoch 3/200] [Batch 664/938] [D loss: 1.128105, acc: 93%] [G loss: 1.099223]\n",
      "[Epoch 3/200] [Batch 665/938] [D loss: 1.078190, acc: 94%] [G loss: 1.093671]\n",
      "[Epoch 3/200] [Batch 666/938] [D loss: 1.109100, acc: 92%] [G loss: 1.178720]\n",
      "[Epoch 3/200] [Batch 667/938] [D loss: 1.058200, acc: 94%] [G loss: 1.143932]\n",
      "[Epoch 3/200] [Batch 668/938] [D loss: 1.101434, acc: 93%] [G loss: 1.195148]\n",
      "[Epoch 3/200] [Batch 669/938] [D loss: 1.087499, acc: 92%] [G loss: 1.141684]\n",
      "[Epoch 3/200] [Batch 670/938] [D loss: 1.093088, acc: 93%] [G loss: 1.152310]\n",
      "[Epoch 3/200] [Batch 671/938] [D loss: 1.068815, acc: 97%] [G loss: 1.176488]\n",
      "[Epoch 3/200] [Batch 672/938] [D loss: 1.074179, acc: 97%] [G loss: 1.184518]\n",
      "[Epoch 3/200] [Batch 673/938] [D loss: 1.103312, acc: 94%] [G loss: 1.194209]\n",
      "[Epoch 3/200] [Batch 674/938] [D loss: 1.062074, acc: 97%] [G loss: 1.200993]\n",
      "[Epoch 3/200] [Batch 675/938] [D loss: 1.107754, acc: 92%] [G loss: 1.148431]\n",
      "[Epoch 3/200] [Batch 676/938] [D loss: 1.118520, acc: 97%] [G loss: 1.045906]\n",
      "[Epoch 3/200] [Batch 677/938] [D loss: 1.053476, acc: 93%] [G loss: 1.081143]\n",
      "[Epoch 3/200] [Batch 678/938] [D loss: 1.090494, acc: 95%] [G loss: 1.116796]\n",
      "[Epoch 3/200] [Batch 679/938] [D loss: 1.084137, acc: 94%] [G loss: 1.177518]\n",
      "[Epoch 3/200] [Batch 680/938] [D loss: 1.068824, acc: 93%] [G loss: 1.149856]\n",
      "[Epoch 3/200] [Batch 681/938] [D loss: 1.076910, acc: 93%] [G loss: 1.142225]\n",
      "[Epoch 3/200] [Batch 682/938] [D loss: 1.094865, acc: 96%] [G loss: 1.196652]\n",
      "[Epoch 3/200] [Batch 683/938] [D loss: 1.072764, acc: 93%] [G loss: 1.104016]\n",
      "[Epoch 3/200] [Batch 684/938] [D loss: 1.095285, acc: 95%] [G loss: 1.100342]\n",
      "[Epoch 3/200] [Batch 685/938] [D loss: 1.069685, acc: 94%] [G loss: 1.191746]\n",
      "[Epoch 3/200] [Batch 686/938] [D loss: 1.103871, acc: 93%] [G loss: 1.228759]\n",
      "[Epoch 3/200] [Batch 687/938] [D loss: 1.080121, acc: 97%] [G loss: 1.179339]\n",
      "[Epoch 3/200] [Batch 688/938] [D loss: 1.110743, acc: 96%] [G loss: 1.176545]\n",
      "[Epoch 3/200] [Batch 689/938] [D loss: 1.079887, acc: 96%] [G loss: 1.225835]\n",
      "[Epoch 3/200] [Batch 690/938] [D loss: 1.120579, acc: 95%] [G loss: 1.159226]\n",
      "[Epoch 3/200] [Batch 691/938] [D loss: 1.077195, acc: 98%] [G loss: 1.150319]\n",
      "[Epoch 3/200] [Batch 692/938] [D loss: 1.058958, acc: 97%] [G loss: 1.076297]\n",
      "[Epoch 3/200] [Batch 693/938] [D loss: 1.110454, acc: 93%] [G loss: 1.141081]\n",
      "[Epoch 3/200] [Batch 694/938] [D loss: 1.088348, acc: 95%] [G loss: 1.090647]\n",
      "[Epoch 3/200] [Batch 695/938] [D loss: 1.073630, acc: 94%] [G loss: 1.114497]\n",
      "[Epoch 3/200] [Batch 696/938] [D loss: 1.079449, acc: 96%] [G loss: 1.185469]\n",
      "[Epoch 3/200] [Batch 697/938] [D loss: 1.054020, acc: 96%] [G loss: 1.175130]\n",
      "[Epoch 3/200] [Batch 698/938] [D loss: 1.129765, acc: 92%] [G loss: 1.118031]\n",
      "[Epoch 3/200] [Batch 699/938] [D loss: 1.111107, acc: 94%] [G loss: 1.156321]\n",
      "[Epoch 3/200] [Batch 700/938] [D loss: 1.089191, acc: 91%] [G loss: 1.196737]\n",
      "[Epoch 3/200] [Batch 701/938] [D loss: 1.069054, acc: 96%] [G loss: 1.090146]\n",
      "[Epoch 3/200] [Batch 702/938] [D loss: 1.115595, acc: 96%] [G loss: 1.206267]\n",
      "[Epoch 3/200] [Batch 703/938] [D loss: 1.073620, acc: 93%] [G loss: 1.176906]\n",
      "[Epoch 3/200] [Batch 704/938] [D loss: 1.110342, acc: 95%] [G loss: 1.139920]\n",
      "[Epoch 3/200] [Batch 705/938] [D loss: 1.090907, acc: 96%] [G loss: 1.070360]\n",
      "[Epoch 3/200] [Batch 706/938] [D loss: 1.061208, acc: 95%] [G loss: 1.140312]\n",
      "[Epoch 3/200] [Batch 707/938] [D loss: 1.094754, acc: 96%] [G loss: 1.102897]\n",
      "[Epoch 3/200] [Batch 708/938] [D loss: 1.096538, acc: 96%] [G loss: 1.100118]\n",
      "[Epoch 3/200] [Batch 709/938] [D loss: 1.100586, acc: 95%] [G loss: 1.133934]\n",
      "[Epoch 3/200] [Batch 710/938] [D loss: 1.089410, acc: 94%] [G loss: 1.156926]\n",
      "[Epoch 3/200] [Batch 711/938] [D loss: 1.072604, acc: 96%] [G loss: 1.155982]\n",
      "[Epoch 3/200] [Batch 712/938] [D loss: 1.104030, acc: 96%] [G loss: 1.042866]\n",
      "[Epoch 3/200] [Batch 713/938] [D loss: 1.091805, acc: 97%] [G loss: 1.138625]\n",
      "[Epoch 3/200] [Batch 714/938] [D loss: 1.066295, acc: 93%] [G loss: 1.164748]\n",
      "[Epoch 3/200] [Batch 715/938] [D loss: 1.091953, acc: 96%] [G loss: 1.145216]\n",
      "[Epoch 3/200] [Batch 716/938] [D loss: 1.101128, acc: 99%] [G loss: 1.151597]\n",
      "[Epoch 3/200] [Batch 717/938] [D loss: 1.069958, acc: 96%] [G loss: 1.194517]\n",
      "[Epoch 3/200] [Batch 718/938] [D loss: 1.112266, acc: 94%] [G loss: 1.169321]\n",
      "[Epoch 3/200] [Batch 719/938] [D loss: 1.047070, acc: 96%] [G loss: 1.103449]\n",
      "[Epoch 3/200] [Batch 720/938] [D loss: 1.094425, acc: 94%] [G loss: 1.161353]\n",
      "[Epoch 3/200] [Batch 721/938] [D loss: 1.052497, acc: 96%] [G loss: 1.188090]\n",
      "[Epoch 3/200] [Batch 722/938] [D loss: 1.107530, acc: 93%] [G loss: 1.135087]\n",
      "[Epoch 3/200] [Batch 723/938] [D loss: 1.078831, acc: 95%] [G loss: 1.124948]\n",
      "[Epoch 3/200] [Batch 724/938] [D loss: 1.050989, acc: 93%] [G loss: 1.157541]\n",
      "[Epoch 3/200] [Batch 725/938] [D loss: 1.085383, acc: 97%] [G loss: 1.119883]\n",
      "[Epoch 3/200] [Batch 726/938] [D loss: 1.103666, acc: 91%] [G loss: 1.135111]\n",
      "[Epoch 3/200] [Batch 727/938] [D loss: 1.079116, acc: 96%] [G loss: 1.177064]\n",
      "[Epoch 3/200] [Batch 728/938] [D loss: 1.075095, acc: 90%] [G loss: 1.206703]\n",
      "[Epoch 3/200] [Batch 729/938] [D loss: 1.109473, acc: 96%] [G loss: 1.183554]\n",
      "[Epoch 3/200] [Batch 730/938] [D loss: 1.101593, acc: 96%] [G loss: 1.221763]\n",
      "[Epoch 3/200] [Batch 731/938] [D loss: 1.069514, acc: 99%] [G loss: 1.105066]\n",
      "[Epoch 3/200] [Batch 732/938] [D loss: 1.086771, acc: 93%] [G loss: 1.072626]\n",
      "[Epoch 3/200] [Batch 733/938] [D loss: 1.076858, acc: 93%] [G loss: 1.104652]\n",
      "[Epoch 3/200] [Batch 734/938] [D loss: 1.089128, acc: 93%] [G loss: 1.082825]\n",
      "[Epoch 3/200] [Batch 735/938] [D loss: 1.075434, acc: 94%] [G loss: 1.128331]\n",
      "[Epoch 3/200] [Batch 736/938] [D loss: 1.098937, acc: 95%] [G loss: 1.104964]\n",
      "[Epoch 3/200] [Batch 737/938] [D loss: 1.095916, acc: 94%] [G loss: 1.204514]\n",
      "[Epoch 3/200] [Batch 738/938] [D loss: 1.089537, acc: 93%] [G loss: 1.208692]\n",
      "[Epoch 3/200] [Batch 739/938] [D loss: 1.087027, acc: 98%] [G loss: 1.094560]\n",
      "[Epoch 3/200] [Batch 740/938] [D loss: 1.076902, acc: 94%] [G loss: 1.158558]\n",
      "[Epoch 3/200] [Batch 741/938] [D loss: 1.071676, acc: 93%] [G loss: 1.169574]\n",
      "[Epoch 3/200] [Batch 742/938] [D loss: 1.100773, acc: 92%] [G loss: 1.106303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 743/938] [D loss: 1.062413, acc: 96%] [G loss: 1.156876]\n",
      "[Epoch 3/200] [Batch 744/938] [D loss: 1.076898, acc: 93%] [G loss: 1.168446]\n",
      "[Epoch 3/200] [Batch 745/938] [D loss: 1.104719, acc: 92%] [G loss: 1.188348]\n",
      "[Epoch 3/200] [Batch 746/938] [D loss: 1.056970, acc: 96%] [G loss: 1.087587]\n",
      "[Epoch 3/200] [Batch 747/938] [D loss: 1.092535, acc: 93%] [G loss: 1.130273]\n",
      "[Epoch 3/200] [Batch 748/938] [D loss: 1.059979, acc: 96%] [G loss: 1.151369]\n",
      "[Epoch 3/200] [Batch 749/938] [D loss: 1.061355, acc: 94%] [G loss: 1.164321]\n",
      "[Epoch 3/200] [Batch 750/938] [D loss: 1.057740, acc: 93%] [G loss: 1.178787]\n",
      "[Epoch 3/200] [Batch 751/938] [D loss: 1.117828, acc: 95%] [G loss: 1.154780]\n",
      "[Epoch 3/200] [Batch 752/938] [D loss: 1.101167, acc: 95%] [G loss: 1.130159]\n",
      "[Epoch 3/200] [Batch 753/938] [D loss: 1.093781, acc: 96%] [G loss: 1.112145]\n",
      "[Epoch 3/200] [Batch 754/938] [D loss: 1.085533, acc: 96%] [G loss: 1.147693]\n",
      "[Epoch 3/200] [Batch 755/938] [D loss: 1.053780, acc: 97%] [G loss: 1.175237]\n",
      "[Epoch 3/200] [Batch 756/938] [D loss: 1.081135, acc: 95%] [G loss: 1.162237]\n",
      "[Epoch 3/200] [Batch 757/938] [D loss: 1.090440, acc: 96%] [G loss: 1.179157]\n",
      "[Epoch 3/200] [Batch 758/938] [D loss: 1.078153, acc: 93%] [G loss: 1.181586]\n",
      "[Epoch 3/200] [Batch 759/938] [D loss: 1.090382, acc: 97%] [G loss: 1.148910]\n",
      "[Epoch 3/200] [Batch 760/938] [D loss: 1.074041, acc: 94%] [G loss: 1.094574]\n",
      "[Epoch 3/200] [Batch 761/938] [D loss: 1.049732, acc: 92%] [G loss: 1.205179]\n",
      "[Epoch 3/200] [Batch 762/938] [D loss: 1.129778, acc: 94%] [G loss: 1.151379]\n",
      "[Epoch 3/200] [Batch 763/938] [D loss: 1.092615, acc: 92%] [G loss: 1.213050]\n",
      "[Epoch 3/200] [Batch 764/938] [D loss: 1.073426, acc: 95%] [G loss: 1.148912]\n",
      "[Epoch 3/200] [Batch 765/938] [D loss: 1.079915, acc: 94%] [G loss: 1.102739]\n",
      "[Epoch 3/200] [Batch 766/938] [D loss: 1.088293, acc: 92%] [G loss: 1.212483]\n",
      "[Epoch 3/200] [Batch 767/938] [D loss: 1.102202, acc: 96%] [G loss: 1.176451]\n",
      "[Epoch 3/200] [Batch 768/938] [D loss: 1.067449, acc: 94%] [G loss: 1.131547]\n",
      "[Epoch 3/200] [Batch 769/938] [D loss: 1.111313, acc: 94%] [G loss: 1.120564]\n",
      "[Epoch 3/200] [Batch 770/938] [D loss: 1.125881, acc: 92%] [G loss: 1.098384]\n",
      "[Epoch 3/200] [Batch 771/938] [D loss: 1.066994, acc: 96%] [G loss: 1.127049]\n",
      "[Epoch 3/200] [Batch 772/938] [D loss: 1.083058, acc: 94%] [G loss: 1.136704]\n",
      "[Epoch 3/200] [Batch 773/938] [D loss: 1.103301, acc: 94%] [G loss: 1.154298]\n",
      "[Epoch 3/200] [Batch 774/938] [D loss: 1.097921, acc: 94%] [G loss: 1.174729]\n",
      "[Epoch 3/200] [Batch 775/938] [D loss: 1.093780, acc: 90%] [G loss: 1.117705]\n",
      "[Epoch 3/200] [Batch 776/938] [D loss: 1.065848, acc: 96%] [G loss: 1.190058]\n",
      "[Epoch 3/200] [Batch 777/938] [D loss: 1.077849, acc: 96%] [G loss: 1.104655]\n",
      "[Epoch 3/200] [Batch 778/938] [D loss: 1.079081, acc: 94%] [G loss: 1.105682]\n",
      "[Epoch 3/200] [Batch 779/938] [D loss: 1.086479, acc: 94%] [G loss: 1.086166]\n",
      "[Epoch 3/200] [Batch 780/938] [D loss: 1.115210, acc: 89%] [G loss: 1.117041]\n",
      "[Epoch 3/200] [Batch 781/938] [D loss: 1.090742, acc: 96%] [G loss: 1.121693]\n",
      "[Epoch 3/200] [Batch 782/938] [D loss: 1.043639, acc: 96%] [G loss: 1.160258]\n",
      "[Epoch 3/200] [Batch 783/938] [D loss: 1.065416, acc: 96%] [G loss: 1.102484]\n",
      "[Epoch 3/200] [Batch 784/938] [D loss: 1.101916, acc: 92%] [G loss: 1.087155]\n",
      "[Epoch 3/200] [Batch 785/938] [D loss: 1.084617, acc: 96%] [G loss: 1.156924]\n",
      "[Epoch 3/200] [Batch 786/938] [D loss: 1.084190, acc: 96%] [G loss: 1.216403]\n",
      "[Epoch 3/200] [Batch 787/938] [D loss: 1.089430, acc: 96%] [G loss: 1.164840]\n",
      "[Epoch 3/200] [Batch 788/938] [D loss: 1.062063, acc: 94%] [G loss: 1.239903]\n",
      "[Epoch 3/200] [Batch 789/938] [D loss: 1.092314, acc: 96%] [G loss: 1.127438]\n",
      "[Epoch 3/200] [Batch 790/938] [D loss: 1.085658, acc: 96%] [G loss: 1.166348]\n",
      "[Epoch 3/200] [Batch 791/938] [D loss: 1.101901, acc: 92%] [G loss: 1.122914]\n",
      "[Epoch 3/200] [Batch 792/938] [D loss: 1.135261, acc: 92%] [G loss: 1.130424]\n",
      "[Epoch 3/200] [Batch 793/938] [D loss: 1.093950, acc: 96%] [G loss: 1.159751]\n",
      "[Epoch 3/200] [Batch 794/938] [D loss: 1.115214, acc: 95%] [G loss: 1.220601]\n",
      "[Epoch 3/200] [Batch 795/938] [D loss: 1.059913, acc: 96%] [G loss: 1.120030]\n",
      "[Epoch 3/200] [Batch 796/938] [D loss: 1.137349, acc: 92%] [G loss: 1.097120]\n",
      "[Epoch 3/200] [Batch 797/938] [D loss: 1.075511, acc: 94%] [G loss: 1.125816]\n",
      "[Epoch 3/200] [Batch 798/938] [D loss: 1.088835, acc: 96%] [G loss: 1.182068]\n",
      "[Epoch 3/200] [Batch 799/938] [D loss: 1.075627, acc: 96%] [G loss: 1.157140]\n",
      "[Epoch 3/200] [Batch 800/938] [D loss: 1.102532, acc: 94%] [G loss: 1.244413]\n",
      "[Epoch 3/200] [Batch 801/938] [D loss: 1.068400, acc: 93%] [G loss: 1.264190]\n",
      "[Epoch 3/200] [Batch 802/938] [D loss: 1.102177, acc: 95%] [G loss: 1.170765]\n",
      "[Epoch 3/200] [Batch 803/938] [D loss: 1.089893, acc: 91%] [G loss: 1.134341]\n",
      "[Epoch 3/200] [Batch 804/938] [D loss: 1.113434, acc: 94%] [G loss: 1.162918]\n",
      "[Epoch 3/200] [Batch 805/938] [D loss: 1.102719, acc: 95%] [G loss: 1.096277]\n",
      "[Epoch 3/200] [Batch 806/938] [D loss: 1.103831, acc: 95%] [G loss: 1.135707]\n",
      "[Epoch 3/200] [Batch 807/938] [D loss: 1.103846, acc: 92%] [G loss: 1.170418]\n",
      "[Epoch 3/200] [Batch 808/938] [D loss: 1.124316, acc: 94%] [G loss: 1.132936]\n",
      "[Epoch 3/200] [Batch 809/938] [D loss: 1.106665, acc: 94%] [G loss: 1.212366]\n",
      "[Epoch 3/200] [Batch 810/938] [D loss: 1.103092, acc: 92%] [G loss: 1.216814]\n",
      "[Epoch 3/200] [Batch 811/938] [D loss: 1.053806, acc: 96%] [G loss: 1.169463]\n",
      "[Epoch 3/200] [Batch 812/938] [D loss: 1.050143, acc: 96%] [G loss: 1.128362]\n",
      "[Epoch 3/200] [Batch 813/938] [D loss: 1.125884, acc: 93%] [G loss: 1.165998]\n",
      "[Epoch 3/200] [Batch 814/938] [D loss: 1.063418, acc: 95%] [G loss: 1.169039]\n",
      "[Epoch 3/200] [Batch 815/938] [D loss: 1.120243, acc: 93%] [G loss: 1.141097]\n",
      "[Epoch 3/200] [Batch 816/938] [D loss: 1.102179, acc: 93%] [G loss: 1.153365]\n",
      "[Epoch 3/200] [Batch 817/938] [D loss: 1.080670, acc: 92%] [G loss: 1.113591]\n",
      "[Epoch 3/200] [Batch 818/938] [D loss: 1.074511, acc: 93%] [G loss: 1.085194]\n",
      "[Epoch 3/200] [Batch 819/938] [D loss: 1.089073, acc: 92%] [G loss: 1.117022]\n",
      "[Epoch 3/200] [Batch 820/938] [D loss: 1.091519, acc: 96%] [G loss: 1.128699]\n",
      "[Epoch 3/200] [Batch 821/938] [D loss: 1.098120, acc: 92%] [G loss: 1.136681]\n",
      "[Epoch 3/200] [Batch 822/938] [D loss: 1.096282, acc: 92%] [G loss: 1.170510]\n",
      "[Epoch 3/200] [Batch 823/938] [D loss: 1.111494, acc: 94%] [G loss: 1.156287]\n",
      "[Epoch 3/200] [Batch 824/938] [D loss: 1.084776, acc: 96%] [G loss: 1.132396]\n",
      "[Epoch 3/200] [Batch 825/938] [D loss: 1.104261, acc: 89%] [G loss: 1.210802]\n",
      "[Epoch 3/200] [Batch 826/938] [D loss: 1.056291, acc: 97%] [G loss: 1.162887]\n",
      "[Epoch 3/200] [Batch 827/938] [D loss: 1.043518, acc: 97%] [G loss: 1.206225]\n",
      "[Epoch 3/200] [Batch 828/938] [D loss: 1.076401, acc: 98%] [G loss: 1.101698]\n",
      "[Epoch 3/200] [Batch 829/938] [D loss: 1.093181, acc: 94%] [G loss: 1.133535]\n",
      "[Epoch 3/200] [Batch 830/938] [D loss: 1.133646, acc: 96%] [G loss: 1.038184]\n",
      "[Epoch 3/200] [Batch 831/938] [D loss: 1.075192, acc: 92%] [G loss: 1.114679]\n",
      "[Epoch 3/200] [Batch 832/938] [D loss: 1.073783, acc: 95%] [G loss: 1.141890]\n",
      "[Epoch 3/200] [Batch 833/938] [D loss: 1.053874, acc: 96%] [G loss: 1.162198]\n",
      "[Epoch 3/200] [Batch 834/938] [D loss: 1.037898, acc: 97%] [G loss: 1.140411]\n",
      "[Epoch 3/200] [Batch 835/938] [D loss: 1.097687, acc: 96%] [G loss: 1.092296]\n",
      "[Epoch 3/200] [Batch 836/938] [D loss: 1.090565, acc: 96%] [G loss: 1.078136]\n",
      "[Epoch 3/200] [Batch 837/938] [D loss: 1.103249, acc: 96%] [G loss: 1.084777]\n",
      "[Epoch 3/200] [Batch 838/938] [D loss: 1.051901, acc: 93%] [G loss: 1.205223]\n",
      "[Epoch 3/200] [Batch 839/938] [D loss: 1.104479, acc: 93%] [G loss: 1.161985]\n",
      "[Epoch 3/200] [Batch 840/938] [D loss: 1.098829, acc: 93%] [G loss: 1.139324]\n",
      "[Epoch 3/200] [Batch 841/938] [D loss: 1.095578, acc: 93%] [G loss: 1.101813]\n",
      "[Epoch 3/200] [Batch 842/938] [D loss: 1.104248, acc: 93%] [G loss: 1.121314]\n",
      "[Epoch 3/200] [Batch 843/938] [D loss: 1.081808, acc: 95%] [G loss: 1.185961]\n",
      "[Epoch 3/200] [Batch 844/938] [D loss: 1.066849, acc: 92%] [G loss: 1.210072]\n",
      "[Epoch 3/200] [Batch 845/938] [D loss: 1.132226, acc: 94%] [G loss: 1.122842]\n",
      "[Epoch 3/200] [Batch 846/938] [D loss: 1.080861, acc: 93%] [G loss: 1.090664]\n",
      "[Epoch 3/200] [Batch 847/938] [D loss: 1.095244, acc: 98%] [G loss: 1.056786]\n",
      "[Epoch 3/200] [Batch 848/938] [D loss: 1.046722, acc: 94%] [G loss: 1.120377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/200] [Batch 849/938] [D loss: 1.053841, acc: 95%] [G loss: 1.109937]\n",
      "[Epoch 3/200] [Batch 850/938] [D loss: 1.093114, acc: 96%] [G loss: 1.145603]\n",
      "[Epoch 3/200] [Batch 851/938] [D loss: 1.062429, acc: 95%] [G loss: 1.229333]\n",
      "[Epoch 3/200] [Batch 852/938] [D loss: 1.084542, acc: 96%] [G loss: 1.162672]\n",
      "[Epoch 3/200] [Batch 853/938] [D loss: 1.087257, acc: 89%] [G loss: 1.196904]\n",
      "[Epoch 3/200] [Batch 854/938] [D loss: 1.058009, acc: 93%] [G loss: 1.177096]\n",
      "[Epoch 3/200] [Batch 855/938] [D loss: 1.074641, acc: 93%] [G loss: 1.200356]\n",
      "[Epoch 3/200] [Batch 856/938] [D loss: 1.100564, acc: 92%] [G loss: 1.123554]\n",
      "[Epoch 3/200] [Batch 857/938] [D loss: 1.086806, acc: 95%] [G loss: 1.086080]\n",
      "[Epoch 3/200] [Batch 858/938] [D loss: 1.096856, acc: 93%] [G loss: 1.175517]\n",
      "[Epoch 3/200] [Batch 859/938] [D loss: 1.093374, acc: 93%] [G loss: 1.196726]\n",
      "[Epoch 3/200] [Batch 860/938] [D loss: 1.096534, acc: 96%] [G loss: 1.220770]\n",
      "[Epoch 3/200] [Batch 861/938] [D loss: 1.103612, acc: 96%] [G loss: 1.214602]\n",
      "[Epoch 3/200] [Batch 862/938] [D loss: 1.120157, acc: 94%] [G loss: 1.128796]\n",
      "[Epoch 3/200] [Batch 863/938] [D loss: 1.055009, acc: 96%] [G loss: 1.099834]\n",
      "[Epoch 3/200] [Batch 864/938] [D loss: 1.078074, acc: 94%] [G loss: 1.111333]\n",
      "[Epoch 3/200] [Batch 865/938] [D loss: 1.114486, acc: 88%] [G loss: 1.170748]\n",
      "[Epoch 3/200] [Batch 866/938] [D loss: 1.076454, acc: 96%] [G loss: 1.179349]\n",
      "[Epoch 3/200] [Batch 867/938] [D loss: 1.135662, acc: 92%] [G loss: 1.083260]\n",
      "[Epoch 3/200] [Batch 868/938] [D loss: 1.085393, acc: 94%] [G loss: 1.079995]\n",
      "[Epoch 3/200] [Batch 869/938] [D loss: 1.084696, acc: 96%] [G loss: 1.285551]\n",
      "[Epoch 3/200] [Batch 870/938] [D loss: 1.063065, acc: 95%] [G loss: 1.147049]\n",
      "[Epoch 3/200] [Batch 871/938] [D loss: 1.056707, acc: 95%] [G loss: 1.233694]\n",
      "[Epoch 3/200] [Batch 872/938] [D loss: 1.100994, acc: 92%] [G loss: 1.203094]\n",
      "[Epoch 3/200] [Batch 873/938] [D loss: 1.125594, acc: 92%] [G loss: 1.074573]\n",
      "[Epoch 3/200] [Batch 874/938] [D loss: 1.090435, acc: 97%] [G loss: 1.056958]\n",
      "[Epoch 3/200] [Batch 875/938] [D loss: 1.119906, acc: 93%] [G loss: 1.045810]\n",
      "[Epoch 3/200] [Batch 876/938] [D loss: 1.087322, acc: 95%] [G loss: 1.131459]\n",
      "[Epoch 3/200] [Batch 877/938] [D loss: 1.087794, acc: 94%] [G loss: 1.168794]\n",
      "[Epoch 3/200] [Batch 878/938] [D loss: 1.071923, acc: 94%] [G loss: 1.184029]\n",
      "[Epoch 3/200] [Batch 879/938] [D loss: 1.089732, acc: 96%] [G loss: 1.165262]\n",
      "[Epoch 3/200] [Batch 880/938] [D loss: 1.071664, acc: 96%] [G loss: 1.126444]\n",
      "[Epoch 3/200] [Batch 881/938] [D loss: 1.088938, acc: 94%] [G loss: 1.172196]\n",
      "[Epoch 3/200] [Batch 882/938] [D loss: 1.069965, acc: 92%] [G loss: 1.104540]\n",
      "[Epoch 3/200] [Batch 883/938] [D loss: 1.119332, acc: 87%] [G loss: 1.168497]\n",
      "[Epoch 3/200] [Batch 884/938] [D loss: 1.092642, acc: 93%] [G loss: 1.118499]\n",
      "[Epoch 3/200] [Batch 885/938] [D loss: 1.072487, acc: 94%] [G loss: 1.081728]\n",
      "[Epoch 3/200] [Batch 886/938] [D loss: 1.103203, acc: 93%] [G loss: 1.149417]\n",
      "[Epoch 3/200] [Batch 887/938] [D loss: 1.116135, acc: 95%] [G loss: 1.114308]\n",
      "[Epoch 3/200] [Batch 888/938] [D loss: 1.081342, acc: 96%] [G loss: 1.138977]\n",
      "[Epoch 3/200] [Batch 889/938] [D loss: 1.077793, acc: 97%] [G loss: 1.248456]\n",
      "[Epoch 3/200] [Batch 890/938] [D loss: 1.066520, acc: 92%] [G loss: 1.158278]\n",
      "[Epoch 3/200] [Batch 891/938] [D loss: 1.068893, acc: 95%] [G loss: 1.176710]\n",
      "[Epoch 3/200] [Batch 892/938] [D loss: 1.088848, acc: 96%] [G loss: 1.170014]\n",
      "[Epoch 3/200] [Batch 893/938] [D loss: 1.097577, acc: 93%] [G loss: 1.183637]\n",
      "[Epoch 3/200] [Batch 894/938] [D loss: 1.060370, acc: 95%] [G loss: 1.225248]\n",
      "[Epoch 3/200] [Batch 895/938] [D loss: 1.061765, acc: 96%] [G loss: 1.179663]\n",
      "[Epoch 3/200] [Batch 896/938] [D loss: 1.039018, acc: 96%] [G loss: 1.143999]\n",
      "[Epoch 3/200] [Batch 897/938] [D loss: 1.069462, acc: 96%] [G loss: 1.099405]\n",
      "[Epoch 3/200] [Batch 898/938] [D loss: 1.079502, acc: 93%] [G loss: 1.120039]\n",
      "[Epoch 3/200] [Batch 899/938] [D loss: 1.092014, acc: 94%] [G loss: 1.094846]\n",
      "[Epoch 3/200] [Batch 900/938] [D loss: 1.099238, acc: 95%] [G loss: 1.181373]\n",
      "[Epoch 3/200] [Batch 901/938] [D loss: 1.086333, acc: 93%] [G loss: 1.179271]\n",
      "[Epoch 3/200] [Batch 902/938] [D loss: 1.085573, acc: 95%] [G loss: 1.162957]\n",
      "[Epoch 3/200] [Batch 903/938] [D loss: 1.094287, acc: 93%] [G loss: 1.188568]\n",
      "[Epoch 3/200] [Batch 904/938] [D loss: 1.093576, acc: 96%] [G loss: 1.136033]\n",
      "[Epoch 3/200] [Batch 905/938] [D loss: 1.083965, acc: 96%] [G loss: 1.163951]\n",
      "[Epoch 3/200] [Batch 906/938] [D loss: 1.100199, acc: 97%] [G loss: 1.228094]\n",
      "[Epoch 3/200] [Batch 907/938] [D loss: 1.064282, acc: 92%] [G loss: 1.175856]\n",
      "[Epoch 3/200] [Batch 908/938] [D loss: 1.104012, acc: 94%] [G loss: 1.157503]\n",
      "[Epoch 3/200] [Batch 909/938] [D loss: 1.096858, acc: 92%] [G loss: 1.115641]\n",
      "[Epoch 3/200] [Batch 910/938] [D loss: 1.076892, acc: 95%] [G loss: 1.111778]\n",
      "[Epoch 3/200] [Batch 911/938] [D loss: 1.094393, acc: 94%] [G loss: 1.107518]\n",
      "[Epoch 3/200] [Batch 912/938] [D loss: 1.086202, acc: 94%] [G loss: 1.213950]\n",
      "[Epoch 3/200] [Batch 913/938] [D loss: 1.096826, acc: 95%] [G loss: 1.181925]\n",
      "[Epoch 3/200] [Batch 914/938] [D loss: 1.071441, acc: 95%] [G loss: 1.207138]\n",
      "[Epoch 3/200] [Batch 915/938] [D loss: 1.087924, acc: 95%] [G loss: 1.155167]\n",
      "[Epoch 3/200] [Batch 916/938] [D loss: 1.038125, acc: 96%] [G loss: 1.123925]\n",
      "[Epoch 3/200] [Batch 917/938] [D loss: 1.056804, acc: 97%] [G loss: 1.163513]\n",
      "[Epoch 3/200] [Batch 918/938] [D loss: 1.066823, acc: 97%] [G loss: 1.124109]\n",
      "[Epoch 3/200] [Batch 919/938] [D loss: 1.102603, acc: 96%] [G loss: 1.107101]\n",
      "[Epoch 3/200] [Batch 920/938] [D loss: 1.119329, acc: 94%] [G loss: 1.092959]\n",
      "[Epoch 3/200] [Batch 921/938] [D loss: 1.111444, acc: 94%] [G loss: 1.198137]\n",
      "[Epoch 3/200] [Batch 922/938] [D loss: 1.095693, acc: 96%] [G loss: 1.144968]\n",
      "[Epoch 3/200] [Batch 923/938] [D loss: 1.067512, acc: 97%] [G loss: 1.131910]\n",
      "[Epoch 3/200] [Batch 924/938] [D loss: 1.068035, acc: 98%] [G loss: 1.194076]\n",
      "[Epoch 3/200] [Batch 925/938] [D loss: 1.069266, acc: 96%] [G loss: 1.229579]\n",
      "[Epoch 3/200] [Batch 926/938] [D loss: 1.073007, acc: 96%] [G loss: 1.124428]\n",
      "[Epoch 3/200] [Batch 927/938] [D loss: 1.071513, acc: 96%] [G loss: 1.112036]\n",
      "[Epoch 3/200] [Batch 928/938] [D loss: 1.094224, acc: 93%] [G loss: 1.140668]\n",
      "[Epoch 3/200] [Batch 929/938] [D loss: 1.145890, acc: 94%] [G loss: 1.094504]\n",
      "[Epoch 3/200] [Batch 930/938] [D loss: 1.040178, acc: 96%] [G loss: 1.138748]\n",
      "[Epoch 3/200] [Batch 931/938] [D loss: 1.090081, acc: 96%] [G loss: 1.228151]\n",
      "[Epoch 3/200] [Batch 932/938] [D loss: 1.087796, acc: 90%] [G loss: 1.202017]\n",
      "[Epoch 3/200] [Batch 933/938] [D loss: 1.120697, acc: 96%] [G loss: 1.204380]\n",
      "[Epoch 3/200] [Batch 934/938] [D loss: 1.090117, acc: 92%] [G loss: 1.196957]\n",
      "[Epoch 3/200] [Batch 935/938] [D loss: 1.067057, acc: 96%] [G loss: 1.148036]\n",
      "[Epoch 3/200] [Batch 936/938] [D loss: 1.087723, acc: 96%] [G loss: 1.159880]\n",
      "[Epoch 3/200] [Batch 937/938] [D loss: 1.106772, acc: 93%] [G loss: 1.145562]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0279f44da5b458fb6b9b8419a907d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/200] [Batch 0/938] [D loss: 1.085440, acc: 96%] [G loss: 1.108306]\n",
      "[Epoch 4/200] [Batch 1/938] [D loss: 1.128209, acc: 93%] [G loss: 1.155629]\n",
      "[Epoch 4/200] [Batch 2/938] [D loss: 1.104040, acc: 94%] [G loss: 1.115922]\n",
      "[Epoch 4/200] [Batch 3/938] [D loss: 1.077890, acc: 94%] [G loss: 1.159619]\n",
      "[Epoch 4/200] [Batch 4/938] [D loss: 1.063697, acc: 98%] [G loss: 1.147598]\n",
      "[Epoch 4/200] [Batch 5/938] [D loss: 1.098213, acc: 95%] [G loss: 1.113286]\n",
      "[Epoch 4/200] [Batch 6/938] [D loss: 1.089861, acc: 92%] [G loss: 1.047541]\n",
      "[Epoch 4/200] [Batch 7/938] [D loss: 1.140277, acc: 96%] [G loss: 1.079596]\n",
      "[Epoch 4/200] [Batch 8/938] [D loss: 1.071740, acc: 95%] [G loss: 1.119836]\n",
      "[Epoch 4/200] [Batch 9/938] [D loss: 1.105976, acc: 89%] [G loss: 1.133847]\n",
      "[Epoch 4/200] [Batch 10/938] [D loss: 1.106529, acc: 90%] [G loss: 1.202336]\n",
      "[Epoch 4/200] [Batch 11/938] [D loss: 1.112050, acc: 93%] [G loss: 1.112284]\n",
      "[Epoch 4/200] [Batch 12/938] [D loss: 1.063220, acc: 97%] [G loss: 1.110770]\n",
      "[Epoch 4/200] [Batch 13/938] [D loss: 1.056330, acc: 94%] [G loss: 1.165050]\n",
      "[Epoch 4/200] [Batch 14/938] [D loss: 1.090190, acc: 96%] [G loss: 1.174645]\n",
      "[Epoch 4/200] [Batch 15/938] [D loss: 1.103624, acc: 93%] [G loss: 1.107769]\n",
      "[Epoch 4/200] [Batch 16/938] [D loss: 1.064227, acc: 94%] [G loss: 1.183192]\n",
      "[Epoch 4/200] [Batch 17/938] [D loss: 1.083201, acc: 96%] [G loss: 1.146804]\n",
      "[Epoch 4/200] [Batch 18/938] [D loss: 1.086486, acc: 94%] [G loss: 1.123038]\n",
      "[Epoch 4/200] [Batch 19/938] [D loss: 1.103276, acc: 93%] [G loss: 1.129165]\n",
      "[Epoch 4/200] [Batch 20/938] [D loss: 1.097916, acc: 89%] [G loss: 1.168116]\n",
      "[Epoch 4/200] [Batch 21/938] [D loss: 1.089711, acc: 92%] [G loss: 1.167146]\n",
      "[Epoch 4/200] [Batch 22/938] [D loss: 1.115238, acc: 94%] [G loss: 1.156898]\n",
      "[Epoch 4/200] [Batch 23/938] [D loss: 1.099691, acc: 92%] [G loss: 1.236696]\n",
      "[Epoch 4/200] [Batch 24/938] [D loss: 1.103265, acc: 96%] [G loss: 1.147413]\n",
      "[Epoch 4/200] [Batch 25/938] [D loss: 1.120167, acc: 95%] [G loss: 1.078891]\n",
      "[Epoch 4/200] [Batch 26/938] [D loss: 1.126765, acc: 92%] [G loss: 1.175214]\n",
      "[Epoch 4/200] [Batch 27/938] [D loss: 1.094592, acc: 97%] [G loss: 1.126078]\n",
      "[Epoch 4/200] [Batch 28/938] [D loss: 1.086507, acc: 96%] [G loss: 1.098089]\n",
      "[Epoch 4/200] [Batch 29/938] [D loss: 1.078007, acc: 97%] [G loss: 1.151633]\n",
      "[Epoch 4/200] [Batch 30/938] [D loss: 1.124648, acc: 93%] [G loss: 1.144172]\n",
      "[Epoch 4/200] [Batch 31/938] [D loss: 1.084208, acc: 96%] [G loss: 1.183286]\n",
      "[Epoch 4/200] [Batch 32/938] [D loss: 1.079117, acc: 97%] [G loss: 1.178069]\n",
      "[Epoch 4/200] [Batch 33/938] [D loss: 1.111761, acc: 98%] [G loss: 1.158148]\n",
      "[Epoch 4/200] [Batch 34/938] [D loss: 1.085573, acc: 94%] [G loss: 1.088552]\n",
      "[Epoch 4/200] [Batch 35/938] [D loss: 1.091233, acc: 94%] [G loss: 1.157904]\n",
      "[Epoch 4/200] [Batch 36/938] [D loss: 1.065352, acc: 96%] [G loss: 1.170038]\n",
      "[Epoch 4/200] [Batch 37/938] [D loss: 1.111228, acc: 94%] [G loss: 1.115483]\n",
      "[Epoch 4/200] [Batch 38/938] [D loss: 1.067344, acc: 94%] [G loss: 1.176633]\n",
      "[Epoch 4/200] [Batch 39/938] [D loss: 1.070007, acc: 94%] [G loss: 1.205857]\n",
      "[Epoch 4/200] [Batch 40/938] [D loss: 1.087476, acc: 95%] [G loss: 1.157458]\n",
      "[Epoch 4/200] [Batch 41/938] [D loss: 1.079131, acc: 97%] [G loss: 1.164571]\n",
      "[Epoch 4/200] [Batch 42/938] [D loss: 1.089317, acc: 99%] [G loss: 1.098342]\n",
      "[Epoch 4/200] [Batch 43/938] [D loss: 1.073285, acc: 96%] [G loss: 1.184184]\n",
      "[Epoch 4/200] [Batch 44/938] [D loss: 1.060115, acc: 96%] [G loss: 1.121985]\n",
      "[Epoch 4/200] [Batch 45/938] [D loss: 1.095698, acc: 96%] [G loss: 1.137684]\n",
      "[Epoch 4/200] [Batch 46/938] [D loss: 1.077907, acc: 94%] [G loss: 1.126505]\n",
      "[Epoch 4/200] [Batch 47/938] [D loss: 1.076301, acc: 95%] [G loss: 1.166685]\n",
      "[Epoch 4/200] [Batch 48/938] [D loss: 1.083236, acc: 99%] [G loss: 1.185674]\n",
      "[Epoch 4/200] [Batch 49/938] [D loss: 1.086925, acc: 95%] [G loss: 1.194720]\n",
      "[Epoch 4/200] [Batch 50/938] [D loss: 1.108780, acc: 96%] [G loss: 1.166611]\n",
      "[Epoch 4/200] [Batch 51/938] [D loss: 1.068020, acc: 100%] [G loss: 1.121849]\n",
      "[Epoch 4/200] [Batch 52/938] [D loss: 1.101434, acc: 93%] [G loss: 1.155409]\n",
      "[Epoch 4/200] [Batch 53/938] [D loss: 1.082899, acc: 96%] [G loss: 1.081341]\n",
      "[Epoch 4/200] [Batch 54/938] [D loss: 1.038024, acc: 98%] [G loss: 1.154086]\n",
      "[Epoch 4/200] [Batch 55/938] [D loss: 1.072245, acc: 96%] [G loss: 1.125240]\n",
      "[Epoch 4/200] [Batch 56/938] [D loss: 1.105832, acc: 92%] [G loss: 1.135031]\n",
      "[Epoch 4/200] [Batch 57/938] [D loss: 1.073882, acc: 94%] [G loss: 1.201159]\n",
      "[Epoch 4/200] [Batch 58/938] [D loss: 1.062593, acc: 94%] [G loss: 1.128240]\n",
      "[Epoch 4/200] [Batch 59/938] [D loss: 1.077005, acc: 93%] [G loss: 1.087393]\n",
      "[Epoch 4/200] [Batch 60/938] [D loss: 1.098080, acc: 96%] [G loss: 1.160966]\n",
      "[Epoch 4/200] [Batch 61/938] [D loss: 1.078027, acc: 96%] [G loss: 1.181160]\n",
      "[Epoch 4/200] [Batch 62/938] [D loss: 1.069152, acc: 97%] [G loss: 1.166236]\n",
      "[Epoch 4/200] [Batch 63/938] [D loss: 1.051512, acc: 98%] [G loss: 1.121272]\n",
      "[Epoch 4/200] [Batch 64/938] [D loss: 1.086718, acc: 92%] [G loss: 1.137410]\n",
      "[Epoch 4/200] [Batch 65/938] [D loss: 1.106115, acc: 92%] [G loss: 1.091886]\n",
      "[Epoch 4/200] [Batch 66/938] [D loss: 1.075036, acc: 96%] [G loss: 1.105862]\n",
      "[Epoch 4/200] [Batch 67/938] [D loss: 1.134376, acc: 93%] [G loss: 1.182333]\n",
      "[Epoch 4/200] [Batch 68/938] [D loss: 1.111097, acc: 92%] [G loss: 1.183503]\n",
      "[Epoch 4/200] [Batch 69/938] [D loss: 1.127333, acc: 92%] [G loss: 1.099822]\n",
      "[Epoch 4/200] [Batch 70/938] [D loss: 1.077866, acc: 92%] [G loss: 1.169178]\n",
      "[Epoch 4/200] [Batch 71/938] [D loss: 1.120322, acc: 96%] [G loss: 1.190574]\n",
      "[Epoch 4/200] [Batch 72/938] [D loss: 1.081391, acc: 95%] [G loss: 1.190865]\n",
      "[Epoch 4/200] [Batch 73/938] [D loss: 1.094052, acc: 96%] [G loss: 1.109104]\n",
      "[Epoch 4/200] [Batch 74/938] [D loss: 1.067987, acc: 94%] [G loss: 1.103788]\n",
      "[Epoch 4/200] [Batch 75/938] [D loss: 1.082811, acc: 92%] [G loss: 1.115353]\n",
      "[Epoch 4/200] [Batch 76/938] [D loss: 1.053419, acc: 96%] [G loss: 1.139118]\n",
      "[Epoch 4/200] [Batch 77/938] [D loss: 1.050411, acc: 97%] [G loss: 1.216767]\n",
      "[Epoch 4/200] [Batch 78/938] [D loss: 1.110288, acc: 90%] [G loss: 1.178762]\n",
      "[Epoch 4/200] [Batch 79/938] [D loss: 1.072743, acc: 95%] [G loss: 1.158254]\n",
      "[Epoch 4/200] [Batch 80/938] [D loss: 1.092034, acc: 93%] [G loss: 1.064479]\n",
      "[Epoch 4/200] [Batch 81/938] [D loss: 1.092509, acc: 92%] [G loss: 1.151435]\n",
      "[Epoch 4/200] [Batch 82/938] [D loss: 1.126138, acc: 92%] [G loss: 1.097243]\n",
      "[Epoch 4/200] [Batch 83/938] [D loss: 1.044013, acc: 94%] [G loss: 1.205322]\n",
      "[Epoch 4/200] [Batch 84/938] [D loss: 1.088355, acc: 92%] [G loss: 1.103487]\n",
      "[Epoch 4/200] [Batch 85/938] [D loss: 1.044641, acc: 98%] [G loss: 1.170945]\n",
      "[Epoch 4/200] [Batch 86/938] [D loss: 1.074502, acc: 95%] [G loss: 1.112172]\n",
      "[Epoch 4/200] [Batch 87/938] [D loss: 1.137637, acc: 92%] [G loss: 1.096074]\n",
      "[Epoch 4/200] [Batch 88/938] [D loss: 1.089443, acc: 93%] [G loss: 1.113273]\n",
      "[Epoch 4/200] [Batch 89/938] [D loss: 1.082800, acc: 96%] [G loss: 1.196293]\n",
      "[Epoch 4/200] [Batch 90/938] [D loss: 1.111284, acc: 93%] [G loss: 1.159611]\n",
      "[Epoch 4/200] [Batch 91/938] [D loss: 1.149542, acc: 95%] [G loss: 1.185094]\n",
      "[Epoch 4/200] [Batch 92/938] [D loss: 1.094731, acc: 97%] [G loss: 1.045138]\n",
      "[Epoch 4/200] [Batch 93/938] [D loss: 1.120625, acc: 91%] [G loss: 1.110498]\n",
      "[Epoch 4/200] [Batch 94/938] [D loss: 1.081682, acc: 96%] [G loss: 1.169281]\n",
      "[Epoch 4/200] [Batch 95/938] [D loss: 1.072980, acc: 94%] [G loss: 1.130500]\n",
      "[Epoch 4/200] [Batch 96/938] [D loss: 1.066938, acc: 95%] [G loss: 1.171268]\n",
      "[Epoch 4/200] [Batch 97/938] [D loss: 1.101526, acc: 96%] [G loss: 1.160669]\n",
      "[Epoch 4/200] [Batch 98/938] [D loss: 1.086291, acc: 94%] [G loss: 1.121399]\n",
      "[Epoch 4/200] [Batch 99/938] [D loss: 1.077240, acc: 96%] [G loss: 1.099473]\n",
      "[Epoch 4/200] [Batch 100/938] [D loss: 1.083576, acc: 92%] [G loss: 1.144671]\n",
      "[Epoch 4/200] [Batch 101/938] [D loss: 1.115055, acc: 95%] [G loss: 1.195590]\n",
      "[Epoch 4/200] [Batch 102/938] [D loss: 1.101374, acc: 94%] [G loss: 1.220692]\n",
      "[Epoch 4/200] [Batch 103/938] [D loss: 1.106924, acc: 95%] [G loss: 1.150311]\n",
      "[Epoch 4/200] [Batch 104/938] [D loss: 1.100638, acc: 92%] [G loss: 1.165237]\n",
      "[Epoch 4/200] [Batch 105/938] [D loss: 1.074835, acc: 90%] [G loss: 1.190214]\n",
      "[Epoch 4/200] [Batch 106/938] [D loss: 1.052249, acc: 96%] [G loss: 1.140908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/200] [Batch 107/938] [D loss: 1.107784, acc: 91%] [G loss: 1.143923]\n",
      "[Epoch 4/200] [Batch 108/938] [D loss: 1.067514, acc: 95%] [G loss: 1.173959]\n",
      "[Epoch 4/200] [Batch 109/938] [D loss: 1.076336, acc: 96%] [G loss: 1.143832]\n",
      "[Epoch 4/200] [Batch 110/938] [D loss: 1.092389, acc: 94%] [G loss: 1.093866]\n",
      "[Epoch 4/200] [Batch 111/938] [D loss: 1.082676, acc: 97%] [G loss: 1.183120]\n",
      "[Epoch 4/200] [Batch 112/938] [D loss: 1.068938, acc: 97%] [G loss: 1.163622]\n",
      "[Epoch 4/200] [Batch 113/938] [D loss: 1.136184, acc: 92%] [G loss: 1.206111]\n",
      "[Epoch 4/200] [Batch 114/938] [D loss: 1.098047, acc: 94%] [G loss: 1.093589]\n",
      "[Epoch 4/200] [Batch 115/938] [D loss: 1.096424, acc: 92%] [G loss: 1.162901]\n",
      "[Epoch 4/200] [Batch 116/938] [D loss: 1.119993, acc: 92%] [G loss: 1.197501]\n",
      "[Epoch 4/200] [Batch 117/938] [D loss: 1.063900, acc: 95%] [G loss: 1.128723]\n",
      "[Epoch 4/200] [Batch 118/938] [D loss: 1.069776, acc: 96%] [G loss: 1.165712]\n",
      "[Epoch 4/200] [Batch 119/938] [D loss: 1.072054, acc: 95%] [G loss: 1.208736]\n",
      "[Epoch 4/200] [Batch 120/938] [D loss: 1.079570, acc: 96%] [G loss: 1.237914]\n",
      "[Epoch 4/200] [Batch 121/938] [D loss: 1.051043, acc: 96%] [G loss: 1.124967]\n",
      "[Epoch 4/200] [Batch 122/938] [D loss: 1.086762, acc: 93%] [G loss: 1.183688]\n",
      "[Epoch 4/200] [Batch 123/938] [D loss: 1.080763, acc: 94%] [G loss: 1.133245]\n",
      "[Epoch 4/200] [Batch 124/938] [D loss: 1.075240, acc: 96%] [G loss: 1.112512]\n",
      "[Epoch 4/200] [Batch 125/938] [D loss: 1.036367, acc: 95%] [G loss: 1.135559]\n",
      "[Epoch 4/200] [Batch 126/938] [D loss: 1.104385, acc: 96%] [G loss: 1.056918]\n",
      "[Epoch 4/200] [Batch 127/938] [D loss: 1.085539, acc: 94%] [G loss: 1.185758]\n",
      "[Epoch 4/200] [Batch 128/938] [D loss: 1.092533, acc: 94%] [G loss: 1.140688]\n",
      "[Epoch 4/200] [Batch 129/938] [D loss: 1.085529, acc: 95%] [G loss: 1.168362]\n",
      "[Epoch 4/200] [Batch 130/938] [D loss: 1.149001, acc: 92%] [G loss: 1.145681]\n",
      "[Epoch 4/200] [Batch 131/938] [D loss: 1.106285, acc: 95%] [G loss: 1.159727]\n",
      "[Epoch 4/200] [Batch 132/938] [D loss: 1.112325, acc: 94%] [G loss: 1.072800]\n",
      "[Epoch 4/200] [Batch 133/938] [D loss: 1.083427, acc: 95%] [G loss: 1.139345]\n",
      "[Epoch 4/200] [Batch 134/938] [D loss: 1.074416, acc: 94%] [G loss: 1.105258]\n",
      "[Epoch 4/200] [Batch 135/938] [D loss: 1.094434, acc: 93%] [G loss: 1.150583]\n",
      "[Epoch 4/200] [Batch 136/938] [D loss: 1.074194, acc: 96%] [G loss: 1.114647]\n",
      "[Epoch 4/200] [Batch 137/938] [D loss: 1.048185, acc: 96%] [G loss: 1.149065]\n",
      "[Epoch 4/200] [Batch 138/938] [D loss: 1.047064, acc: 96%] [G loss: 1.167535]\n",
      "[Epoch 4/200] [Batch 139/938] [D loss: 1.081848, acc: 92%] [G loss: 1.119707]\n",
      "[Epoch 4/200] [Batch 140/938] [D loss: 1.081640, acc: 95%] [G loss: 1.095164]\n",
      "[Epoch 4/200] [Batch 141/938] [D loss: 1.057631, acc: 93%] [G loss: 1.115912]\n",
      "[Epoch 4/200] [Batch 142/938] [D loss: 1.093265, acc: 95%] [G loss: 1.105168]\n",
      "[Epoch 4/200] [Batch 143/938] [D loss: 1.123673, acc: 92%] [G loss: 1.102348]\n",
      "[Epoch 4/200] [Batch 144/938] [D loss: 1.053073, acc: 95%] [G loss: 1.170313]\n",
      "[Epoch 4/200] [Batch 145/938] [D loss: 1.106524, acc: 98%] [G loss: 1.101292]\n",
      "[Epoch 4/200] [Batch 146/938] [D loss: 1.088247, acc: 95%] [G loss: 1.082955]\n",
      "[Epoch 4/200] [Batch 147/938] [D loss: 1.082294, acc: 96%] [G loss: 1.097868]\n",
      "[Epoch 4/200] [Batch 148/938] [D loss: 1.087107, acc: 92%] [G loss: 1.182901]\n",
      "[Epoch 4/200] [Batch 149/938] [D loss: 1.117267, acc: 95%] [G loss: 1.140044]\n",
      "[Epoch 4/200] [Batch 150/938] [D loss: 1.086952, acc: 95%] [G loss: 1.100491]\n",
      "[Epoch 4/200] [Batch 151/938] [D loss: 1.090715, acc: 93%] [G loss: 1.130684]\n",
      "[Epoch 4/200] [Batch 152/938] [D loss: 1.075237, acc: 94%] [G loss: 1.168384]\n",
      "[Epoch 4/200] [Batch 153/938] [D loss: 1.087616, acc: 96%] [G loss: 1.200704]\n",
      "[Epoch 4/200] [Batch 154/938] [D loss: 1.102781, acc: 96%] [G loss: 1.166510]\n",
      "[Epoch 4/200] [Batch 155/938] [D loss: 1.048823, acc: 94%] [G loss: 1.236349]\n",
      "[Epoch 4/200] [Batch 156/938] [D loss: 1.079918, acc: 93%] [G loss: 1.114676]\n",
      "[Epoch 4/200] [Batch 157/938] [D loss: 1.065335, acc: 95%] [G loss: 1.177962]\n",
      "[Epoch 4/200] [Batch 158/938] [D loss: 1.074991, acc: 96%] [G loss: 1.069701]\n",
      "[Epoch 4/200] [Batch 159/938] [D loss: 1.078494, acc: 95%] [G loss: 1.142606]\n",
      "[Epoch 4/200] [Batch 160/938] [D loss: 1.067639, acc: 93%] [G loss: 1.175063]\n",
      "[Epoch 4/200] [Batch 161/938] [D loss: 1.082347, acc: 96%] [G loss: 1.128124]\n",
      "[Epoch 4/200] [Batch 162/938] [D loss: 1.098447, acc: 96%] [G loss: 1.111561]\n",
      "[Epoch 4/200] [Batch 163/938] [D loss: 1.079008, acc: 95%] [G loss: 1.133883]\n",
      "[Epoch 4/200] [Batch 164/938] [D loss: 1.052320, acc: 92%] [G loss: 1.164421]\n",
      "[Epoch 4/200] [Batch 165/938] [D loss: 1.108306, acc: 96%] [G loss: 1.225721]\n",
      "[Epoch 4/200] [Batch 166/938] [D loss: 1.059432, acc: 93%] [G loss: 1.280566]\n",
      "[Epoch 4/200] [Batch 167/938] [D loss: 1.128888, acc: 94%] [G loss: 1.139768]\n",
      "[Epoch 4/200] [Batch 168/938] [D loss: 1.120913, acc: 92%] [G loss: 1.098126]\n",
      "[Epoch 4/200] [Batch 169/938] [D loss: 1.079967, acc: 89%] [G loss: 1.203149]\n",
      "[Epoch 4/200] [Batch 170/938] [D loss: 1.063020, acc: 96%] [G loss: 1.145722]\n",
      "[Epoch 4/200] [Batch 171/938] [D loss: 1.108089, acc: 97%] [G loss: 1.119453]\n",
      "[Epoch 4/200] [Batch 172/938] [D loss: 1.074140, acc: 96%] [G loss: 1.253478]\n",
      "[Epoch 4/200] [Batch 173/938] [D loss: 1.077713, acc: 94%] [G loss: 1.190035]\n",
      "[Epoch 4/200] [Batch 174/938] [D loss: 1.081081, acc: 97%] [G loss: 1.135155]\n",
      "[Epoch 4/200] [Batch 175/938] [D loss: 1.046884, acc: 96%] [G loss: 1.165455]\n",
      "[Epoch 4/200] [Batch 176/938] [D loss: 1.103002, acc: 93%] [G loss: 1.101789]\n",
      "[Epoch 4/200] [Batch 177/938] [D loss: 1.075875, acc: 95%] [G loss: 1.112154]\n",
      "[Epoch 4/200] [Batch 178/938] [D loss: 1.075030, acc: 92%] [G loss: 1.213287]\n",
      "[Epoch 4/200] [Batch 179/938] [D loss: 1.065844, acc: 97%] [G loss: 1.166830]\n",
      "[Epoch 4/200] [Batch 180/938] [D loss: 1.087277, acc: 96%] [G loss: 1.169999]\n",
      "[Epoch 4/200] [Batch 181/938] [D loss: 1.104859, acc: 97%] [G loss: 1.122761]\n",
      "[Epoch 4/200] [Batch 182/938] [D loss: 1.069231, acc: 98%] [G loss: 1.128187]\n",
      "[Epoch 4/200] [Batch 183/938] [D loss: 1.103777, acc: 95%] [G loss: 1.058003]\n",
      "[Epoch 4/200] [Batch 184/938] [D loss: 1.117426, acc: 95%] [G loss: 1.153265]\n",
      "[Epoch 4/200] [Batch 185/938] [D loss: 1.116377, acc: 92%] [G loss: 1.183814]\n",
      "[Epoch 4/200] [Batch 186/938] [D loss: 1.127404, acc: 93%] [G loss: 1.077273]\n",
      "[Epoch 4/200] [Batch 187/938] [D loss: 1.091384, acc: 93%] [G loss: 1.102174]\n",
      "[Epoch 4/200] [Batch 188/938] [D loss: 1.107925, acc: 93%] [G loss: 1.150353]\n",
      "[Epoch 4/200] [Batch 189/938] [D loss: 1.106300, acc: 96%] [G loss: 1.147525]\n",
      "[Epoch 4/200] [Batch 190/938] [D loss: 1.044770, acc: 99%] [G loss: 1.185173]\n",
      "[Epoch 4/200] [Batch 191/938] [D loss: 1.080715, acc: 92%] [G loss: 1.127429]\n",
      "[Epoch 4/200] [Batch 192/938] [D loss: 1.071220, acc: 96%] [G loss: 1.142831]\n",
      "[Epoch 4/200] [Batch 193/938] [D loss: 1.084862, acc: 96%] [G loss: 1.114922]\n",
      "[Epoch 4/200] [Batch 194/938] [D loss: 1.090118, acc: 96%] [G loss: 1.115286]\n",
      "[Epoch 4/200] [Batch 195/938] [D loss: 1.094466, acc: 94%] [G loss: 1.124525]\n",
      "[Epoch 4/200] [Batch 196/938] [D loss: 1.072433, acc: 96%] [G loss: 1.062214]\n",
      "[Epoch 4/200] [Batch 197/938] [D loss: 1.079624, acc: 95%] [G loss: 1.184167]\n",
      "[Epoch 4/200] [Batch 198/938] [D loss: 1.073326, acc: 94%] [G loss: 1.130361]\n",
      "[Epoch 4/200] [Batch 199/938] [D loss: 1.073084, acc: 97%] [G loss: 1.199481]\n",
      "[Epoch 4/200] [Batch 200/938] [D loss: 1.151070, acc: 94%] [G loss: 1.185034]\n",
      "[Epoch 4/200] [Batch 201/938] [D loss: 1.077213, acc: 92%] [G loss: 1.062418]\n",
      "[Epoch 4/200] [Batch 202/938] [D loss: 1.109091, acc: 92%] [G loss: 1.091460]\n",
      "[Epoch 4/200] [Batch 203/938] [D loss: 1.080029, acc: 96%] [G loss: 1.169045]\n",
      "[Epoch 4/200] [Batch 204/938] [D loss: 1.036320, acc: 97%] [G loss: 1.200475]\n",
      "[Epoch 4/200] [Batch 205/938] [D loss: 1.063474, acc: 96%] [G loss: 1.169866]\n",
      "[Epoch 4/200] [Batch 206/938] [D loss: 1.071382, acc: 94%] [G loss: 1.177794]\n",
      "[Epoch 4/200] [Batch 207/938] [D loss: 1.066205, acc: 96%] [G loss: 1.101843]\n",
      "[Epoch 4/200] [Batch 208/938] [D loss: 1.080325, acc: 96%] [G loss: 1.180699]\n",
      "[Epoch 4/200] [Batch 209/938] [D loss: 1.123390, acc: 94%] [G loss: 1.159800]\n",
      "[Epoch 4/200] [Batch 210/938] [D loss: 1.066172, acc: 96%] [G loss: 1.112887]\n",
      "[Epoch 4/200] [Batch 211/938] [D loss: 1.078693, acc: 96%] [G loss: 1.192116]\n",
      "[Epoch 4/200] [Batch 212/938] [D loss: 1.050608, acc: 97%] [G loss: 1.096799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/200] [Batch 213/938] [D loss: 1.079363, acc: 96%] [G loss: 1.112981]\n",
      "[Epoch 4/200] [Batch 214/938] [D loss: 1.081783, acc: 93%] [G loss: 1.222423]\n",
      "[Epoch 4/200] [Batch 215/938] [D loss: 1.079402, acc: 94%] [G loss: 1.162114]\n",
      "[Epoch 4/200] [Batch 216/938] [D loss: 1.089114, acc: 94%] [G loss: 1.141466]\n",
      "[Epoch 4/200] [Batch 217/938] [D loss: 1.095149, acc: 92%] [G loss: 1.180652]\n",
      "[Epoch 4/200] [Batch 218/938] [D loss: 1.103521, acc: 97%] [G loss: 1.099407]\n",
      "[Epoch 4/200] [Batch 219/938] [D loss: 1.089524, acc: 95%] [G loss: 1.168461]\n",
      "[Epoch 4/200] [Batch 220/938] [D loss: 1.120011, acc: 93%] [G loss: 1.131237]\n",
      "[Epoch 4/200] [Batch 221/938] [D loss: 1.088681, acc: 96%] [G loss: 1.091328]\n",
      "[Epoch 4/200] [Batch 222/938] [D loss: 1.122661, acc: 93%] [G loss: 1.100647]\n",
      "[Epoch 4/200] [Batch 223/938] [D loss: 1.059124, acc: 97%] [G loss: 1.116059]\n",
      "[Epoch 4/200] [Batch 224/938] [D loss: 1.085222, acc: 92%] [G loss: 1.344396]\n",
      "[Epoch 4/200] [Batch 225/938] [D loss: 1.060758, acc: 96%] [G loss: 1.102397]\n",
      "[Epoch 4/200] [Batch 226/938] [D loss: 1.103630, acc: 97%] [G loss: 1.168599]\n",
      "[Epoch 4/200] [Batch 227/938] [D loss: 1.085165, acc: 92%] [G loss: 1.165728]\n",
      "[Epoch 4/200] [Batch 228/938] [D loss: 1.080844, acc: 94%] [G loss: 1.165578]\n",
      "[Epoch 4/200] [Batch 229/938] [D loss: 1.082327, acc: 96%] [G loss: 1.087028]\n",
      "[Epoch 4/200] [Batch 230/938] [D loss: 1.109936, acc: 94%] [G loss: 1.171959]\n",
      "[Epoch 4/200] [Batch 231/938] [D loss: 1.067980, acc: 96%] [G loss: 1.190092]\n",
      "[Epoch 4/200] [Batch 232/938] [D loss: 1.084507, acc: 94%] [G loss: 1.195657]\n",
      "[Epoch 4/200] [Batch 233/938] [D loss: 1.100446, acc: 94%] [G loss: 1.080129]\n",
      "[Epoch 4/200] [Batch 234/938] [D loss: 1.143020, acc: 93%] [G loss: 1.152910]\n",
      "[Epoch 4/200] [Batch 235/938] [D loss: 1.107495, acc: 96%] [G loss: 1.107793]\n",
      "[Epoch 4/200] [Batch 236/938] [D loss: 1.099380, acc: 95%] [G loss: 1.106874]\n",
      "[Epoch 4/200] [Batch 237/938] [D loss: 1.109793, acc: 92%] [G loss: 1.136516]\n",
      "[Epoch 4/200] [Batch 238/938] [D loss: 1.068551, acc: 95%] [G loss: 1.173931]\n",
      "[Epoch 4/200] [Batch 239/938] [D loss: 1.092449, acc: 92%] [G loss: 1.063522]\n",
      "[Epoch 4/200] [Batch 240/938] [D loss: 1.066504, acc: 95%] [G loss: 1.136023]\n",
      "[Epoch 4/200] [Batch 241/938] [D loss: 1.129453, acc: 95%] [G loss: 1.043793]\n",
      "[Epoch 4/200] [Batch 242/938] [D loss: 1.081023, acc: 93%] [G loss: 1.153008]\n",
      "[Epoch 4/200] [Batch 243/938] [D loss: 1.073570, acc: 96%] [G loss: 1.124886]\n",
      "[Epoch 4/200] [Batch 244/938] [D loss: 1.061833, acc: 95%] [G loss: 1.132532]\n",
      "[Epoch 4/200] [Batch 245/938] [D loss: 1.070001, acc: 96%] [G loss: 1.172354]\n",
      "[Epoch 4/200] [Batch 246/938] [D loss: 1.042731, acc: 96%] [G loss: 1.106826]\n",
      "[Epoch 4/200] [Batch 247/938] [D loss: 1.146022, acc: 91%] [G loss: 1.117536]\n",
      "[Epoch 4/200] [Batch 248/938] [D loss: 1.098432, acc: 95%] [G loss: 1.099229]\n",
      "[Epoch 4/200] [Batch 249/938] [D loss: 1.093210, acc: 93%] [G loss: 1.113743]\n",
      "[Epoch 4/200] [Batch 250/938] [D loss: 1.107150, acc: 96%] [G loss: 1.067809]\n",
      "[Epoch 4/200] [Batch 251/938] [D loss: 1.064676, acc: 95%] [G loss: 1.148043]\n",
      "[Epoch 4/200] [Batch 252/938] [D loss: 1.062606, acc: 96%] [G loss: 1.126408]\n",
      "[Epoch 4/200] [Batch 253/938] [D loss: 1.103881, acc: 93%] [G loss: 1.124373]\n",
      "[Epoch 4/200] [Batch 254/938] [D loss: 1.103212, acc: 94%] [G loss: 1.140000]\n",
      "[Epoch 4/200] [Batch 255/938] [D loss: 1.058136, acc: 92%] [G loss: 1.153166]\n",
      "[Epoch 4/200] [Batch 256/938] [D loss: 1.074309, acc: 97%] [G loss: 1.158614]\n",
      "[Epoch 4/200] [Batch 257/938] [D loss: 1.085814, acc: 94%] [G loss: 1.198433]\n",
      "[Epoch 4/200] [Batch 258/938] [D loss: 1.058692, acc: 96%] [G loss: 1.103450]\n",
      "[Epoch 4/200] [Batch 259/938] [D loss: 1.082508, acc: 93%] [G loss: 1.094394]\n",
      "[Epoch 4/200] [Batch 260/938] [D loss: 1.117910, acc: 97%] [G loss: 1.093238]\n",
      "[Epoch 4/200] [Batch 261/938] [D loss: 1.050615, acc: 96%] [G loss: 1.113097]\n",
      "[Epoch 4/200] [Batch 262/938] [D loss: 1.078745, acc: 94%] [G loss: 1.180403]\n",
      "[Epoch 4/200] [Batch 263/938] [D loss: 1.064882, acc: 98%] [G loss: 1.126169]\n",
      "[Epoch 4/200] [Batch 264/938] [D loss: 1.078875, acc: 94%] [G loss: 1.091156]\n",
      "[Epoch 4/200] [Batch 265/938] [D loss: 1.097504, acc: 91%] [G loss: 1.139497]\n",
      "[Epoch 4/200] [Batch 266/938] [D loss: 1.103006, acc: 98%] [G loss: 1.154366]\n",
      "[Epoch 4/200] [Batch 267/938] [D loss: 1.088032, acc: 95%] [G loss: 1.145093]\n",
      "[Epoch 4/200] [Batch 268/938] [D loss: 1.066944, acc: 94%] [G loss: 1.198245]\n",
      "[Epoch 4/200] [Batch 269/938] [D loss: 1.083698, acc: 92%] [G loss: 1.115784]\n",
      "[Epoch 4/200] [Batch 270/938] [D loss: 1.094351, acc: 96%] [G loss: 1.063458]\n",
      "[Epoch 4/200] [Batch 271/938] [D loss: 1.088077, acc: 96%] [G loss: 1.153085]\n",
      "[Epoch 4/200] [Batch 272/938] [D loss: 1.107446, acc: 96%] [G loss: 1.132573]\n",
      "[Epoch 4/200] [Batch 273/938] [D loss: 1.083332, acc: 96%] [G loss: 1.072122]\n",
      "[Epoch 4/200] [Batch 274/938] [D loss: 1.067813, acc: 96%] [G loss: 1.155238]\n",
      "[Epoch 4/200] [Batch 275/938] [D loss: 1.066358, acc: 96%] [G loss: 1.212965]\n",
      "[Epoch 4/200] [Batch 276/938] [D loss: 1.086402, acc: 93%] [G loss: 1.126191]\n",
      "[Epoch 4/200] [Batch 277/938] [D loss: 1.091628, acc: 96%] [G loss: 1.209156]\n",
      "[Epoch 4/200] [Batch 278/938] [D loss: 1.101763, acc: 95%] [G loss: 1.156274]\n",
      "[Epoch 4/200] [Batch 279/938] [D loss: 1.081256, acc: 98%] [G loss: 1.134791]\n",
      "[Epoch 4/200] [Batch 280/938] [D loss: 1.079285, acc: 95%] [G loss: 1.069319]\n",
      "[Epoch 4/200] [Batch 281/938] [D loss: 1.067260, acc: 95%] [G loss: 1.166368]\n",
      "[Epoch 4/200] [Batch 282/938] [D loss: 1.089910, acc: 96%] [G loss: 1.184528]\n",
      "[Epoch 4/200] [Batch 283/938] [D loss: 1.120631, acc: 95%] [G loss: 1.118042]\n",
      "[Epoch 4/200] [Batch 284/938] [D loss: 1.070799, acc: 96%] [G loss: 1.097432]\n",
      "[Epoch 4/200] [Batch 285/938] [D loss: 1.047567, acc: 93%] [G loss: 1.162445]\n",
      "[Epoch 4/200] [Batch 286/938] [D loss: 1.114284, acc: 95%] [G loss: 1.130075]\n",
      "[Epoch 4/200] [Batch 287/938] [D loss: 1.065661, acc: 95%] [G loss: 1.117079]\n",
      "[Epoch 4/200] [Batch 288/938] [D loss: 1.124447, acc: 94%] [G loss: 1.098633]\n",
      "[Epoch 4/200] [Batch 289/938] [D loss: 1.113756, acc: 97%] [G loss: 1.172577]\n",
      "[Epoch 4/200] [Batch 290/938] [D loss: 1.061719, acc: 98%] [G loss: 1.126019]\n",
      "[Epoch 4/200] [Batch 291/938] [D loss: 1.081434, acc: 97%] [G loss: 1.179382]\n",
      "[Epoch 4/200] [Batch 292/938] [D loss: 1.090675, acc: 94%] [G loss: 1.184693]\n",
      "[Epoch 4/200] [Batch 293/938] [D loss: 1.090239, acc: 97%] [G loss: 1.171163]\n",
      "[Epoch 4/200] [Batch 294/938] [D loss: 1.089280, acc: 96%] [G loss: 1.201549]\n",
      "[Epoch 4/200] [Batch 295/938] [D loss: 1.056050, acc: 92%] [G loss: 1.227172]\n",
      "[Epoch 4/200] [Batch 296/938] [D loss: 1.125266, acc: 94%] [G loss: 1.155309]\n",
      "[Epoch 4/200] [Batch 297/938] [D loss: 1.113192, acc: 97%] [G loss: 1.085499]\n",
      "[Epoch 4/200] [Batch 298/938] [D loss: 1.095037, acc: 96%] [G loss: 1.095432]\n",
      "[Epoch 4/200] [Batch 299/938] [D loss: 1.077631, acc: 96%] [G loss: 1.167362]\n",
      "[Epoch 4/200] [Batch 300/938] [D loss: 1.127173, acc: 90%] [G loss: 1.126436]\n",
      "[Epoch 4/200] [Batch 301/938] [D loss: 1.057164, acc: 96%] [G loss: 1.148494]\n",
      "[Epoch 4/200] [Batch 302/938] [D loss: 1.095963, acc: 96%] [G loss: 1.207697]\n",
      "[Epoch 4/200] [Batch 303/938] [D loss: 1.064763, acc: 92%] [G loss: 1.129991]\n",
      "[Epoch 4/200] [Batch 304/938] [D loss: 1.069896, acc: 96%] [G loss: 1.159285]\n",
      "[Epoch 4/200] [Batch 305/938] [D loss: 1.115374, acc: 95%] [G loss: 1.114982]\n",
      "[Epoch 4/200] [Batch 306/938] [D loss: 1.096645, acc: 98%] [G loss: 1.182004]\n",
      "[Epoch 4/200] [Batch 307/938] [D loss: 1.085787, acc: 93%] [G loss: 1.109807]\n",
      "[Epoch 4/200] [Batch 308/938] [D loss: 1.085297, acc: 96%] [G loss: 1.098031]\n",
      "[Epoch 4/200] [Batch 309/938] [D loss: 1.091225, acc: 97%] [G loss: 1.119515]\n",
      "[Epoch 4/200] [Batch 310/938] [D loss: 1.130400, acc: 92%] [G loss: 1.148021]\n",
      "[Epoch 4/200] [Batch 311/938] [D loss: 1.043772, acc: 95%] [G loss: 1.161440]\n",
      "[Epoch 4/200] [Batch 312/938] [D loss: 1.073276, acc: 94%] [G loss: 1.209010]\n",
      "[Epoch 4/200] [Batch 313/938] [D loss: 1.148107, acc: 96%] [G loss: 1.132357]\n",
      "[Epoch 4/200] [Batch 314/938] [D loss: 1.061419, acc: 98%] [G loss: 1.111790]\n",
      "[Epoch 4/200] [Batch 315/938] [D loss: 1.088334, acc: 95%] [G loss: 1.067129]\n",
      "[Epoch 4/200] [Batch 316/938] [D loss: 1.061303, acc: 95%] [G loss: 1.156347]\n",
      "[Epoch 4/200] [Batch 317/938] [D loss: 1.091755, acc: 93%] [G loss: 1.173337]\n",
      "[Epoch 4/200] [Batch 318/938] [D loss: 1.095785, acc: 96%] [G loss: 1.212736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/200] [Batch 319/938] [D loss: 1.052151, acc: 98%] [G loss: 1.207306]\n",
      "[Epoch 4/200] [Batch 320/938] [D loss: 1.069471, acc: 94%] [G loss: 1.132184]\n",
      "[Epoch 4/200] [Batch 321/938] [D loss: 1.100644, acc: 96%] [G loss: 1.144508]\n",
      "[Epoch 4/200] [Batch 322/938] [D loss: 1.041316, acc: 97%] [G loss: 1.132318]\n",
      "[Epoch 4/200] [Batch 323/938] [D loss: 1.070356, acc: 95%] [G loss: 1.175296]\n",
      "[Epoch 4/200] [Batch 324/938] [D loss: 1.068794, acc: 93%] [G loss: 1.116359]\n",
      "[Epoch 4/200] [Batch 325/938] [D loss: 1.079199, acc: 95%] [G loss: 1.138889]\n",
      "[Epoch 4/200] [Batch 326/938] [D loss: 1.076421, acc: 92%] [G loss: 1.088183]\n",
      "[Epoch 4/200] [Batch 327/938] [D loss: 1.084484, acc: 95%] [G loss: 1.117081]\n",
      "[Epoch 4/200] [Batch 328/938] [D loss: 1.054626, acc: 94%] [G loss: 1.086707]\n",
      "[Epoch 4/200] [Batch 329/938] [D loss: 1.103831, acc: 96%] [G loss: 1.150083]\n",
      "[Epoch 4/200] [Batch 330/938] [D loss: 1.088156, acc: 93%] [G loss: 1.238668]\n",
      "[Epoch 4/200] [Batch 331/938] [D loss: 1.069881, acc: 97%] [G loss: 1.160311]\n",
      "[Epoch 4/200] [Batch 332/938] [D loss: 1.093729, acc: 97%] [G loss: 1.112946]\n",
      "[Epoch 4/200] [Batch 333/938] [D loss: 1.085806, acc: 95%] [G loss: 1.065604]\n",
      "[Epoch 4/200] [Batch 334/938] [D loss: 1.083786, acc: 96%] [G loss: 1.179639]\n",
      "[Epoch 4/200] [Batch 335/938] [D loss: 1.111649, acc: 92%] [G loss: 1.084831]\n",
      "[Epoch 4/200] [Batch 336/938] [D loss: 1.116313, acc: 96%] [G loss: 1.154884]\n",
      "[Epoch 4/200] [Batch 337/938] [D loss: 1.063135, acc: 95%] [G loss: 1.213274]\n",
      "[Epoch 4/200] [Batch 338/938] [D loss: 1.082752, acc: 96%] [G loss: 1.162216]\n",
      "[Epoch 4/200] [Batch 339/938] [D loss: 1.106410, acc: 93%] [G loss: 1.149860]\n",
      "[Epoch 4/200] [Batch 340/938] [D loss: 1.112256, acc: 94%] [G loss: 1.107075]\n",
      "[Epoch 4/200] [Batch 341/938] [D loss: 1.077524, acc: 97%] [G loss: 1.112099]\n",
      "[Epoch 4/200] [Batch 342/938] [D loss: 1.141573, acc: 88%] [G loss: 1.128179]\n",
      "[Epoch 4/200] [Batch 343/938] [D loss: 1.069941, acc: 94%] [G loss: 1.090058]\n",
      "[Epoch 4/200] [Batch 344/938] [D loss: 1.143944, acc: 92%] [G loss: 1.100597]\n",
      "[Epoch 4/200] [Batch 345/938] [D loss: 1.080905, acc: 96%] [G loss: 1.134485]\n",
      "[Epoch 4/200] [Batch 346/938] [D loss: 1.101705, acc: 96%] [G loss: 1.132840]\n",
      "[Epoch 4/200] [Batch 347/938] [D loss: 1.102959, acc: 96%] [G loss: 1.121078]\n",
      "[Epoch 4/200] [Batch 348/938] [D loss: 1.075717, acc: 96%] [G loss: 1.230006]\n",
      "[Epoch 4/200] [Batch 349/938] [D loss: 1.098183, acc: 93%] [G loss: 1.107439]\n",
      "[Epoch 4/200] [Batch 350/938] [D loss: 1.098796, acc: 94%] [G loss: 1.102245]\n",
      "[Epoch 4/200] [Batch 351/938] [D loss: 1.059224, acc: 95%] [G loss: 1.155468]\n",
      "[Epoch 4/200] [Batch 352/938] [D loss: 1.114672, acc: 94%] [G loss: 1.149478]\n",
      "[Epoch 4/200] [Batch 353/938] [D loss: 1.122288, acc: 94%] [G loss: 1.120468]\n",
      "[Epoch 4/200] [Batch 354/938] [D loss: 1.095775, acc: 95%] [G loss: 1.113674]\n",
      "[Epoch 4/200] [Batch 355/938] [D loss: 1.073577, acc: 92%] [G loss: 1.149840]\n",
      "[Epoch 4/200] [Batch 356/938] [D loss: 1.119656, acc: 93%] [G loss: 1.148337]\n",
      "[Epoch 4/200] [Batch 357/938] [D loss: 1.068218, acc: 94%] [G loss: 1.134725]\n",
      "[Epoch 4/200] [Batch 358/938] [D loss: 1.111367, acc: 92%] [G loss: 1.123500]\n",
      "[Epoch 4/200] [Batch 359/938] [D loss: 1.075592, acc: 94%] [G loss: 1.220478]\n",
      "[Epoch 4/200] [Batch 360/938] [D loss: 1.091476, acc: 95%] [G loss: 1.200129]\n",
      "[Epoch 4/200] [Batch 361/938] [D loss: 1.106599, acc: 96%] [G loss: 1.192537]\n",
      "[Epoch 4/200] [Batch 362/938] [D loss: 1.094877, acc: 91%] [G loss: 1.109203]\n",
      "[Epoch 4/200] [Batch 363/938] [D loss: 1.102784, acc: 95%] [G loss: 1.103606]\n",
      "[Epoch 4/200] [Batch 364/938] [D loss: 1.094985, acc: 96%] [G loss: 1.087474]\n",
      "[Epoch 4/200] [Batch 365/938] [D loss: 1.088054, acc: 95%] [G loss: 1.162766]\n",
      "[Epoch 4/200] [Batch 366/938] [D loss: 1.060073, acc: 96%] [G loss: 1.151353]\n",
      "[Epoch 4/200] [Batch 367/938] [D loss: 1.105717, acc: 96%] [G loss: 1.171140]\n",
      "[Epoch 4/200] [Batch 368/938] [D loss: 1.061708, acc: 92%] [G loss: 1.161323]\n",
      "[Epoch 4/200] [Batch 369/938] [D loss: 1.100195, acc: 96%] [G loss: 1.150062]\n",
      "[Epoch 4/200] [Batch 370/938] [D loss: 1.055911, acc: 100%] [G loss: 1.074038]\n",
      "[Epoch 4/200] [Batch 371/938] [D loss: 1.130915, acc: 92%] [G loss: 1.093491]\n",
      "[Epoch 4/200] [Batch 372/938] [D loss: 1.113022, acc: 94%] [G loss: 1.139211]\n",
      "[Epoch 4/200] [Batch 373/938] [D loss: 1.093132, acc: 96%] [G loss: 1.122006]\n",
      "[Epoch 4/200] [Batch 374/938] [D loss: 1.051451, acc: 92%] [G loss: 1.135852]\n",
      "[Epoch 4/200] [Batch 375/938] [D loss: 1.062752, acc: 98%] [G loss: 1.225527]\n",
      "[Epoch 4/200] [Batch 376/938] [D loss: 1.122947, acc: 97%] [G loss: 1.159035]\n",
      "[Epoch 4/200] [Batch 377/938] [D loss: 1.055094, acc: 97%] [G loss: 1.069117]\n",
      "[Epoch 4/200] [Batch 378/938] [D loss: 1.126288, acc: 91%] [G loss: 1.088535]\n",
      "[Epoch 4/200] [Batch 379/938] [D loss: 1.128251, acc: 93%] [G loss: 1.103175]\n",
      "[Epoch 4/200] [Batch 380/938] [D loss: 1.094273, acc: 94%] [G loss: 1.141415]\n",
      "[Epoch 4/200] [Batch 381/938] [D loss: 1.054505, acc: 94%] [G loss: 1.218134]\n",
      "[Epoch 4/200] [Batch 382/938] [D loss: 1.099599, acc: 96%] [G loss: 1.096903]\n",
      "[Epoch 4/200] [Batch 383/938] [D loss: 1.075939, acc: 93%] [G loss: 1.098697]\n",
      "[Epoch 4/200] [Batch 384/938] [D loss: 1.084071, acc: 94%] [G loss: 1.109074]\n",
      "[Epoch 4/200] [Batch 385/938] [D loss: 1.105785, acc: 95%] [G loss: 1.112694]\n",
      "[Epoch 4/200] [Batch 386/938] [D loss: 1.066193, acc: 95%] [G loss: 1.128267]\n",
      "[Epoch 4/200] [Batch 387/938] [D loss: 1.079481, acc: 96%] [G loss: 1.172428]\n",
      "[Epoch 4/200] [Batch 388/938] [D loss: 1.111998, acc: 93%] [G loss: 1.117144]\n",
      "[Epoch 4/200] [Batch 389/938] [D loss: 1.059261, acc: 98%] [G loss: 1.099474]\n",
      "[Epoch 4/200] [Batch 390/938] [D loss: 1.075384, acc: 96%] [G loss: 1.081729]\n",
      "[Epoch 4/200] [Batch 391/938] [D loss: 1.100176, acc: 89%] [G loss: 1.152773]\n",
      "[Epoch 4/200] [Batch 392/938] [D loss: 1.083160, acc: 96%] [G loss: 1.166309]\n",
      "[Epoch 4/200] [Batch 393/938] [D loss: 1.110707, acc: 95%] [G loss: 1.139080]\n",
      "[Epoch 4/200] [Batch 394/938] [D loss: 1.120258, acc: 91%] [G loss: 1.150668]\n",
      "[Epoch 4/200] [Batch 395/938] [D loss: 1.093914, acc: 94%] [G loss: 1.126252]\n",
      "[Epoch 4/200] [Batch 396/938] [D loss: 1.086656, acc: 95%] [G loss: 1.132332]\n",
      "[Epoch 4/200] [Batch 397/938] [D loss: 1.058222, acc: 93%] [G loss: 1.162040]\n",
      "[Epoch 4/200] [Batch 398/938] [D loss: 1.071723, acc: 97%] [G loss: 1.183895]\n",
      "[Epoch 4/200] [Batch 399/938] [D loss: 1.069088, acc: 96%] [G loss: 1.084865]\n",
      "[Epoch 4/200] [Batch 400/938] [D loss: 1.061194, acc: 92%] [G loss: 1.130617]\n",
      "[Epoch 4/200] [Batch 401/938] [D loss: 1.113716, acc: 95%] [G loss: 1.169529]\n",
      "[Epoch 4/200] [Batch 402/938] [D loss: 1.081465, acc: 97%] [G loss: 1.163987]\n",
      "[Epoch 4/200] [Batch 403/938] [D loss: 1.075310, acc: 95%] [G loss: 1.160721]\n",
      "[Epoch 4/200] [Batch 404/938] [D loss: 1.123943, acc: 93%] [G loss: 1.107870]\n",
      "[Epoch 4/200] [Batch 405/938] [D loss: 1.115199, acc: 92%] [G loss: 1.129236]\n",
      "[Epoch 4/200] [Batch 406/938] [D loss: 1.123145, acc: 96%] [G loss: 1.109308]\n",
      "[Epoch 4/200] [Batch 407/938] [D loss: 1.106508, acc: 96%] [G loss: 1.168119]\n",
      "[Epoch 4/200] [Batch 408/938] [D loss: 1.087012, acc: 92%] [G loss: 1.139928]\n",
      "[Epoch 4/200] [Batch 409/938] [D loss: 1.059791, acc: 92%] [G loss: 1.177137]\n",
      "[Epoch 4/200] [Batch 410/938] [D loss: 1.090104, acc: 93%] [G loss: 1.200830]\n",
      "[Epoch 4/200] [Batch 411/938] [D loss: 1.117114, acc: 95%] [G loss: 1.111382]\n",
      "[Epoch 4/200] [Batch 412/938] [D loss: 1.046991, acc: 96%] [G loss: 1.080365]\n",
      "[Epoch 4/200] [Batch 413/938] [D loss: 1.097288, acc: 95%] [G loss: 1.103481]\n",
      "[Epoch 4/200] [Batch 414/938] [D loss: 1.065461, acc: 96%] [G loss: 1.099287]\n",
      "[Epoch 4/200] [Batch 415/938] [D loss: 1.108258, acc: 96%] [G loss: 1.091786]\n",
      "[Epoch 4/200] [Batch 416/938] [D loss: 1.078480, acc: 94%] [G loss: 1.203540]\n",
      "[Epoch 4/200] [Batch 417/938] [D loss: 1.088331, acc: 92%] [G loss: 1.117470]\n",
      "[Epoch 4/200] [Batch 418/938] [D loss: 1.097065, acc: 96%] [G loss: 1.134460]\n",
      "[Epoch 4/200] [Batch 419/938] [D loss: 1.091926, acc: 96%] [G loss: 1.117581]\n",
      "[Epoch 4/200] [Batch 420/938] [D loss: 1.067272, acc: 98%] [G loss: 1.127295]\n",
      "[Epoch 4/200] [Batch 421/938] [D loss: 1.111615, acc: 94%] [G loss: 1.154467]\n",
      "[Epoch 4/200] [Batch 422/938] [D loss: 1.064084, acc: 96%] [G loss: 1.147513]\n",
      "[Epoch 4/200] [Batch 423/938] [D loss: 1.106612, acc: 95%] [G loss: 1.055623]\n",
      "[Epoch 4/200] [Batch 424/938] [D loss: 1.129805, acc: 95%] [G loss: 1.146886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/200] [Batch 425/938] [D loss: 1.079505, acc: 96%] [G loss: 1.158518]\n",
      "[Epoch 4/200] [Batch 426/938] [D loss: 1.074367, acc: 98%] [G loss: 1.172156]\n",
      "[Epoch 4/200] [Batch 427/938] [D loss: 1.103593, acc: 92%] [G loss: 1.154557]\n",
      "[Epoch 4/200] [Batch 428/938] [D loss: 1.107198, acc: 96%] [G loss: 1.118174]\n",
      "[Epoch 4/200] [Batch 429/938] [D loss: 1.081930, acc: 96%] [G loss: 1.087414]\n",
      "[Epoch 4/200] [Batch 430/938] [D loss: 1.104280, acc: 92%] [G loss: 1.164209]\n",
      "[Epoch 4/200] [Batch 431/938] [D loss: 1.139342, acc: 93%] [G loss: 1.172876]\n",
      "[Epoch 4/200] [Batch 432/938] [D loss: 1.051584, acc: 96%] [G loss: 1.077385]\n",
      "[Epoch 4/200] [Batch 433/938] [D loss: 1.070190, acc: 98%] [G loss: 1.137451]\n",
      "[Epoch 4/200] [Batch 434/938] [D loss: 1.106651, acc: 95%] [G loss: 1.102748]\n",
      "[Epoch 4/200] [Batch 435/938] [D loss: 1.081700, acc: 94%] [G loss: 1.126190]\n",
      "[Epoch 4/200] [Batch 436/938] [D loss: 1.068545, acc: 96%] [G loss: 1.090822]\n",
      "[Epoch 4/200] [Batch 437/938] [D loss: 1.113548, acc: 96%] [G loss: 1.096537]\n",
      "[Epoch 4/200] [Batch 438/938] [D loss: 1.117667, acc: 92%] [G loss: 1.149850]\n",
      "[Epoch 4/200] [Batch 439/938] [D loss: 1.109777, acc: 96%] [G loss: 1.170954]\n",
      "[Epoch 4/200] [Batch 440/938] [D loss: 1.085302, acc: 95%] [G loss: 1.120149]\n",
      "[Epoch 4/200] [Batch 441/938] [D loss: 1.067024, acc: 97%] [G loss: 1.211974]\n",
      "[Epoch 4/200] [Batch 442/938] [D loss: 1.091585, acc: 95%] [G loss: 1.129703]\n",
      "[Epoch 4/200] [Batch 443/938] [D loss: 1.108374, acc: 96%] [G loss: 1.075396]\n",
      "[Epoch 4/200] [Batch 444/938] [D loss: 1.082538, acc: 96%] [G loss: 1.154044]\n",
      "[Epoch 4/200] [Batch 445/938] [D loss: 1.098171, acc: 95%] [G loss: 1.099297]\n",
      "[Epoch 4/200] [Batch 446/938] [D loss: 1.091427, acc: 96%] [G loss: 1.119409]\n",
      "[Epoch 4/200] [Batch 447/938] [D loss: 1.044905, acc: 97%] [G loss: 1.108389]\n",
      "[Epoch 4/200] [Batch 448/938] [D loss: 1.066779, acc: 93%] [G loss: 1.092693]\n",
      "[Epoch 4/200] [Batch 449/938] [D loss: 1.140288, acc: 95%] [G loss: 1.079378]\n",
      "[Epoch 4/200] [Batch 450/938] [D loss: 1.087299, acc: 94%] [G loss: 1.104701]\n",
      "[Epoch 4/200] [Batch 451/938] [D loss: 1.089241, acc: 94%] [G loss: 1.180066]\n",
      "[Epoch 4/200] [Batch 452/938] [D loss: 1.098272, acc: 96%] [G loss: 1.159944]\n",
      "[Epoch 4/200] [Batch 453/938] [D loss: 1.077532, acc: 96%] [G loss: 1.158990]\n",
      "[Epoch 4/200] [Batch 454/938] [D loss: 1.088976, acc: 96%] [G loss: 1.184435]\n",
      "[Epoch 4/200] [Batch 455/938] [D loss: 1.073812, acc: 96%] [G loss: 1.132088]\n",
      "[Epoch 4/200] [Batch 456/938] [D loss: 1.097570, acc: 96%] [G loss: 1.150298]\n",
      "[Epoch 4/200] [Batch 457/938] [D loss: 1.058689, acc: 96%] [G loss: 1.142252]\n",
      "[Epoch 4/200] [Batch 458/938] [D loss: 1.091498, acc: 96%] [G loss: 1.161069]\n",
      "[Epoch 4/200] [Batch 459/938] [D loss: 1.106155, acc: 95%] [G loss: 1.170885]\n",
      "[Epoch 4/200] [Batch 460/938] [D loss: 1.084155, acc: 94%] [G loss: 1.098971]\n",
      "[Epoch 4/200] [Batch 461/938] [D loss: 1.070776, acc: 96%] [G loss: 1.139173]\n",
      "[Epoch 4/200] [Batch 462/938] [D loss: 1.075963, acc: 95%] [G loss: 1.219851]\n",
      "[Epoch 4/200] [Batch 463/938] [D loss: 1.102154, acc: 93%] [G loss: 1.163123]\n",
      "[Epoch 4/200] [Batch 464/938] [D loss: 1.131398, acc: 93%] [G loss: 1.139026]\n",
      "[Epoch 4/200] [Batch 465/938] [D loss: 1.069996, acc: 94%] [G loss: 1.149785]\n",
      "[Epoch 4/200] [Batch 466/938] [D loss: 1.076885, acc: 95%] [G loss: 1.092210]\n",
      "[Epoch 4/200] [Batch 467/938] [D loss: 1.093399, acc: 96%] [G loss: 1.178673]\n",
      "[Epoch 4/200] [Batch 468/938] [D loss: 1.106296, acc: 92%] [G loss: 1.190100]\n",
      "[Epoch 4/200] [Batch 469/938] [D loss: 1.112736, acc: 96%] [G loss: 1.185837]\n",
      "[Epoch 4/200] [Batch 470/938] [D loss: 1.099332, acc: 94%] [G loss: 1.080803]\n",
      "[Epoch 4/200] [Batch 471/938] [D loss: 1.114695, acc: 96%] [G loss: 1.108910]\n",
      "[Epoch 4/200] [Batch 472/938] [D loss: 1.074417, acc: 94%] [G loss: 1.229711]\n",
      "[Epoch 4/200] [Batch 473/938] [D loss: 1.081470, acc: 96%] [G loss: 1.105417]\n",
      "[Epoch 4/200] [Batch 474/938] [D loss: 1.073738, acc: 96%] [G loss: 1.128312]\n",
      "[Epoch 4/200] [Batch 475/938] [D loss: 1.057770, acc: 96%] [G loss: 1.180391]\n",
      "[Epoch 4/200] [Batch 476/938] [D loss: 1.079440, acc: 95%] [G loss: 1.214612]\n",
      "[Epoch 4/200] [Batch 477/938] [D loss: 1.048466, acc: 96%] [G loss: 1.100946]\n",
      "[Epoch 4/200] [Batch 478/938] [D loss: 1.073030, acc: 94%] [G loss: 1.176300]\n",
      "[Epoch 4/200] [Batch 479/938] [D loss: 1.077574, acc: 98%] [G loss: 1.122196]\n",
      "[Epoch 4/200] [Batch 480/938] [D loss: 1.108466, acc: 95%] [G loss: 1.134988]\n",
      "[Epoch 4/200] [Batch 481/938] [D loss: 1.065159, acc: 95%] [G loss: 1.102145]\n",
      "[Epoch 4/200] [Batch 482/938] [D loss: 1.078436, acc: 96%] [G loss: 1.063551]\n",
      "[Epoch 4/200] [Batch 483/938] [D loss: 1.088486, acc: 96%] [G loss: 1.191799]\n",
      "[Epoch 4/200] [Batch 484/938] [D loss: 1.048347, acc: 93%] [G loss: 1.154992]\n",
      "[Epoch 4/200] [Batch 485/938] [D loss: 1.124838, acc: 95%] [G loss: 1.141275]\n",
      "[Epoch 4/200] [Batch 486/938] [D loss: 1.072948, acc: 94%] [G loss: 1.126675]\n",
      "[Epoch 4/200] [Batch 487/938] [D loss: 1.067341, acc: 97%] [G loss: 1.141280]\n",
      "[Epoch 4/200] [Batch 488/938] [D loss: 1.123814, acc: 93%] [G loss: 1.197278]\n",
      "[Epoch 4/200] [Batch 489/938] [D loss: 1.104874, acc: 96%] [G loss: 1.139631]\n",
      "[Epoch 4/200] [Batch 490/938] [D loss: 1.131024, acc: 94%] [G loss: 1.105889]\n",
      "[Epoch 4/200] [Batch 491/938] [D loss: 1.104488, acc: 99%] [G loss: 1.086670]\n",
      "[Epoch 4/200] [Batch 492/938] [D loss: 1.070794, acc: 96%] [G loss: 1.144066]\n",
      "[Epoch 4/200] [Batch 493/938] [D loss: 1.109840, acc: 95%] [G loss: 1.135605]\n",
      "[Epoch 4/200] [Batch 494/938] [D loss: 1.063907, acc: 95%] [G loss: 1.200958]\n",
      "[Epoch 4/200] [Batch 495/938] [D loss: 1.084761, acc: 97%] [G loss: 1.137332]\n",
      "[Epoch 4/200] [Batch 496/938] [D loss: 1.070491, acc: 98%] [G loss: 1.183343]\n",
      "[Epoch 4/200] [Batch 497/938] [D loss: 1.056368, acc: 96%] [G loss: 1.209021]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-87c4352fa44e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mauxiliary_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "n_cpu = 8\n",
    "latent_dim = 100\n",
    "n_classes = 10\n",
    "img_size = 32\n",
    "channels = 1\n",
    "sample_interval = 400\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(n_classes, latent_dim)\n",
    "\n",
    "        self.init_size = img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "\n",
    "    \n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(torch.FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(torch.LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(torch.FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(torch.FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(torch.FloatTensor))\n",
    "        labels = Variable(labels.type(torch.LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_labels = Variable(torch.LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs)\n",
    "        g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        \n",
    "        G_losses.append(g_loss.item())\n",
    "        D_losses.append(d_loss.item())\n",
    "        \n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, g_loss.item())\n",
    "        )\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
