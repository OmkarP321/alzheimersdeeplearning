{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = torch.load(\"../64torchiodataset.pt\")\n",
    "\n",
    "\n",
    "lengths = [\n",
    "    int(len(dataset) * 0.8),\n",
    "    int(len(dataset) * 0.1),\n",
    "    int(len(dataset) * 0.1) + 1\n",
    "]\n",
    "\n",
    "# trainset, valset, testset = random_split(dataset, lengths)\n",
    "# image_datasets = {'train': trainset, 'val': valset, 'test': testset}\n",
    "# dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "#               for x in ['train', 'val', 'test']}\n",
    "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}  \n",
    "\n",
    "\n",
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "NC = [sample[0] for sample in NC]\n",
    "NCgan1 = [torch.unsqueeze(sample[0][0], 0) for sample in NC]\n",
    "NCgan2 = [torch.unsqueeze(sample[1][0], 0) for sample in NC]\n",
    "NCgan3 = [torch.unsqueeze(sample[2][0], 0) for sample in NC]\n",
    "\n",
    "AD = [sample[0] for sample in AD]\n",
    "ADgan1 = [torch.unsqueeze(sample[0][0], 0) for sample in AD]\n",
    "ADgan2 = [torch.unsqueeze(sample[1][0], 0) for sample in AD]\n",
    "ADgan3 = [torch.unsqueeze(sample[2][0], 0) for sample in AD]\n",
    "\n",
    "gan1 = []\n",
    "for i in range(len(ADgan1)):\n",
    "    gan1.append({\"A\": NCgan1[i], \"B\": ADgan1[i]})\n",
    "\n",
    "gan2 = []\n",
    "for i in range(len(ADgan2)):\n",
    "    gan2.append({\"A\": NCgan2[i], \"B\": ADgan2[i]})\n",
    "    \n",
    "gan3 = []\n",
    "for i in range(len(ADgan3)):\n",
    "    gan3.append({\"A\": NCgan3[i], \"B\": ADgan3[i]})\n",
    "    \n",
    "batch_size = 1\n",
    "dataloader1 = DataLoader(gan1, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader2 = DataLoader(gan2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader3 = DataLoader(gan3, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))\n",
    "\n",
    "\n",
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "##############################\n",
    "#           RESNET\n",
    "##############################\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.ReflectionPad2d(3), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "##############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 200\n",
    "dataset_name = 'CycleGAN'\n",
    "batch_size = 1\n",
    "lr=0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "n_cpu = 8\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "channels = 1\n",
    "sample_interval = 100\n",
    "checkpoint_interval = 100\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "\n",
    "# Create sample and checkpoint directories\n",
    "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_images(batches_done, images):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(dataloader3))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=4, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=4, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=4, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=4, normalize=True)\n",
    "\n",
    "    image_grid = torch.stack((real_A, fake_B, real_B, fake_A), 0)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n",
    "    images.append(fake_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/476] [D loss: 1.501719] [G loss: 18.543667, adv: 1.582060, cycle: 1.134011, identity: 1.124300] ETA: 4:12:17.791061\n",
      "[Epoch 0/200] [Batch 1/476] [D loss: 0.949080] [G loss: 16.386539, adv: 1.299144, cycle: 1.012905, identity: 0.991669] ETA: 7:16:09.497637\n",
      "[Epoch 0/200] [Batch 2/476] [D loss: 1.178904] [G loss: 14.796091, adv: 0.874161, cycle: 0.942109, identity: 0.900167] ETA: 1:54:18.548270\n",
      "[Epoch 0/200] [Batch 3/476] [D loss: 1.328289] [G loss: 15.049623, adv: 1.293544, cycle: 0.920362, identity: 0.910493] ETA: 2:08:57.180261\n",
      "[Epoch 0/200] [Batch 4/476] [D loss: 1.335779] [G loss: 15.521496, adv: 1.337486, cycle: 0.959807, identity: 0.917188] ETA: 2:13:52.130725\n",
      "[Epoch 0/200] [Batch 5/476] [D loss: 1.566680] [G loss: 14.951328, adv: 1.298429, cycle: 0.922444, identity: 0.885692] ETA: 2:26:08.766841\n",
      "[Epoch 0/200] [Batch 6/476] [D loss: 1.333152] [G loss: 15.137934, adv: 2.075458, cycle: 0.871168, identity: 0.870160] ETA: 2:28:36.244237\n",
      "[Epoch 0/200] [Batch 7/476] [D loss: 1.454586] [G loss: 14.630297, adv: 1.563256, cycle: 0.877584, identity: 0.858239] ETA: 2:09:26.359673\n",
      "[Epoch 0/200] [Batch 8/476] [D loss: 1.323660] [G loss: 15.140553, adv: 1.779292, cycle: 0.901593, identity: 0.869066] ETA: 2:17:48.121899\n",
      "[Epoch 0/200] [Batch 9/476] [D loss: 1.668686] [G loss: 16.095467, adv: 2.976444, cycle: 0.884380, identity: 0.855046] ETA: 2:40:40.624269\n",
      "[Epoch 0/200] [Batch 10/476] [D loss: 1.493311] [G loss: 16.433769, adv: 2.233699, cycle: 0.958291, identity: 0.923431] ETA: 2:32:38.457127\n",
      "[Epoch 0/200] [Batch 11/476] [D loss: 1.451159] [G loss: 14.807091, adv: 1.768452, cycle: 0.884255, identity: 0.839218] ETA: 2:07:00.446022\n",
      "[Epoch 0/200] [Batch 12/476] [D loss: 1.184625] [G loss: 15.509354, adv: 2.139957, cycle: 0.901565, identity: 0.870749] ETA: 1:48:11.900287\n",
      "[Epoch 0/200] [Batch 13/476] [D loss: 1.226341] [G loss: 15.561531, adv: 1.474187, cycle: 0.959146, identity: 0.899177] ETA: 2:24:52.389665\n",
      "[Epoch 0/200] [Batch 14/476] [D loss: 1.451492] [G loss: 16.359148, adv: 2.810312, cycle: 0.919697, identity: 0.870374] ETA: 2:32:07.412533\n",
      "[Epoch 0/200] [Batch 15/476] [D loss: 1.441332] [G loss: 13.885548, adv: 1.035837, cycle: 0.861256, identity: 0.847431] ETA: 2:35:47.492594\n",
      "[Epoch 0/200] [Batch 16/476] [D loss: 0.889299] [G loss: 14.211376, adv: 0.968765, cycle: 0.881087, identity: 0.886349] ETA: 2:14:43.949009\n",
      "[Epoch 0/200] [Batch 17/476] [D loss: 1.330482] [G loss: 14.933400, adv: 2.477897, cycle: 0.831988, identity: 0.827125] ETA: 2:12:59.565233\n",
      "[Epoch 0/200] [Batch 18/476] [D loss: 1.013468] [G loss: 14.407331, adv: 1.446391, cycle: 0.860020, identity: 0.872147] ETA: 2:25:34.686977\n",
      "[Epoch 0/200] [Batch 19/476] [D loss: 1.563228] [G loss: 15.260987, adv: 1.967585, cycle: 0.886472, identity: 0.885737] ETA: 2:24:43.831150\n",
      "[Epoch 0/200] [Batch 20/476] [D loss: 1.023036] [G loss: 14.719901, adv: 1.792649, cycle: 0.863935, identity: 0.857580] ETA: 2:33:57.078233\n",
      "[Epoch 0/200] [Batch 21/476] [D loss: 0.945907] [G loss: 14.307903, adv: 1.576241, cycle: 0.846718, identity: 0.852896] ETA: 2:23:52.658763\n",
      "[Epoch 0/200] [Batch 22/476] [D loss: 1.201035] [G loss: 14.058818, adv: 1.695911, cycle: 0.833835, identity: 0.804912] ETA: 2:28:51.810152\n",
      "[Epoch 0/200] [Batch 23/476] [D loss: 0.889567] [G loss: 13.962301, adv: 1.340988, cycle: 0.838919, identity: 0.846424] ETA: 2:53:55.081688\n",
      "[Epoch 0/200] [Batch 24/476] [D loss: 1.108612] [G loss: 13.909070, adv: 1.733431, cycle: 0.821552, identity: 0.792024] ETA: 2:26:14.913401\n",
      "[Epoch 0/200] [Batch 25/476] [D loss: 0.891666] [G loss: 14.170221, adv: 1.421935, cycle: 0.850214, identity: 0.849229] ETA: 2:24:42.557613\n",
      "[Epoch 0/200] [Batch 26/476] [D loss: 1.005921] [G loss: 15.063793, adv: 1.846390, cycle: 0.887300, identity: 0.868882] ETA: 2:33:30.809447\n",
      "[Epoch 0/200] [Batch 27/476] [D loss: 0.864676] [G loss: 13.540903, adv: 1.451325, cycle: 0.814767, identity: 0.788382] ETA: 2:21:06.674403\n",
      "[Epoch 0/200] [Batch 28/476] [D loss: 1.254863] [G loss: 15.066758, adv: 2.037699, cycle: 0.881381, identity: 0.843049] ETA: 2:14:02.857947\n",
      "[Epoch 0/200] [Batch 29/476] [D loss: 1.406092] [G loss: 15.317527, adv: 1.900907, cycle: 0.899112, identity: 0.885099] ETA: 2:11:45.427633\n",
      "[Epoch 0/200] [Batch 30/476] [D loss: 1.108528] [G loss: 14.408322, adv: 1.525895, cycle: 0.864818, identity: 0.846849] ETA: 2:11:44.141982\n",
      "[Epoch 0/200] [Batch 31/476] [D loss: 1.900324] [G loss: 15.070587, adv: 2.203712, cycle: 0.862567, identity: 0.848241] ETA: 2:17:42.788742\n",
      "[Epoch 0/200] [Batch 32/476] [D loss: 2.069250] [G loss: 14.754953, adv: 2.995410, cycle: 0.795415, identity: 0.761079] ETA: 2:23:17.036407\n",
      "[Epoch 0/200] [Batch 33/476] [D loss: 1.196420] [G loss: 13.296614, adv: 1.187455, cycle: 0.810785, identity: 0.800262] ETA: 2:07:22.599609\n",
      "[Epoch 0/200] [Batch 34/476] [D loss: 0.909651] [G loss: 14.049833, adv: 1.556074, cycle: 0.837824, identity: 0.823104] ETA: 2:07:21.997447\n",
      "[Epoch 0/200] [Batch 35/476] [D loss: 0.458489] [G loss: 13.349968, adv: 0.814785, cycle: 0.835804, identity: 0.835428] ETA: 2:27:24.870754\n",
      "[Epoch 0/200] [Batch 36/476] [D loss: 1.861699] [G loss: 13.158468, adv: 1.296636, cycle: 0.797210, identity: 0.777946] ETA: 2:26:36.200951\n",
      "[Epoch 0/200] [Batch 37/476] [D loss: 0.842275] [G loss: 13.642633, adv: 1.338711, cycle: 0.819776, identity: 0.821232] ETA: 2:31:04.106584\n",
      "[Epoch 0/200] [Batch 38/476] [D loss: 0.901560] [G loss: 14.509804, adv: 1.236512, cycle: 0.878809, identity: 0.897041] ETA: 2:17:26.321802\n",
      "[Epoch 0/200] [Batch 39/476] [D loss: 0.993629] [G loss: 14.883354, adv: 1.188850, cycle: 0.913037, identity: 0.912826] ETA: 2:15:54.438890\n",
      "[Epoch 0/200] [Batch 40/476] [D loss: 1.124124] [G loss: 13.102771, adv: 1.306758, cycle: 0.787999, identity: 0.783205] ETA: 2:30:17.968569\n",
      "[Epoch 0/200] [Batch 41/476] [D loss: 0.838611] [G loss: 13.720140, adv: 1.577294, cycle: 0.806013, identity: 0.816544] ETA: 2:23:50.890154\n",
      "[Epoch 0/200] [Batch 42/476] [D loss: 0.961806] [G loss: 13.258076, adv: 1.358063, cycle: 0.797611, identity: 0.784780] ETA: 2:12:35.417198\n",
      "[Epoch 0/200] [Batch 43/476] [D loss: 0.813863] [G loss: 12.364193, adv: 0.855512, cycle: 0.771770, identity: 0.758196] ETA: 2:08:01.045387\n",
      "[Epoch 0/200] [Batch 44/476] [D loss: 0.686249] [G loss: 13.109430, adv: 1.040830, cycle: 0.809482, identity: 0.794757] ETA: 2:21:29.958914\n",
      "[Epoch 0/200] [Batch 45/476] [D loss: 0.655936] [G loss: 12.659335, adv: 0.917733, cycle: 0.787014, identity: 0.774291] ETA: 2:30:03.156730\n",
      "[Epoch 0/200] [Batch 46/476] [D loss: 0.809400] [G loss: 13.663496, adv: 0.936296, cycle: 0.852815, identity: 0.839811] ETA: 2:08:50.622741\n",
      "[Epoch 0/200] [Batch 47/476] [D loss: 0.830519] [G loss: 13.056850, adv: 1.375952, cycle: 0.780043, identity: 0.776094] ETA: 2:10:56.223284\n",
      "[Epoch 0/200] [Batch 48/476] [D loss: 1.103448] [G loss: 13.003632, adv: 1.347679, cycle: 0.779543, identity: 0.772105] ETA: 2:23:04.179981\n",
      "[Epoch 0/200] [Batch 49/476] [D loss: 0.783760] [G loss: 12.731962, adv: 0.691474, cycle: 0.804860, identity: 0.798377] ETA: 2:32:11.315819\n",
      "[Epoch 0/200] [Batch 50/476] [D loss: 0.691052] [G loss: 12.229933, adv: 0.703413, cycle: 0.766472, identity: 0.772361] ETA: 2:05:46.113968\n",
      "[Epoch 0/200] [Batch 51/476] [D loss: 0.733382] [G loss: 12.788956, adv: 1.156532, cycle: 0.775799, identity: 0.774887] ETA: 1:55:50.523129\n",
      "[Epoch 0/200] [Batch 52/476] [D loss: 0.933059] [G loss: 13.704411, adv: 1.565121, cycle: 0.810347, identity: 0.807163] ETA: 1:55:13.700297\n",
      "[Epoch 0/200] [Batch 53/476] [D loss: 0.903133] [G loss: 13.045343, adv: 0.798500, cycle: 0.821477, identity: 0.806416] ETA: 2:04:27.386594\n",
      "[Epoch 0/200] [Batch 54/476] [D loss: 0.621027] [G loss: 13.199051, adv: 0.921570, cycle: 0.819761, identity: 0.815974] ETA: 1:52:33.265771\n",
      "[Epoch 0/200] [Batch 55/476] [D loss: 0.604211] [G loss: 11.710379, adv: 0.769113, cycle: 0.729548, identity: 0.729158] ETA: 1:51:03.115296\n",
      "[Epoch 0/200] [Batch 56/476] [D loss: 0.468755] [G loss: 14.386976, adv: 0.971359, cycle: 0.898407, identity: 0.886311] ETA: 1:55:55.624752\n",
      "[Epoch 0/200] [Batch 57/476] [D loss: 0.456192] [G loss: 13.019524, adv: 1.210762, cycle: 0.789129, identity: 0.783495] ETA: 2:00:29.232403\n",
      "[Epoch 0/200] [Batch 58/476] [D loss: 1.062623] [G loss: 12.790764, adv: 1.009508, cycle: 0.788618, identity: 0.779016] ETA: 2:04:13.338641\n",
      "[Epoch 0/200] [Batch 59/476] [D loss: 1.059492] [G loss: 13.638636, adv: 1.596292, cycle: 0.806193, identity: 0.796083] ETA: 2:03:08.453880\n",
      "[Epoch 0/200] [Batch 60/476] [D loss: 2.006072] [G loss: 12.592596, adv: 1.129201, cycle: 0.770962, identity: 0.750755] ETA: 2:04:35.774374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 61/476] [D loss: 0.857085] [G loss: 12.511961, adv: 0.598195, cycle: 0.794197, identity: 0.794359] ETA: 2:08:24.430213\n",
      "[Epoch 0/200] [Batch 62/476] [D loss: 0.761402] [G loss: 13.469864, adv: 0.930161, cycle: 0.839690, identity: 0.828560] ETA: 1:58:32.240899\n",
      "[Epoch 0/200] [Batch 63/476] [D loss: 1.068980] [G loss: 14.176489, adv: 1.172626, cycle: 0.872244, identity: 0.856284] ETA: 1:55:17.800413\n",
      "[Epoch 0/200] [Batch 64/476] [D loss: 0.409445] [G loss: 13.028399, adv: 0.610654, cycle: 0.831173, identity: 0.821203] ETA: 1:58:03.738647\n",
      "[Epoch 0/200] [Batch 65/476] [D loss: 0.715007] [G loss: 13.476646, adv: 1.129915, cycle: 0.823764, identity: 0.821819] ETA: 2:11:03.764547\n",
      "[Epoch 0/200] [Batch 66/476] [D loss: 0.448818] [G loss: 12.557994, adv: 0.748142, cycle: 0.793723, identity: 0.774525] ETA: 1:50:22.062231\n",
      "[Epoch 0/200] [Batch 67/476] [D loss: 0.513179] [G loss: 13.069698, adv: 1.087348, cycle: 0.799966, identity: 0.796538] ETA: 2:04:43.865980\n",
      "[Epoch 0/200] [Batch 68/476] [D loss: 0.560975] [G loss: 11.817822, adv: 0.585676, cycle: 0.756673, identity: 0.733083] ETA: 2:02:35.116659\n",
      "[Epoch 0/200] [Batch 69/476] [D loss: 0.584500] [G loss: 12.309114, adv: 0.708492, cycle: 0.772982, identity: 0.774161] ETA: 2:00:55.220273\n",
      "[Epoch 0/200] [Batch 70/476] [D loss: 0.507487] [G loss: 13.137926, adv: 0.841367, cycle: 0.821002, identity: 0.817309] ETA: 2:01:38.645704\n",
      "[Epoch 0/200] [Batch 71/476] [D loss: 0.710961] [G loss: 12.725137, adv: 0.481060, cycle: 0.819332, identity: 0.810151] ETA: 1:55:14.224872\n",
      "[Epoch 0/200] [Batch 72/476] [D loss: 0.507737] [G loss: 13.090202, adv: 0.621127, cycle: 0.831940, identity: 0.829935] ETA: 2:00:17.274166\n",
      "[Epoch 0/200] [Batch 73/476] [D loss: 0.552370] [G loss: 12.857157, adv: 0.685941, cycle: 0.814134, identity: 0.805975] ETA: 2:14:17.425894\n",
      "[Epoch 0/200] [Batch 74/476] [D loss: 0.462106] [G loss: 12.448647, adv: 0.329124, cycle: 0.808958, identity: 0.805989] ETA: 2:40:22.089030\n",
      "[Epoch 0/200] [Batch 75/476] [D loss: 0.525951] [G loss: 12.189581, adv: 0.348866, cycle: 0.791733, identity: 0.784677] ETA: 2:15:45.842880\n",
      "[Epoch 0/200] [Batch 76/476] [D loss: 0.398961] [G loss: 11.955957, adv: 0.560354, cycle: 0.762314, identity: 0.754493] ETA: 2:05:28.630028\n",
      "[Epoch 0/200] [Batch 77/476] [D loss: 0.394354] [G loss: 11.705204, adv: 0.569069, cycle: 0.743581, identity: 0.740065] ETA: 2:02:19.384588\n",
      "[Epoch 0/200] [Batch 78/476] [D loss: 0.377665] [G loss: 12.392145, adv: 0.648827, cycle: 0.788102, identity: 0.772460] ETA: 2:05:27.700656\n",
      "[Epoch 0/200] [Batch 79/476] [D loss: 0.463024] [G loss: 11.873423, adv: 0.539438, cycle: 0.757213, identity: 0.752371] ETA: 2:03:58.721353\n",
      "[Epoch 0/200] [Batch 80/476] [D loss: 0.511321] [G loss: 12.109033, adv: 0.463090, cycle: 0.778558, identity: 0.772073] ETA: 2:18:05.000114\n",
      "[Epoch 0/200] [Batch 81/476] [D loss: 0.337573] [G loss: 12.755804, adv: 0.557180, cycle: 0.815187, identity: 0.809350] ETA: 2:07:14.934934\n",
      "[Epoch 0/200] [Batch 82/476] [D loss: 0.635921] [G loss: 11.711067, adv: 0.851164, cycle: 0.730479, identity: 0.711022] ETA: 2:02:49.613967\n",
      "[Epoch 0/200] [Batch 83/476] [D loss: 1.130383] [G loss: 11.997601, adv: 0.480862, cycle: 0.770793, identity: 0.761762] ETA: 1:57:33.863462\n",
      "[Epoch 0/200] [Batch 84/476] [D loss: 0.473835] [G loss: 13.130127, adv: 0.737748, cycle: 0.825776, identity: 0.826923] ETA: 1:58:15.243629\n",
      "[Epoch 0/200] [Batch 85/476] [D loss: 0.407347] [G loss: 10.982870, adv: 0.335888, cycle: 0.710189, identity: 0.709018] ETA: 2:12:13.884652\n",
      "[Epoch 0/200] [Batch 86/476] [D loss: 0.469711] [G loss: 12.091026, adv: 0.521068, cycle: 0.772166, identity: 0.769659] ETA: 2:01:18.074704\n",
      "[Epoch 0/200] [Batch 87/476] [D loss: 0.434283] [G loss: 11.975456, adv: 0.438081, cycle: 0.777652, identity: 0.752170] ETA: 1:58:21.845530\n",
      "[Epoch 0/200] [Batch 88/476] [D loss: 0.439557] [G loss: 11.831446, adv: 0.552835, cycle: 0.755704, identity: 0.744313] ETA: 2:01:39.101486\n",
      "[Epoch 0/200] [Batch 89/476] [D loss: 0.402470] [G loss: 12.457637, adv: 0.552206, cycle: 0.796144, identity: 0.788798] ETA: 1:54:43.006638\n",
      "[Epoch 0/200] [Batch 90/476] [D loss: 0.247463] [G loss: 11.544027, adv: 0.457851, cycle: 0.738665, identity: 0.739906] ETA: 2:00:18.811049\n",
      "[Epoch 0/200] [Batch 91/476] [D loss: 0.320358] [G loss: 11.925074, adv: 0.503859, cycle: 0.760947, identity: 0.762349] ETA: 2:04:10.231909\n",
      "[Epoch 0/200] [Batch 92/476] [D loss: 0.260585] [G loss: 11.914598, adv: 0.361151, cycle: 0.771486, identity: 0.767717] ETA: 2:03:58.770467\n",
      "[Epoch 0/200] [Batch 93/476] [D loss: 0.370534] [G loss: 11.189271, adv: 0.671861, cycle: 0.702945, identity: 0.697593] ETA: 1:58:35.569573\n",
      "[Epoch 0/200] [Batch 94/476] [D loss: 0.264239] [G loss: 11.541630, adv: 0.319569, cycle: 0.749793, identity: 0.744826] ETA: 2:05:01.582618\n",
      "[Epoch 0/200] [Batch 95/476] [D loss: 0.406229] [G loss: 11.426504, adv: 0.251821, cycle: 0.743351, identity: 0.748234] ETA: 1:59:24.488205\n",
      "[Epoch 0/200] [Batch 96/476] [D loss: 0.315573] [G loss: 11.972770, adv: 0.481836, cycle: 0.769322, identity: 0.759542] ETA: 1:59:01.851685\n",
      "[Epoch 0/200] [Batch 97/476] [D loss: 0.463418] [G loss: 12.384640, adv: 0.841842, cycle: 0.772102, identity: 0.764355] ETA: 2:00:45.874402\n",
      "[Epoch 0/200] [Batch 98/476] [D loss: 0.229445] [G loss: 11.675602, adv: 0.450794, cycle: 0.748012, identity: 0.748937] ETA: 2:00:40.583173\n",
      "[Epoch 0/200] [Batch 99/476] [D loss: 0.378336] [G loss: 12.498208, adv: 0.549227, cycle: 0.794844, identity: 0.800108] ETA: 1:52:33.971666\n",
      "[Epoch 0/200] [Batch 100/476] [D loss: 0.349151] [G loss: 10.914240, adv: 0.377400, cycle: 0.704888, identity: 0.697592] ETA: 1:59:06.811581\n",
      "[Epoch 0/200] [Batch 101/476] [D loss: 0.313841] [G loss: 11.205244, adv: 0.353809, cycle: 0.720380, identity: 0.729527] ETA: 5:28:52.088405\n",
      "[Epoch 0/200] [Batch 102/476] [D loss: 0.399435] [G loss: 12.065235, adv: 0.322886, cycle: 0.782727, identity: 0.783015] ETA: 2:01:36.349284\n",
      "[Epoch 0/200] [Batch 103/476] [D loss: 0.378116] [G loss: 12.886183, adv: 0.627997, cycle: 0.822835, identity: 0.805967] ETA: 2:02:34.972676\n",
      "[Epoch 0/200] [Batch 104/476] [D loss: 0.393335] [G loss: 12.057408, adv: 0.621466, cycle: 0.766610, identity: 0.753969] ETA: 2:04:55.873892\n",
      "[Epoch 0/200] [Batch 105/476] [D loss: 0.322460] [G loss: 11.477467, adv: 0.532124, cycle: 0.732532, identity: 0.724005] ETA: 2:11:24.581636\n",
      "[Epoch 0/200] [Batch 106/476] [D loss: 0.384901] [G loss: 11.147053, adv: 0.371556, cycle: 0.721364, identity: 0.712372] ETA: 2:20:56.154978\n",
      "[Epoch 0/200] [Batch 107/476] [D loss: 0.440948] [G loss: 11.792347, adv: 0.662657, cycle: 0.742512, identity: 0.740914] ETA: 2:17:59.701049\n",
      "[Epoch 0/200] [Batch 108/476] [D loss: 0.367935] [G loss: 11.136923, adv: 0.404879, cycle: 0.714669, identity: 0.717071] ETA: 2:16:43.323710\n",
      "[Epoch 0/200] [Batch 109/476] [D loss: 0.246183] [G loss: 11.375494, adv: 0.394888, cycle: 0.728505, identity: 0.739110] ETA: 2:18:24.805589\n",
      "[Epoch 0/200] [Batch 110/476] [D loss: 0.438891] [G loss: 11.273337, adv: 0.415013, cycle: 0.723660, identity: 0.724346] ETA: 2:06:57.485480\n",
      "[Epoch 0/200] [Batch 111/476] [D loss: 0.427609] [G loss: 11.890720, adv: 0.405697, cycle: 0.768498, identity: 0.760008] ETA: 1:57:26.618001\n",
      "[Epoch 0/200] [Batch 112/476] [D loss: 0.283570] [G loss: 11.379545, adv: 0.333046, cycle: 0.739623, identity: 0.730054] ETA: 1:58:57.068184\n",
      "[Epoch 0/200] [Batch 113/476] [D loss: 0.286406] [G loss: 11.753088, adv: 0.506294, cycle: 0.748794, identity: 0.751771] ETA: 2:02:15.836147\n",
      "[Epoch 0/200] [Batch 114/476] [D loss: 0.281084] [G loss: 11.543886, adv: 0.537941, cycle: 0.736012, identity: 0.729166] ETA: 2:10:10.791815\n",
      "[Epoch 0/200] [Batch 115/476] [D loss: 0.340268] [G loss: 12.110926, adv: 0.731178, cycle: 0.756170, identity: 0.763610] ETA: 2:13:07.989310\n",
      "[Epoch 0/200] [Batch 116/476] [D loss: 0.276862] [G loss: 10.622254, adv: 0.372842, cycle: 0.678967, identity: 0.691948] ETA: 2:06:38.732979\n",
      "[Epoch 0/200] [Batch 117/476] [D loss: 0.448107] [G loss: 12.282581, adv: 0.310448, cycle: 0.797935, identity: 0.798556] ETA: 2:02:20.242820\n",
      "[Epoch 0/200] [Batch 118/476] [D loss: 0.334718] [G loss: 11.216388, adv: 0.372548, cycle: 0.721121, identity: 0.726526] ETA: 1:47:11.262092\n",
      "[Epoch 0/200] [Batch 119/476] [D loss: 0.416561] [G loss: 12.049784, adv: 0.616350, cycle: 0.761613, identity: 0.763461] ETA: 2:00:01.036305\n",
      "[Epoch 0/200] [Batch 120/476] [D loss: 0.292893] [G loss: 11.946568, adv: 0.360128, cycle: 0.772535, identity: 0.772218] ETA: 1:46:06.067247\n",
      "[Epoch 0/200] [Batch 121/476] [D loss: 0.635359] [G loss: 12.149761, adv: 0.817260, cycle: 0.759810, identity: 0.746880] ETA: 1:59:25.816509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 122/476] [D loss: 0.285113] [G loss: 11.724781, adv: 0.413489, cycle: 0.755050, identity: 0.752159] ETA: 2:00:14.432783\n",
      "[Epoch 0/200] [Batch 123/476] [D loss: 0.331450] [G loss: 11.173638, adv: 0.365829, cycle: 0.724923, identity: 0.711716] ETA: 1:51:39.699834\n",
      "[Epoch 0/200] [Batch 124/476] [D loss: 0.313849] [G loss: 11.526251, adv: 0.373365, cycle: 0.742756, identity: 0.745066] ETA: 2:01:54.314401\n",
      "[Epoch 0/200] [Batch 125/476] [D loss: 0.333218] [G loss: 11.228354, adv: 0.405072, cycle: 0.722914, identity: 0.718828] ETA: 2:17:14.883928\n",
      "[Epoch 0/200] [Batch 126/476] [D loss: 0.304243] [G loss: 11.376582, adv: 0.743700, cycle: 0.710578, identity: 0.705420] ETA: 2:19:07.182323\n",
      "[Epoch 0/200] [Batch 127/476] [D loss: 0.274955] [G loss: 11.269028, adv: 0.480984, cycle: 0.718495, identity: 0.720618] ETA: 2:07:53.607584\n",
      "[Epoch 0/200] [Batch 128/476] [D loss: 0.512780] [G loss: 11.047346, adv: 0.161028, cycle: 0.728195, identity: 0.720874] ETA: 2:01:36.485138\n",
      "[Epoch 0/200] [Batch 129/476] [D loss: 0.248055] [G loss: 11.514727, adv: 0.624979, cycle: 0.724556, identity: 0.728837] ETA: 2:09:12.711582\n",
      "[Epoch 0/200] [Batch 130/476] [D loss: 0.301178] [G loss: 11.567024, adv: 0.389576, cycle: 0.745386, identity: 0.744717] ETA: 2:13:44.808819\n",
      "[Epoch 0/200] [Batch 131/476] [D loss: 0.443731] [G loss: 12.771614, adv: 0.663434, cycle: 0.810405, identity: 0.800826] ETA: 2:15:32.230272\n",
      "[Epoch 0/200] [Batch 132/476] [D loss: 0.253673] [G loss: 11.321692, adv: 0.289708, cycle: 0.735552, identity: 0.735292] ETA: 2:17:49.047235\n",
      "[Epoch 0/200] [Batch 133/476] [D loss: 0.397914] [G loss: 10.764392, adv: 0.285028, cycle: 0.699649, identity: 0.696576] ETA: 2:16:17.662658\n",
      "[Epoch 0/200] [Batch 134/476] [D loss: 0.351877] [G loss: 12.039804, adv: 0.445034, cycle: 0.776595, identity: 0.765764] ETA: 2:25:33.040062\n",
      "[Epoch 0/200] [Batch 135/476] [D loss: 0.250455] [G loss: 12.697186, adv: 0.629250, cycle: 0.804404, identity: 0.804779] ETA: 2:16:33.378967\n",
      "[Epoch 0/200] [Batch 136/476] [D loss: 0.337815] [G loss: 11.661498, adv: 0.342344, cycle: 0.754115, identity: 0.755601] ETA: 2:22:47.968288\n",
      "[Epoch 0/200] [Batch 137/476] [D loss: 0.454265] [G loss: 11.245272, adv: 0.464886, cycle: 0.723082, identity: 0.709914] ETA: 2:10:03.961569\n",
      "[Epoch 0/200] [Batch 138/476] [D loss: 0.378796] [G loss: 10.861900, adv: 0.574853, cycle: 0.685597, identity: 0.686216] ETA: 2:11:14.547534\n",
      "[Epoch 0/200] [Batch 139/476] [D loss: 0.446566] [G loss: 12.108426, adv: 0.487809, cycle: 0.774361, identity: 0.775401] ETA: 2:18:03.578131\n",
      "[Epoch 0/200] [Batch 140/476] [D loss: 0.484525] [G loss: 10.871385, adv: 0.215637, cycle: 0.711518, identity: 0.708113] ETA: 2:00:17.055836\n",
      "[Epoch 0/200] [Batch 141/476] [D loss: 0.572769] [G loss: 11.296650, adv: 0.295442, cycle: 0.732930, identity: 0.734382] ETA: 2:16:17.382447\n",
      "[Epoch 0/200] [Batch 142/476] [D loss: 0.201656] [G loss: 11.650606, adv: 0.597744, cycle: 0.739276, identity: 0.732021] ETA: 2:18:14.240564\n",
      "[Epoch 0/200] [Batch 143/476] [D loss: 0.346249] [G loss: 10.912108, adv: 0.534967, cycle: 0.694851, identity: 0.685727] ETA: 2:02:09.011679\n",
      "[Epoch 0/200] [Batch 144/476] [D loss: 0.289912] [G loss: 12.617092, adv: 0.569110, cycle: 0.804914, identity: 0.799768] ETA: 2:15:27.084213\n",
      "[Epoch 0/200] [Batch 145/476] [D loss: 0.305014] [G loss: 11.561111, adv: 0.580649, cycle: 0.733144, identity: 0.729804] ETA: 2:20:49.446144\n",
      "[Epoch 0/200] [Batch 146/476] [D loss: 0.428122] [G loss: 10.806211, adv: 0.177614, cycle: 0.710954, identity: 0.703812] ETA: 2:08:38.600440\n",
      "[Epoch 0/200] [Batch 147/476] [D loss: 0.219667] [G loss: 10.231867, adv: 0.368527, cycle: 0.657854, identity: 0.656960] ETA: 2:00:31.594885\n",
      "[Epoch 0/200] [Batch 148/476] [D loss: 0.319968] [G loss: 10.994710, adv: 0.450826, cycle: 0.702421, identity: 0.703935] ETA: 1:54:36.085444\n",
      "[Epoch 0/200] [Batch 149/476] [D loss: 0.295055] [G loss: 12.088547, adv: 0.422027, cycle: 0.776501, identity: 0.780302] ETA: 1:59:48.747660\n",
      "[Epoch 0/200] [Batch 150/476] [D loss: 0.178712] [G loss: 11.298976, adv: 0.531833, cycle: 0.715663, identity: 0.722103] ETA: 1:54:15.069351\n",
      "[Epoch 0/200] [Batch 151/476] [D loss: 0.164763] [G loss: 12.058212, adv: 0.593802, cycle: 0.764000, identity: 0.764883] ETA: 2:01:15.321759\n",
      "[Epoch 0/200] [Batch 152/476] [D loss: 0.232502] [G loss: 11.545880, adv: 0.321606, cycle: 0.746020, identity: 0.752816] ETA: 1:58:29.523794\n",
      "[Epoch 0/200] [Batch 153/476] [D loss: 0.243397] [G loss: 10.729992, adv: 0.234647, cycle: 0.700691, identity: 0.697688] ETA: 1:57:51.106632\n",
      "[Epoch 0/200] [Batch 154/476] [D loss: 0.260643] [G loss: 11.547750, adv: 0.318991, cycle: 0.752309, identity: 0.741134] ETA: 1:52:08.673893\n",
      "[Epoch 0/200] [Batch 155/476] [D loss: 0.227823] [G loss: 11.531066, adv: 0.325388, cycle: 0.747653, identity: 0.745830] ETA: 1:53:33.013439\n",
      "[Epoch 0/200] [Batch 156/476] [D loss: 0.264852] [G loss: 10.589810, adv: 0.244282, cycle: 0.691495, identity: 0.686116] ETA: 1:51:35.878878\n",
      "[Epoch 0/200] [Batch 157/476] [D loss: 0.267154] [G loss: 10.964094, adv: 0.450520, cycle: 0.698313, identity: 0.706089] ETA: 2:01:10.081239\n",
      "[Epoch 0/200] [Batch 158/476] [D loss: 0.226218] [G loss: 11.243103, adv: 0.446965, cycle: 0.720762, identity: 0.717704] ETA: 1:57:07.952991\n",
      "[Epoch 0/200] [Batch 159/476] [D loss: 0.230491] [G loss: 11.665028, adv: 0.377516, cycle: 0.753765, identity: 0.749972] ETA: 2:00:24.858428\n",
      "[Epoch 0/200] [Batch 160/476] [D loss: 0.436793] [G loss: 11.013542, adv: 0.366323, cycle: 0.709177, identity: 0.711091] ETA: 1:48:39.964828\n",
      "[Epoch 0/200] [Batch 161/476] [D loss: 0.308686] [G loss: 11.101599, adv: 0.264487, cycle: 0.724468, identity: 0.718487] ETA: 2:04:26.682529\n",
      "[Epoch 0/200] [Batch 162/476] [D loss: 0.545060] [G loss: 11.598122, adv: 0.766875, cycle: 0.730159, identity: 0.705932] ETA: 1:54:19.234162\n",
      "[Epoch 0/200] [Batch 163/476] [D loss: 0.205585] [G loss: 11.641150, adv: 0.287169, cycle: 0.759264, identity: 0.752268] ETA: 1:58:47.439653\n",
      "[Epoch 0/200] [Batch 164/476] [D loss: 0.255959] [G loss: 11.108554, adv: 0.159859, cycle: 0.729334, identity: 0.731071] ETA: 1:56:23.234904\n",
      "[Epoch 0/200] [Batch 165/476] [D loss: 0.295841] [G loss: 11.184041, adv: 0.425172, cycle: 0.715636, identity: 0.720501] ETA: 2:07:48.478609\n",
      "[Epoch 0/200] [Batch 166/476] [D loss: 0.233015] [G loss: 10.591976, adv: 0.463592, cycle: 0.673420, identity: 0.678837] ETA: 1:49:43.856253\n",
      "[Epoch 0/200] [Batch 167/476] [D loss: 0.210311] [G loss: 11.300691, adv: 0.337068, cycle: 0.731030, identity: 0.730665] ETA: 1:59:15.824230\n",
      "[Epoch 0/200] [Batch 168/476] [D loss: 0.364359] [G loss: 12.164177, adv: 0.604395, cycle: 0.769789, identity: 0.772378] ETA: 2:01:01.490992\n",
      "[Epoch 0/200] [Batch 169/476] [D loss: 0.306695] [G loss: 11.530805, adv: 0.282301, cycle: 0.750445, identity: 0.748811] ETA: 2:09:14.026466\n",
      "[Epoch 0/200] [Batch 170/476] [D loss: 0.347897] [G loss: 11.459180, adv: 0.181930, cycle: 0.752456, identity: 0.750538] ETA: 1:57:43.135455\n",
      "[Epoch 0/200] [Batch 171/476] [D loss: 0.255164] [G loss: 11.327047, adv: 0.561554, cycle: 0.721228, identity: 0.710642] ETA: 2:03:04.491436\n",
      "[Epoch 0/200] [Batch 172/476] [D loss: 0.111796] [G loss: 10.744027, adv: 0.536149, cycle: 0.683710, identity: 0.674155] ETA: 2:04:38.800461\n",
      "[Epoch 0/200] [Batch 173/476] [D loss: 0.310860] [G loss: 11.435111, adv: 0.390985, cycle: 0.733873, identity: 0.741079] ETA: 2:06:49.923827\n",
      "[Epoch 0/200] [Batch 174/476] [D loss: 0.321254] [G loss: 12.189363, adv: 0.233658, cycle: 0.798232, identity: 0.794677] ETA: 1:49:51.254265\n",
      "[Epoch 0/200] [Batch 175/476] [D loss: 0.475404] [G loss: 12.215865, adv: 0.218133, cycle: 0.799333, identity: 0.800880] ETA: 1:57:18.499546\n",
      "[Epoch 0/200] [Batch 176/476] [D loss: 0.534110] [G loss: 11.873653, adv: 0.695448, cycle: 0.745966, identity: 0.743709] ETA: 2:01:20.023590\n",
      "[Epoch 0/200] [Batch 177/476] [D loss: 0.419486] [G loss: 11.885843, adv: 0.487932, cycle: 0.764943, identity: 0.749696] ETA: 1:58:02.687728\n",
      "[Epoch 0/200] [Batch 178/476] [D loss: 0.227508] [G loss: 11.187489, adv: 0.537615, cycle: 0.717818, identity: 0.694338] ETA: 1:58:55.104850\n",
      "[Epoch 0/200] [Batch 179/476] [D loss: 0.274389] [G loss: 12.347620, adv: 0.702595, cycle: 0.776848, identity: 0.775308] ETA: 2:12:40.592300\n",
      "[Epoch 0/200] [Batch 180/476] [D loss: 0.449299] [G loss: 11.109673, adv: 0.295658, cycle: 0.722150, identity: 0.718502] ETA: 2:12:12.416902\n",
      "[Epoch 0/200] [Batch 181/476] [D loss: 0.214823] [G loss: 11.373323, adv: 0.419829, cycle: 0.731904, identity: 0.726891] ETA: 2:08:06.806171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 182/476] [D loss: 0.233949] [G loss: 11.172281, adv: 0.345657, cycle: 0.719846, identity: 0.725632] ETA: 2:20:45.001854\n",
      "[Epoch 0/200] [Batch 183/476] [D loss: 0.443356] [G loss: 11.885315, adv: 0.568147, cycle: 0.750161, identity: 0.763111] ETA: 2:06:01.414069\n",
      "[Epoch 0/200] [Batch 184/476] [D loss: 0.357236] [G loss: 11.144539, adv: 0.515581, cycle: 0.705473, identity: 0.714846] ETA: 1:57:50.386110\n",
      "[Epoch 0/200] [Batch 185/476] [D loss: 0.244799] [G loss: 10.727571, adv: 0.558142, cycle: 0.680346, identity: 0.673193] ETA: 1:59:15.624181\n",
      "[Epoch 0/200] [Batch 186/476] [D loss: 0.358517] [G loss: 12.185367, adv: 0.700716, cycle: 0.768005, identity: 0.760919] ETA: 2:07:29.363860\n",
      "[Epoch 0/200] [Batch 187/476] [D loss: 0.282114] [G loss: 11.054252, adv: 0.331224, cycle: 0.716040, identity: 0.712525] ETA: 2:09:32.809422\n",
      "[Epoch 0/200] [Batch 188/476] [D loss: 0.204286] [G loss: 11.233307, adv: 0.280892, cycle: 0.729119, identity: 0.732245] ETA: 2:07:39.441832\n",
      "[Epoch 0/200] [Batch 189/476] [D loss: 0.377814] [G loss: 11.622055, adv: 0.271334, cycle: 0.757201, identity: 0.755743] ETA: 2:09:59.873976\n",
      "[Epoch 0/200] [Batch 190/476] [D loss: 0.290151] [G loss: 11.673445, adv: 0.617570, cycle: 0.736635, identity: 0.737904] ETA: 1:54:59.821994\n",
      "[Epoch 0/200] [Batch 191/476] [D loss: 0.322278] [G loss: 11.152039, adv: 0.232378, cycle: 0.732556, identity: 0.718821] ETA: 2:06:58.313286\n",
      "[Epoch 0/200] [Batch 192/476] [D loss: 0.193005] [G loss: 10.722616, adv: 0.391443, cycle: 0.689726, identity: 0.686783] ETA: 2:07:57.082146\n",
      "[Epoch 0/200] [Batch 193/476] [D loss: 0.386769] [G loss: 11.274170, adv: 0.412302, cycle: 0.723142, identity: 0.726090] ETA: 2:09:20.245361\n",
      "[Epoch 0/200] [Batch 194/476] [D loss: 0.219480] [G loss: 10.966530, adv: 0.335624, cycle: 0.707139, identity: 0.711902] ETA: 2:08:18.688335\n",
      "[Epoch 0/200] [Batch 195/476] [D loss: 0.286114] [G loss: 11.720731, adv: 0.613750, cycle: 0.740128, identity: 0.741141] ETA: 2:02:29.782562\n",
      "[Epoch 0/200] [Batch 196/476] [D loss: 0.351462] [G loss: 11.364220, adv: 0.548302, cycle: 0.722283, identity: 0.718618] ETA: 1:50:45.041342\n",
      "[Epoch 0/200] [Batch 197/476] [D loss: 0.270760] [G loss: 10.530228, adv: 0.338709, cycle: 0.682371, identity: 0.673563] ETA: 1:52:24.474957\n",
      "[Epoch 0/200] [Batch 198/476] [D loss: 0.161342] [G loss: 10.619591, adv: 0.343817, cycle: 0.686085, identity: 0.682985] ETA: 2:05:56.822839\n",
      "[Epoch 0/200] [Batch 199/476] [D loss: 0.353162] [G loss: 12.093499, adv: 0.456081, cycle: 0.776474, identity: 0.774535] ETA: 1:56:44.943912\n",
      "[Epoch 0/200] [Batch 200/476] [D loss: 0.335629] [G loss: 10.909853, adv: 0.307686, cycle: 0.709161, identity: 0.702112] ETA: 1:54:18.348846\n",
      "[Epoch 0/200] [Batch 201/476] [D loss: 0.374289] [G loss: 11.168373, adv: 0.234904, cycle: 0.730543, identity: 0.725608] ETA: 5:07:17.507383\n",
      "[Epoch 0/200] [Batch 202/476] [D loss: 0.330063] [G loss: 11.710955, adv: 0.651824, cycle: 0.739075, identity: 0.733677] ETA: 2:07:06.943950\n",
      "[Epoch 0/200] [Batch 203/476] [D loss: 0.172183] [G loss: 10.988926, adv: 0.630911, cycle: 0.691178, identity: 0.689247] ETA: 1:58:17.215636\n",
      "[Epoch 0/200] [Batch 204/476] [D loss: 0.168433] [G loss: 11.445471, adv: 0.462073, cycle: 0.733270, identity: 0.730140] ETA: 1:51:50.050092\n",
      "[Epoch 0/200] [Batch 205/476] [D loss: 0.370401] [G loss: 11.918880, adv: 0.492236, cycle: 0.762338, identity: 0.760654] ETA: 1:58:49.046001\n",
      "[Epoch 0/200] [Batch 206/476] [D loss: 0.175540] [G loss: 11.025797, adv: 0.221423, cycle: 0.721133, identity: 0.718608] ETA: 2:01:37.655750\n",
      "[Epoch 0/200] [Batch 207/476] [D loss: 0.291950] [G loss: 11.979496, adv: 0.513017, cycle: 0.762068, identity: 0.769161] ETA: 2:01:23.876830\n",
      "[Epoch 0/200] [Batch 208/476] [D loss: 0.450584] [G loss: 10.895788, adv: 0.259237, cycle: 0.709803, identity: 0.707704] ETA: 2:02:33.238483\n",
      "[Epoch 0/200] [Batch 209/476] [D loss: 0.236180] [G loss: 10.385425, adv: 0.445072, cycle: 0.663936, identity: 0.660199] ETA: 2:04:38.583590\n",
      "[Epoch 0/200] [Batch 210/476] [D loss: 0.221244] [G loss: 10.156823, adv: 0.529595, cycle: 0.639584, identity: 0.646277] ETA: 2:01:06.230960\n",
      "[Epoch 0/200] [Batch 211/476] [D loss: 0.212507] [G loss: 10.244942, adv: 0.445272, cycle: 0.657129, identity: 0.645677] ETA: 2:06:33.722733\n",
      "[Epoch 0/200] [Batch 212/476] [D loss: 0.217232] [G loss: 10.691719, adv: 0.385213, cycle: 0.686986, identity: 0.687329] ETA: 1:59:51.818772\n",
      "[Epoch 0/200] [Batch 213/476] [D loss: 0.199816] [G loss: 11.099649, adv: 0.295063, cycle: 0.719402, identity: 0.722112] ETA: 2:04:47.123518\n",
      "[Epoch 0/200] [Batch 214/476] [D loss: 0.282797] [G loss: 10.898745, adv: 0.474267, cycle: 0.693765, identity: 0.697366] ETA: 1:55:15.267703\n",
      "[Epoch 0/200] [Batch 215/476] [D loss: 0.313534] [G loss: 11.019470, adv: 0.387593, cycle: 0.710984, identity: 0.704407] ETA: 1:57:38.681152\n",
      "[Epoch 0/200] [Batch 216/476] [D loss: 0.296520] [G loss: 11.916310, adv: 0.674470, cycle: 0.752472, identity: 0.743424] ETA: 2:03:51.517702\n",
      "[Epoch 0/200] [Batch 217/476] [D loss: 0.378432] [G loss: 12.131561, adv: 0.468095, cycle: 0.778546, identity: 0.775601] ETA: 2:02:41.305693\n",
      "[Epoch 0/200] [Batch 218/476] [D loss: 0.265059] [G loss: 11.690914, adv: 0.242192, cycle: 0.763736, identity: 0.762272] ETA: 1:42:10.084380\n",
      "[Epoch 0/200] [Batch 219/476] [D loss: 0.339289] [G loss: 10.881554, adv: 0.249538, cycle: 0.708259, identity: 0.709885] ETA: 1:56:12.581102\n",
      "[Epoch 0/200] [Batch 220/476] [D loss: 0.271838] [G loss: 11.591008, adv: 0.865435, cycle: 0.719485, identity: 0.706145] ETA: 2:01:23.219690\n",
      "[Epoch 0/200] [Batch 221/476] [D loss: 0.315047] [G loss: 11.649374, adv: 0.448376, cycle: 0.745766, identity: 0.748668] ETA: 1:59:21.631236\n",
      "[Epoch 0/200] [Batch 222/476] [D loss: 0.298943] [G loss: 11.408136, adv: 0.380728, cycle: 0.735731, identity: 0.734019] ETA: 1:48:13.950101\n",
      "[Epoch 0/200] [Batch 223/476] [D loss: 0.264422] [G loss: 10.615311, adv: 0.251238, cycle: 0.691978, identity: 0.688859] ETA: 1:52:39.635014\n",
      "[Epoch 0/200] [Batch 224/476] [D loss: 0.186006] [G loss: 10.726920, adv: 0.609718, cycle: 0.672693, identity: 0.678054] ETA: 2:04:39.893494\n",
      "[Epoch 0/200] [Batch 225/476] [D loss: 0.287573] [G loss: 11.362454, adv: 0.558438, cycle: 0.718114, identity: 0.724576] ETA: 2:19:03.585306\n",
      "[Epoch 0/200] [Batch 226/476] [D loss: 0.375515] [G loss: 10.741736, adv: 0.216268, cycle: 0.704735, identity: 0.695623] ETA: 1:58:51.432648\n",
      "[Epoch 0/200] [Batch 227/476] [D loss: 0.267227] [G loss: 11.836010, adv: 0.533040, cycle: 0.753257, identity: 0.754081] ETA: 1:56:35.112657\n",
      "[Epoch 0/200] [Batch 228/476] [D loss: 0.243118] [G loss: 11.442501, adv: 0.672025, cycle: 0.721225, identity: 0.711646] ETA: 2:03:22.410823\n",
      "[Epoch 0/200] [Batch 229/476] [D loss: 0.284503] [G loss: 11.148571, adv: 0.453936, cycle: 0.714892, identity: 0.709144] ETA: 2:04:21.181649\n",
      "[Epoch 0/200] [Batch 230/476] [D loss: 0.226931] [G loss: 11.984888, adv: 0.622206, cycle: 0.759135, identity: 0.754266] ETA: 1:55:05.385449\n",
      "[Epoch 0/200] [Batch 231/476] [D loss: 0.245239] [G loss: 11.057670, adv: 0.328249, cycle: 0.717510, identity: 0.710863] ETA: 2:00:15.286838\n",
      "[Epoch 0/200] [Batch 232/476] [D loss: 0.327484] [G loss: 11.308277, adv: 0.462400, cycle: 0.723130, identity: 0.722916] ETA: 1:59:16.273384\n",
      "[Epoch 0/200] [Batch 233/476] [D loss: 0.262740] [G loss: 11.408958, adv: 0.383660, cycle: 0.739831, identity: 0.725397] ETA: 1:55:21.243062\n",
      "[Epoch 0/200] [Batch 234/476] [D loss: 0.336368] [G loss: 11.164802, adv: 0.265703, cycle: 0.729125, identity: 0.721569] ETA: 2:01:17.753666\n",
      "[Epoch 0/200] [Batch 235/476] [D loss: 0.264699] [G loss: 10.954529, adv: 0.525662, cycle: 0.696823, identity: 0.692128] ETA: 1:59:27.141616\n",
      "[Epoch 0/200] [Batch 236/476] [D loss: 0.214539] [G loss: 11.854992, adv: 0.665542, cycle: 0.746176, identity: 0.745537] ETA: 2:05:42.615430\n",
      "[Epoch 0/200] [Batch 237/476] [D loss: 0.318427] [G loss: 11.845991, adv: 0.385920, cycle: 0.762127, identity: 0.767760] ETA: 2:00:54.746971\n",
      "[Epoch 0/200] [Batch 238/476] [D loss: 0.261079] [G loss: 10.838150, adv: 0.314254, cycle: 0.702566, identity: 0.699646] ETA: 2:12:32.140137\n",
      "[Epoch 0/200] [Batch 239/476] [D loss: 0.303217] [G loss: 11.715006, adv: 0.502552, cycle: 0.747696, identity: 0.747099] ETA: 1:53:58.960492\n",
      "[Epoch 0/200] [Batch 240/476] [D loss: 0.348103] [G loss: 12.022236, adv: 0.293119, cycle: 0.778235, identity: 0.789354] ETA: 2:03:50.522919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 241/476] [D loss: 0.270605] [G loss: 11.410960, adv: 0.709704, cycle: 0.711536, identity: 0.717180] ETA: 2:06:22.336362\n",
      "[Epoch 0/200] [Batch 242/476] [D loss: 0.205919] [G loss: 11.490577, adv: 0.484824, cycle: 0.736328, identity: 0.728495] ETA: 2:05:18.163377\n",
      "[Epoch 0/200] [Batch 243/476] [D loss: 0.232979] [G loss: 10.868603, adv: 0.259314, cycle: 0.707691, identity: 0.706476] ETA: 2:06:42.710703\n",
      "[Epoch 0/200] [Batch 244/476] [D loss: 0.146107] [G loss: 10.837130, adv: 0.196058, cycle: 0.711327, identity: 0.705561] ETA: 2:11:11.019238\n",
      "[Epoch 0/200] [Batch 245/476] [D loss: 0.348247] [G loss: 11.631080, adv: 0.620103, cycle: 0.733609, identity: 0.734977] ETA: 2:04:54.698205\n",
      "[Epoch 0/200] [Batch 246/476] [D loss: 0.229427] [G loss: 11.155394, adv: 0.393437, cycle: 0.715528, identity: 0.721336] ETA: 2:01:19.867640\n",
      "[Epoch 0/200] [Batch 247/476] [D loss: 0.222458] [G loss: 10.808279, adv: 0.477408, cycle: 0.687900, identity: 0.690374] ETA: 2:09:35.824447\n",
      "[Epoch 0/200] [Batch 248/476] [D loss: 0.482362] [G loss: 12.580519, adv: 0.652715, cycle: 0.791897, identity: 0.801767] ETA: 1:59:06.487787\n",
      "[Epoch 0/200] [Batch 249/476] [D loss: 0.206233] [G loss: 10.602475, adv: 0.277233, cycle: 0.687521, identity: 0.690007] ETA: 2:05:43.393938\n",
      "[Epoch 0/200] [Batch 250/476] [D loss: 0.167730] [G loss: 11.745655, adv: 0.534886, cycle: 0.749725, identity: 0.742703] ETA: 2:17:04.917340\n",
      "[Epoch 0/200] [Batch 251/476] [D loss: 0.150258] [G loss: 11.041578, adv: 0.270937, cycle: 0.718060, identity: 0.718008] ETA: 1:59:05.447041\n",
      "[Epoch 0/200] [Batch 252/476] [D loss: 0.218646] [G loss: 11.098321, adv: 0.162663, cycle: 0.729211, identity: 0.728710] ETA: 2:04:05.882835\n",
      "[Epoch 0/200] [Batch 253/476] [D loss: 0.360739] [G loss: 10.962799, adv: 0.440582, cycle: 0.700442, identity: 0.703559] ETA: 2:05:53.285502\n",
      "[Epoch 0/200] [Batch 254/476] [D loss: 0.299536] [G loss: 11.756256, adv: 0.630979, cycle: 0.739998, identity: 0.745060] ETA: 2:06:47.941951\n",
      "[Epoch 0/200] [Batch 255/476] [D loss: 0.333151] [G loss: 11.424723, adv: 0.388810, cycle: 0.737016, identity: 0.733151] ETA: 1:51:19.532723\n",
      "[Epoch 0/200] [Batch 256/476] [D loss: 0.299458] [G loss: 10.859923, adv: 0.482587, cycle: 0.690728, identity: 0.694010] ETA: 1:53:16.107811\n",
      "[Epoch 0/200] [Batch 257/476] [D loss: 0.352272] [G loss: 11.660814, adv: 0.578338, cycle: 0.740223, identity: 0.736049] ETA: 2:07:55.293182\n",
      "[Epoch 0/200] [Batch 258/476] [D loss: 0.253256] [G loss: 11.077131, adv: 0.406397, cycle: 0.710363, identity: 0.713421] ETA: 2:34:21.312428\n",
      "[Epoch 0/200] [Batch 259/476] [D loss: 0.272484] [G loss: 11.405510, adv: 0.530899, cycle: 0.726112, identity: 0.722698] ETA: 2:12:35.361445\n",
      "[Epoch 0/200] [Batch 260/476] [D loss: 0.218052] [G loss: 11.145098, adv: 0.398619, cycle: 0.716725, identity: 0.715847] ETA: 2:17:29.199100\n",
      "[Epoch 0/200] [Batch 261/476] [D loss: 0.247913] [G loss: 11.267259, adv: 0.286501, cycle: 0.732274, identity: 0.731604] ETA: 2:19:16.018362\n",
      "[Epoch 0/200] [Batch 262/476] [D loss: 0.119821] [G loss: 11.264904, adv: 0.402945, cycle: 0.723703, identity: 0.724986] ETA: 2:19:04.929746\n",
      "[Epoch 0/200] [Batch 263/476] [D loss: 0.300722] [G loss: 10.271169, adv: 0.234110, cycle: 0.669254, identity: 0.668903] ETA: 2:13:22.672412\n",
      "[Epoch 0/200] [Batch 264/476] [D loss: 0.221574] [G loss: 10.969707, adv: 0.521574, cycle: 0.696578, identity: 0.696470] ETA: 2:10:48.220785\n",
      "[Epoch 0/200] [Batch 265/476] [D loss: 0.326825] [G loss: 11.485357, adv: 0.531268, cycle: 0.727596, identity: 0.735627] ETA: 2:19:17.635485\n",
      "[Epoch 0/200] [Batch 266/476] [D loss: 0.155021] [G loss: 11.053640, adv: 0.372339, cycle: 0.710414, identity: 0.715433] ETA: 2:20:15.920611\n",
      "[Epoch 0/200] [Batch 267/476] [D loss: 0.207061] [G loss: 11.529140, adv: 0.388972, cycle: 0.743577, identity: 0.740879] ETA: 2:17:15.802788\n",
      "[Epoch 0/200] [Batch 268/476] [D loss: 0.258373] [G loss: 11.137240, adv: 0.342955, cycle: 0.717359, identity: 0.724138] ETA: 2:15:41.311486\n",
      "[Epoch 0/200] [Batch 269/476] [D loss: 0.198049] [G loss: 10.870557, adv: 0.172163, cycle: 0.713260, identity: 0.713159] ETA: 2:17:23.053007\n",
      "[Epoch 0/200] [Batch 270/476] [D loss: 0.369777] [G loss: 10.681075, adv: 0.375496, cycle: 0.685003, identity: 0.691111] ETA: 2:13:46.254475\n",
      "[Epoch 0/200] [Batch 271/476] [D loss: 0.290419] [G loss: 11.953575, adv: 0.813768, cycle: 0.744064, identity: 0.739834] ETA: 2:13:59.478034\n",
      "[Epoch 0/200] [Batch 272/476] [D loss: 0.249376] [G loss: 10.861371, adv: 0.324170, cycle: 0.704078, identity: 0.699284] ETA: 2:17:01.789459\n",
      "[Epoch 0/200] [Batch 273/476] [D loss: 0.239960] [G loss: 11.286704, adv: 0.317170, cycle: 0.728783, identity: 0.736341] ETA: 2:18:52.329826\n",
      "[Epoch 0/200] [Batch 274/476] [D loss: 0.151515] [G loss: 10.775412, adv: 0.344678, cycle: 0.696325, identity: 0.693497] ETA: 2:19:52.534023\n",
      "[Epoch 0/200] [Batch 275/476] [D loss: 0.315748] [G loss: 11.252356, adv: 0.574816, cycle: 0.713701, identity: 0.708107] ETA: 2:20:36.962527\n",
      "[Epoch 0/200] [Batch 276/476] [D loss: 0.328966] [G loss: 11.540154, adv: 0.328116, cycle: 0.748689, identity: 0.745029] ETA: 2:22:49.676141\n",
      "[Epoch 0/200] [Batch 277/476] [D loss: 0.204881] [G loss: 11.328979, adv: 0.496350, cycle: 0.720826, identity: 0.724874] ETA: 2:22:50.015858\n",
      "[Epoch 0/200] [Batch 278/476] [D loss: 0.198672] [G loss: 12.014258, adv: 0.367210, cycle: 0.774553, identity: 0.780303] ETA: 2:18:53.316709\n",
      "[Epoch 0/200] [Batch 279/476] [D loss: 0.205644] [G loss: 11.678361, adv: 0.335615, cycle: 0.758697, identity: 0.751156] ETA: 2:16:56.317534\n",
      "[Epoch 0/200] [Batch 280/476] [D loss: 0.281934] [G loss: 10.502602, adv: 0.126587, cycle: 0.690641, identity: 0.693921] ETA: 2:17:54.912357\n",
      "[Epoch 0/200] [Batch 281/476] [D loss: 0.218036] [G loss: 12.441339, adv: 0.499167, cycle: 0.803157, identity: 0.782120] ETA: 2:19:22.337142\n",
      "[Epoch 0/200] [Batch 282/476] [D loss: 0.161258] [G loss: 10.508508, adv: 0.341655, cycle: 0.674190, identity: 0.684990] ETA: 2:19:27.363471\n",
      "[Epoch 0/200] [Batch 283/476] [D loss: 0.208074] [G loss: 10.591271, adv: 0.325141, cycle: 0.684896, identity: 0.683434] ETA: 2:15:12.778604\n",
      "[Epoch 0/200] [Batch 284/476] [D loss: 0.401266] [G loss: 10.943831, adv: 0.293828, cycle: 0.708741, identity: 0.712519] ETA: 2:15:02.849195\n",
      "[Epoch 0/200] [Batch 285/476] [D loss: 0.187541] [G loss: 11.761065, adv: 0.573499, cycle: 0.750597, identity: 0.736319] ETA: 2:19:47.963408\n",
      "[Epoch 0/200] [Batch 286/476] [D loss: 0.240048] [G loss: 11.118383, adv: 0.202400, cycle: 0.726300, identity: 0.730597] ETA: 2:11:30.370731\n",
      "[Epoch 0/200] [Batch 287/476] [D loss: 0.257169] [G loss: 10.996982, adv: 0.188155, cycle: 0.719299, identity: 0.723167] ETA: 2:18:08.399994\n",
      "[Epoch 0/200] [Batch 288/476] [D loss: 0.202970] [G loss: 11.213943, adv: 0.475109, cycle: 0.714409, identity: 0.718948] ETA: 2:24:28.725159\n",
      "[Epoch 0/200] [Batch 289/476] [D loss: 0.191295] [G loss: 11.636407, adv: 0.559772, cycle: 0.738084, identity: 0.739158] ETA: 2:20:00.621328\n",
      "[Epoch 0/200] [Batch 290/476] [D loss: 0.202363] [G loss: 10.719999, adv: 0.312014, cycle: 0.692387, identity: 0.696823] ETA: 2:26:34.785817\n",
      "[Epoch 0/200] [Batch 291/476] [D loss: 0.274384] [G loss: 11.175473, adv: 0.466212, cycle: 0.713108, identity: 0.715636] ETA: 2:27:52.352685\n",
      "[Epoch 0/200] [Batch 292/476] [D loss: 0.358475] [G loss: 11.643587, adv: 0.675625, cycle: 0.728413, identity: 0.736767] ETA: 2:17:02.297399\n",
      "[Epoch 0/200] [Batch 293/476] [D loss: 0.276043] [G loss: 10.597692, adv: 0.442559, cycle: 0.674906, identity: 0.681215] ETA: 2:24:48.633320\n",
      "[Epoch 0/200] [Batch 294/476] [D loss: 0.196173] [G loss: 10.589760, adv: 0.409089, cycle: 0.678598, identity: 0.678938] ETA: 2:07:24.719319\n",
      "[Epoch 0/200] [Batch 295/476] [D loss: 0.161740] [G loss: 10.438947, adv: 0.314447, cycle: 0.673604, identity: 0.677692] ETA: 2:09:15.059091\n",
      "[Epoch 0/200] [Batch 296/476] [D loss: 0.216745] [G loss: 10.570979, adv: 0.375059, cycle: 0.674545, identity: 0.690093] ETA: 1:56:58.947701\n",
      "[Epoch 0/200] [Batch 297/476] [D loss: 0.191775] [G loss: 11.654360, adv: 0.501074, cycle: 0.740550, identity: 0.749558] ETA: 1:59:10.809671\n",
      "[Epoch 0/200] [Batch 298/476] [D loss: 0.173283] [G loss: 10.666055, adv: 0.299794, cycle: 0.689541, identity: 0.694170] ETA: 1:55:10.306196\n",
      "[Epoch 0/200] [Batch 299/476] [D loss: 0.161368] [G loss: 11.376405, adv: 0.324888, cycle: 0.734811, identity: 0.740681] ETA: 2:03:34.163251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 300/476] [D loss: 0.306987] [G loss: 11.783173, adv: 0.375321, cycle: 0.758621, identity: 0.764328] ETA: 1:53:59.522433\n",
      "[Epoch 0/200] [Batch 301/476] [D loss: 0.270840] [G loss: 11.567945, adv: 0.328958, cycle: 0.746523, identity: 0.754751] ETA: 5:46:27.234725\n",
      "[Epoch 0/200] [Batch 302/476] [D loss: 0.200933] [G loss: 11.242805, adv: 0.492370, cycle: 0.716413, identity: 0.717262] ETA: 2:17:09.417837\n",
      "[Epoch 0/200] [Batch 303/476] [D loss: 0.170038] [G loss: 11.078963, adv: 0.414686, cycle: 0.709945, identity: 0.712965] ETA: 2:12:44.118432\n",
      "[Epoch 0/200] [Batch 304/476] [D loss: 0.219407] [G loss: 11.159268, adv: 0.385382, cycle: 0.718629, identity: 0.717518] ETA: 2:11:45.865711\n",
      "[Epoch 0/200] [Batch 305/476] [D loss: 0.200633] [G loss: 10.989758, adv: 0.321516, cycle: 0.707943, identity: 0.717761] ETA: 2:16:00.333250\n",
      "[Epoch 0/200] [Batch 306/476] [D loss: 0.257821] [G loss: 10.726874, adv: 0.230935, cycle: 0.699604, identity: 0.699980] ETA: 2:20:53.845297\n",
      "[Epoch 0/200] [Batch 307/476] [D loss: 0.195057] [G loss: 11.516783, adv: 0.507551, cycle: 0.730481, identity: 0.740885] ETA: 2:13:10.592478\n",
      "[Epoch 0/200] [Batch 308/476] [D loss: 0.172801] [G loss: 12.054111, adv: 0.381380, cycle: 0.778480, identity: 0.777586] ETA: 2:14:29.171974\n",
      "[Epoch 0/200] [Batch 309/476] [D loss: 0.161087] [G loss: 11.363245, adv: 0.421037, cycle: 0.728471, identity: 0.731499] ETA: 2:28:29.175665\n",
      "[Epoch 0/200] [Batch 310/476] [D loss: 0.250420] [G loss: 10.557673, adv: 0.364434, cycle: 0.684018, identity: 0.670612] ETA: 2:12:25.907228\n",
      "[Epoch 0/200] [Batch 311/476] [D loss: 0.219501] [G loss: 10.714213, adv: 0.640219, cycle: 0.672122, identity: 0.670555] ETA: 2:18:30.284862\n",
      "[Epoch 0/200] [Batch 312/476] [D loss: 0.261027] [G loss: 10.610783, adv: 0.385432, cycle: 0.682302, identity: 0.680467] ETA: 2:18:41.802914\n",
      "[Epoch 0/200] [Batch 313/476] [D loss: 0.228697] [G loss: 10.960585, adv: 0.492827, cycle: 0.694761, identity: 0.704029] ETA: 2:04:50.168081\n",
      "[Epoch 0/200] [Batch 314/476] [D loss: 0.235778] [G loss: 10.900683, adv: 0.321777, cycle: 0.704109, identity: 0.707563] ETA: 2:16:05.531672\n",
      "[Epoch 0/200] [Batch 315/476] [D loss: 0.423261] [G loss: 10.807158, adv: 0.280232, cycle: 0.698544, identity: 0.708298] ETA: 2:24:24.607700\n",
      "[Epoch 0/200] [Batch 316/476] [D loss: 0.357608] [G loss: 11.204252, adv: 0.551104, cycle: 0.707383, identity: 0.715864] ETA: 2:18:44.664448\n",
      "[Epoch 0/200] [Batch 317/476] [D loss: 0.186258] [G loss: 11.302979, adv: 0.545338, cycle: 0.716076, identity: 0.719375] ETA: 2:09:02.991056\n",
      "[Epoch 0/200] [Batch 318/476] [D loss: 0.190846] [G loss: 11.544840, adv: 0.406360, cycle: 0.740302, identity: 0.747091] ETA: 2:32:21.966879\n",
      "[Epoch 0/200] [Batch 319/476] [D loss: 0.186486] [G loss: 10.694374, adv: 0.215016, cycle: 0.697002, identity: 0.701869] ETA: 2:16:56.474575\n",
      "[Epoch 0/200] [Batch 320/476] [D loss: 0.240937] [G loss: 10.190922, adv: 0.481219, cycle: 0.643884, identity: 0.654172] ETA: 2:17:17.222061\n",
      "[Epoch 0/200] [Batch 321/476] [D loss: 0.166488] [G loss: 11.392426, adv: 0.677831, cycle: 0.713113, identity: 0.716693] ETA: 2:14:36.820810\n",
      "[Epoch 0/200] [Batch 322/476] [D loss: 0.254237] [G loss: 11.277086, adv: 0.285996, cycle: 0.732900, identity: 0.732418] ETA: 2:04:47.308677\n",
      "[Epoch 0/200] [Batch 323/476] [D loss: 0.181386] [G loss: 10.244696, adv: 0.337892, cycle: 0.658108, identity: 0.665145] ETA: 2:04:15.448044\n",
      "[Epoch 0/200] [Batch 324/476] [D loss: 0.188182] [G loss: 10.912226, adv: 0.222834, cycle: 0.714295, identity: 0.709288] ETA: 1:57:13.321752\n",
      "[Epoch 0/200] [Batch 325/476] [D loss: 0.299073] [G loss: 10.509626, adv: 0.371295, cycle: 0.677338, identity: 0.672991] ETA: 2:00:26.128042\n",
      "[Epoch 0/200] [Batch 326/476] [D loss: 0.210992] [G loss: 11.022026, adv: 0.841520, cycle: 0.676349, identity: 0.683402] ETA: 1:59:24.933382\n",
      "[Epoch 0/200] [Batch 327/476] [D loss: 0.247416] [G loss: 10.326530, adv: 0.303611, cycle: 0.666429, identity: 0.671726] ETA: 2:06:11.465739\n",
      "[Epoch 0/200] [Batch 328/476] [D loss: 0.242756] [G loss: 11.352916, adv: 0.608204, cycle: 0.715345, identity: 0.718253] ETA: 2:26:38.367010\n",
      "[Epoch 0/200] [Batch 329/476] [D loss: 0.176833] [G loss: 10.586901, adv: 0.428718, cycle: 0.674546, identity: 0.682546] ETA: 2:31:33.565434\n",
      "[Epoch 0/200] [Batch 330/476] [D loss: 0.134743] [G loss: 11.063993, adv: 0.277434, cycle: 0.714298, identity: 0.728715] ETA: 2:19:02.141879\n",
      "[Epoch 0/200] [Batch 331/476] [D loss: 0.293568] [G loss: 10.564717, adv: 0.149346, cycle: 0.692455, identity: 0.698165] ETA: 2:05:56.670653\n",
      "[Epoch 0/200] [Batch 332/476] [D loss: 0.159669] [G loss: 11.543034, adv: 0.496499, cycle: 0.740217, identity: 0.728873] ETA: 1:54:03.571907\n",
      "[Epoch 0/200] [Batch 333/476] [D loss: 0.290035] [G loss: 9.485031, adv: 0.172086, cycle: 0.619545, identity: 0.623498] ETA: 1:58:58.733246\n",
      "[Epoch 0/200] [Batch 334/476] [D loss: 0.245666] [G loss: 11.383450, adv: 0.615016, cycle: 0.715849, identity: 0.721989] ETA: 1:57:06.451006\n",
      "[Epoch 0/200] [Batch 335/476] [D loss: 0.262399] [G loss: 11.380696, adv: 0.423580, cycle: 0.729262, identity: 0.732899] ETA: 1:57:29.899220\n",
      "[Epoch 0/200] [Batch 336/476] [D loss: 0.277572] [G loss: 12.389277, adv: 0.534072, cycle: 0.793124, identity: 0.784792] ETA: 1:51:19.239788\n",
      "[Epoch 0/200] [Batch 337/476] [D loss: 0.170227] [G loss: 10.385677, adv: 0.183265, cycle: 0.680624, identity: 0.679235] ETA: 1:59:36.112337\n",
      "[Epoch 0/200] [Batch 338/476] [D loss: 0.214623] [G loss: 11.204750, adv: 0.403939, cycle: 0.720510, identity: 0.719142] ETA: 2:06:09.457026\n",
      "[Epoch 0/200] [Batch 339/476] [D loss: 0.223037] [G loss: 10.676022, adv: 0.679607, cycle: 0.661388, identity: 0.676507] ETA: 2:26:13.622038\n",
      "[Epoch 0/200] [Batch 340/476] [D loss: 0.337115] [G loss: 11.461891, adv: 0.368093, cycle: 0.742650, identity: 0.733460] ETA: 2:18:08.091431\n",
      "[Epoch 0/200] [Batch 341/476] [D loss: 0.186073] [G loss: 10.819269, adv: 0.410978, cycle: 0.691158, identity: 0.699343] ETA: 2:21:11.330554\n",
      "[Epoch 0/200] [Batch 342/476] [D loss: 0.240827] [G loss: 11.710384, adv: 0.356809, cycle: 0.752911, identity: 0.764892] ETA: 2:25:26.868876\n",
      "[Epoch 0/200] [Batch 343/476] [D loss: 0.197048] [G loss: 10.341501, adv: 0.388341, cycle: 0.665226, identity: 0.660180] ETA: 2:20:52.448786\n",
      "[Epoch 0/200] [Batch 344/476] [D loss: 0.124484] [G loss: 10.664581, adv: 0.488276, cycle: 0.677401, identity: 0.680459] ETA: 2:07:40.163685\n",
      "[Epoch 0/200] [Batch 345/476] [D loss: 0.195573] [G loss: 11.221519, adv: 0.859831, cycle: 0.689452, identity: 0.693433] ETA: 1:59:29.853362\n",
      "[Epoch 0/200] [Batch 346/476] [D loss: 0.265122] [G loss: 11.104897, adv: 0.470185, cycle: 0.709069, identity: 0.708805] ETA: 2:01:35.923998\n",
      "[Epoch 0/200] [Batch 347/476] [D loss: 0.266476] [G loss: 9.995806, adv: 0.282792, cycle: 0.643852, identity: 0.654899] ETA: 1:59:59.010861\n",
      "[Epoch 0/200] [Batch 348/476] [D loss: 0.306726] [G loss: 11.454718, adv: 0.503679, cycle: 0.728705, identity: 0.732799] ETA: 1:58:23.705393\n",
      "[Epoch 0/200] [Batch 349/476] [D loss: 0.251497] [G loss: 12.187143, adv: 0.613392, cycle: 0.769500, identity: 0.775751] ETA: 2:04:26.769977\n",
      "[Epoch 0/200] [Batch 350/476] [D loss: 0.288107] [G loss: 11.338177, adv: 0.348553, cycle: 0.730504, identity: 0.736917] ETA: 2:02:00.627415\n",
      "[Epoch 0/200] [Batch 351/476] [D loss: 0.154841] [G loss: 10.278913, adv: 0.282425, cycle: 0.666198, identity: 0.666901] ETA: 1:59:23.746396\n",
      "[Epoch 0/200] [Batch 352/476] [D loss: 0.172375] [G loss: 10.030828, adv: 0.329802, cycle: 0.649107, identity: 0.641992] ETA: 2:08:39.940979\n",
      "[Epoch 0/200] [Batch 353/476] [D loss: 0.230305] [G loss: 11.661361, adv: 0.441032, cycle: 0.750120, identity: 0.743825] ETA: 2:01:23.626667\n",
      "[Epoch 0/200] [Batch 354/476] [D loss: 0.269990] [G loss: 11.237873, adv: 0.524886, cycle: 0.708008, identity: 0.726582] ETA: 2:07:21.107398\n",
      "[Epoch 0/200] [Batch 355/476] [D loss: 0.289535] [G loss: 10.934193, adv: 0.254807, cycle: 0.709511, identity: 0.716854] ETA: 2:13:56.660560\n",
      "[Epoch 0/200] [Batch 356/476] [D loss: 0.239611] [G loss: 10.753107, adv: 0.199840, cycle: 0.702960, identity: 0.704734] ETA: 2:05:31.071785\n",
      "[Epoch 0/200] [Batch 357/476] [D loss: 0.167312] [G loss: 11.134409, adv: 0.803321, cycle: 0.689228, identity: 0.687762] ETA: 2:09:39.388861\n",
      "[Epoch 0/200] [Batch 358/476] [D loss: 0.159142] [G loss: 10.597813, adv: 0.310038, cycle: 0.686824, identity: 0.683908] ETA: 2:07:36.025696\n",
      "[Epoch 0/200] [Batch 359/476] [D loss: 0.217138] [G loss: 11.017385, adv: 0.364601, cycle: 0.709407, identity: 0.711742] ETA: 2:06:04.615684\n",
      "[Epoch 0/200] [Batch 360/476] [D loss: 0.208037] [G loss: 11.881209, adv: 0.545921, cycle: 0.753573, identity: 0.759911] ETA: 2:03:16.802940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 361/476] [D loss: 0.331057] [G loss: 11.204717, adv: 0.456752, cycle: 0.718113, identity: 0.713368] ETA: 2:03:44.514333\n",
      "[Epoch 0/200] [Batch 362/476] [D loss: 0.219160] [G loss: 10.389437, adv: 0.237358, cycle: 0.677557, identity: 0.675302] ETA: 2:15:44.171284\n",
      "[Epoch 0/200] [Batch 363/476] [D loss: 0.169991] [G loss: 11.861198, adv: 0.608197, cycle: 0.748245, identity: 0.754111] ETA: 2:07:00.575177\n",
      "[Epoch 0/200] [Batch 364/476] [D loss: 0.278701] [G loss: 10.946809, adv: 0.388451, cycle: 0.703526, identity: 0.704620] ETA: 2:18:22.387239\n",
      "[Epoch 0/200] [Batch 365/476] [D loss: 0.267770] [G loss: 10.288122, adv: 0.355123, cycle: 0.662594, identity: 0.661412] ETA: 2:07:54.747322\n",
      "[Epoch 0/200] [Batch 366/476] [D loss: 0.221913] [G loss: 10.328324, adv: 0.523860, cycle: 0.658070, identity: 0.644752] ETA: 2:06:30.963480\n",
      "[Epoch 0/200] [Batch 367/476] [D loss: 0.186654] [G loss: 11.298906, adv: 0.487218, cycle: 0.720537, identity: 0.721264] ETA: 2:08:30.535286\n",
      "[Epoch 0/200] [Batch 368/476] [D loss: 0.419807] [G loss: 12.431684, adv: 0.629671, cycle: 0.783232, identity: 0.793939] ETA: 2:03:10.142208\n",
      "[Epoch 0/200] [Batch 369/476] [D loss: 0.235011] [G loss: 10.724749, adv: 0.775796, cycle: 0.665187, identity: 0.659416] ETA: 2:03:43.910659\n",
      "[Epoch 0/200] [Batch 370/476] [D loss: 0.268937] [G loss: 10.758359, adv: 0.166584, cycle: 0.700655, identity: 0.717045] ETA: 2:08:20.976362\n",
      "[Epoch 0/200] [Batch 371/476] [D loss: 0.163715] [G loss: 11.481982, adv: 0.684865, cycle: 0.719052, identity: 0.721319] ETA: 2:02:34.163600\n",
      "[Epoch 0/200] [Batch 372/476] [D loss: 0.135474] [G loss: 11.546194, adv: 0.415872, cycle: 0.743600, identity: 0.738865] ETA: 2:08:18.191330\n",
      "[Epoch 0/200] [Batch 373/476] [D loss: 0.196399] [G loss: 11.822008, adv: 0.186157, cycle: 0.776578, identity: 0.774014] ETA: 2:14:48.740134\n",
      "[Epoch 0/200] [Batch 374/476] [D loss: 0.242402] [G loss: 11.709695, adv: 0.361134, cycle: 0.755907, identity: 0.757899] ETA: 2:16:10.293334\n",
      "[Epoch 0/200] [Batch 375/476] [D loss: 0.358309] [G loss: 10.692598, adv: 0.076162, cycle: 0.709240, identity: 0.704807] ETA: 2:07:54.819762\n",
      "[Epoch 0/200] [Batch 376/476] [D loss: 0.293182] [G loss: 11.288073, adv: 0.432122, cycle: 0.725741, identity: 0.719707] ETA: 2:05:46.326502\n",
      "[Epoch 0/200] [Batch 377/476] [D loss: 0.238955] [G loss: 12.656749, adv: 1.121029, cycle: 0.767212, identity: 0.772720] ETA: 2:19:20.571408\n",
      "[Epoch 0/200] [Batch 378/476] [D loss: 0.256695] [G loss: 11.628087, adv: 0.364980, cycle: 0.744747, identity: 0.763127] ETA: 2:03:42.301795\n",
      "[Epoch 0/200] [Batch 379/476] [D loss: 0.171813] [G loss: 10.496717, adv: 0.696671, cycle: 0.652006, identity: 0.655997] ETA: 2:04:15.817652\n",
      "[Epoch 0/200] [Batch 380/476] [D loss: 0.224478] [G loss: 10.140227, adv: 0.302230, cycle: 0.653191, identity: 0.661218] ETA: 2:08:14.173465\n",
      "[Epoch 0/200] [Batch 381/476] [D loss: 0.294681] [G loss: 11.291502, adv: 0.329519, cycle: 0.730896, identity: 0.730604] ETA: 2:12:44.987342\n",
      "[Epoch 0/200] [Batch 382/476] [D loss: 0.248602] [G loss: 10.444242, adv: 0.816760, cycle: 0.639937, identity: 0.645623] ETA: 2:00:32.027339\n",
      "[Epoch 0/200] [Batch 383/476] [D loss: 0.153790] [G loss: 11.524164, adv: 0.662449, cycle: 0.722534, identity: 0.727275] ETA: 1:58:49.861764\n",
      "[Epoch 0/200] [Batch 384/476] [D loss: 0.161819] [G loss: 10.642015, adv: 0.461262, cycle: 0.677793, identity: 0.680565] ETA: 1:53:20.463875\n",
      "[Epoch 0/200] [Batch 385/476] [D loss: 0.170440] [G loss: 10.990379, adv: 0.573323, cycle: 0.693303, identity: 0.696806] ETA: 1:45:49.884002\n",
      "[Epoch 0/200] [Batch 386/476] [D loss: 0.421954] [G loss: 10.524508, adv: 0.111169, cycle: 0.691227, identity: 0.700214] ETA: 1:52:21.184653\n",
      "[Epoch 0/200] [Batch 387/476] [D loss: 0.185151] [G loss: 10.630650, adv: 0.559267, cycle: 0.670216, identity: 0.673844] ETA: 1:53:07.454174\n",
      "[Epoch 0/200] [Batch 388/476] [D loss: 0.293370] [G loss: 10.866597, adv: 0.852860, cycle: 0.663350, identity: 0.676048] ETA: 1:58:43.224215\n",
      "[Epoch 0/200] [Batch 389/476] [D loss: 0.295852] [G loss: 10.561203, adv: 0.284905, cycle: 0.686603, identity: 0.682053] ETA: 2:09:58.826289\n",
      "[Epoch 0/200] [Batch 390/476] [D loss: 0.182327] [G loss: 10.763449, adv: 0.477759, cycle: 0.686366, identity: 0.684406] ETA: 2:14:19.463937\n",
      "[Epoch 0/200] [Batch 391/476] [D loss: 0.223303] [G loss: 12.541138, adv: 1.111367, cycle: 0.763060, identity: 0.759833] ETA: 2:07:22.285733\n",
      "[Epoch 0/200] [Batch 392/476] [D loss: 0.334452] [G loss: 11.114260, adv: 0.102373, cycle: 0.739181, identity: 0.724016] ETA: 2:02:17.932835\n",
      "[Epoch 0/200] [Batch 393/476] [D loss: 0.248456] [G loss: 11.457608, adv: 0.498876, cycle: 0.734365, identity: 0.723017] ETA: 2:02:28.207954\n",
      "[Epoch 0/200] [Batch 394/476] [D loss: 0.419853] [G loss: 12.730516, adv: 0.680188, cycle: 0.800804, identity: 0.808457] ETA: 2:05:18.877376\n",
      "[Epoch 0/200] [Batch 395/476] [D loss: 0.244954] [G loss: 11.200861, adv: 0.352737, cycle: 0.722034, identity: 0.725556] ETA: 2:13:27.819887\n",
      "[Epoch 0/200] [Batch 396/476] [D loss: 0.251161] [G loss: 11.351346, adv: 0.558915, cycle: 0.719349, identity: 0.719789] ETA: 1:59:43.018484\n",
      "[Epoch 0/200] [Batch 397/476] [D loss: 0.261101] [G loss: 12.029976, adv: 0.552319, cycle: 0.766258, identity: 0.763016] ETA: 2:08:28.163901\n",
      "[Epoch 0/200] [Batch 398/476] [D loss: 0.259202] [G loss: 10.598623, adv: 0.314992, cycle: 0.685239, identity: 0.686248] ETA: 1:59:45.059398\n",
      "[Epoch 0/200] [Batch 399/476] [D loss: 0.254355] [G loss: 10.671138, adv: 0.285517, cycle: 0.693051, identity: 0.691023] ETA: 1:58:31.865104\n",
      "[Epoch 0/200] [Batch 400/476] [D loss: 0.138527] [G loss: 10.847054, adv: 0.313888, cycle: 0.699414, identity: 0.707806] ETA: 1:50:56.199932\n",
      "[Epoch 0/200] [Batch 401/476] [D loss: 0.230455] [G loss: 11.263965, adv: 0.531247, cycle: 0.710936, identity: 0.724672] ETA: 5:08:47.250451\n",
      "[Epoch 0/200] [Batch 402/476] [D loss: 0.169837] [G loss: 12.092024, adv: 0.912465, cycle: 0.742576, identity: 0.750759] ETA: 1:55:25.945265\n",
      "[Epoch 0/200] [Batch 403/476] [D loss: 0.274354] [G loss: 10.841933, adv: 0.622606, cycle: 0.683037, identity: 0.677791] ETA: 1:49:16.995310\n",
      "[Epoch 0/200] [Batch 404/476] [D loss: 0.100436] [G loss: 10.281033, adv: 0.200863, cycle: 0.669452, identity: 0.677130] ETA: 1:53:38.375985\n",
      "[Epoch 0/200] [Batch 405/476] [D loss: 0.263061] [G loss: 11.273172, adv: 0.260899, cycle: 0.732120, identity: 0.738216] ETA: 2:21:41.098466\n",
      "[Epoch 0/200] [Batch 406/476] [D loss: 0.234389] [G loss: 11.923865, adv: 1.085656, cycle: 0.718157, identity: 0.731327] ETA: 2:14:23.708795\n",
      "[Epoch 0/200] [Batch 407/476] [D loss: 0.226647] [G loss: 11.311071, adv: 0.693733, cycle: 0.706462, identity: 0.710544] ETA: 2:10:55.044524\n",
      "[Epoch 0/200] [Batch 408/476] [D loss: 0.224546] [G loss: 10.737502, adv: 0.413877, cycle: 0.686831, identity: 0.691062] ETA: 2:02:02.185158\n",
      "[Epoch 0/200] [Batch 409/476] [D loss: 0.225613] [G loss: 11.666064, adv: 0.369076, cycle: 0.750992, identity: 0.757413] ETA: 2:07:27.004586\n",
      "[Epoch 0/200] [Batch 410/476] [D loss: 0.173063] [G loss: 10.994045, adv: 0.378996, cycle: 0.706470, identity: 0.710069] ETA: 2:06:46.289659\n",
      "[Epoch 0/200] [Batch 411/476] [D loss: 0.273802] [G loss: 10.244735, adv: 0.645571, cycle: 0.637233, identity: 0.645366] ETA: 2:02:38.971337\n",
      "[Epoch 0/200] [Batch 412/476] [D loss: 0.354604] [G loss: 10.644598, adv: 0.592161, cycle: 0.668357, identity: 0.673774] ETA: 2:01:51.344942\n",
      "[Epoch 0/200] [Batch 413/476] [D loss: 0.209414] [G loss: 10.073820, adv: 0.255531, cycle: 0.656118, identity: 0.651421] ETA: 1:58:06.927716\n",
      "[Epoch 0/200] [Batch 414/476] [D loss: 0.204478] [G loss: 11.080288, adv: 0.363475, cycle: 0.710984, identity: 0.721396] ETA: 2:03:42.647677\n",
      "[Epoch 0/200] [Batch 415/476] [D loss: 0.443369] [G loss: 11.474568, adv: 0.429834, cycle: 0.730423, identity: 0.748101] ETA: 1:59:17.240320\n",
      "[Epoch 0/200] [Batch 416/476] [D loss: 0.236777] [G loss: 11.660103, adv: 0.475405, cycle: 0.747080, identity: 0.742780] ETA: 1:58:29.211288\n",
      "[Epoch 0/200] [Batch 417/476] [D loss: 0.248137] [G loss: 10.100437, adv: 0.497604, cycle: 0.640700, identity: 0.639168] ETA: 1:59:53.494723\n",
      "[Epoch 0/200] [Batch 418/476] [D loss: 0.186616] [G loss: 10.582083, adv: 0.495444, cycle: 0.679808, identity: 0.657712] ETA: 2:05:13.041967\n",
      "[Epoch 0/200] [Batch 419/476] [D loss: 0.280588] [G loss: 11.945040, adv: 0.843878, cycle: 0.737147, identity: 0.745938] ETA: 2:17:46.952599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 420/476] [D loss: 0.228149] [G loss: 11.233893, adv: 0.263228, cycle: 0.731879, identity: 0.730376] ETA: 2:20:20.843468\n",
      "[Epoch 0/200] [Batch 421/476] [D loss: 0.269226] [G loss: 10.528720, adv: 0.230592, cycle: 0.687452, identity: 0.684722] ETA: 2:17:37.897506\n",
      "[Epoch 0/200] [Batch 422/476] [D loss: 0.270106] [G loss: 11.351448, adv: 0.797872, cycle: 0.699006, identity: 0.712703] ETA: 2:11:09.506346\n",
      "[Epoch 0/200] [Batch 423/476] [D loss: 0.246655] [G loss: 10.943610, adv: 0.759376, cycle: 0.676666, identity: 0.683514] ETA: 1:58:30.697355\n",
      "[Epoch 0/200] [Batch 424/476] [D loss: 0.273282] [G loss: 11.106889, adv: 0.210472, cycle: 0.723624, identity: 0.732035] ETA: 1:58:49.151344\n",
      "[Epoch 0/200] [Batch 425/476] [D loss: 0.152747] [G loss: 10.818686, adv: 0.521624, cycle: 0.687030, identity: 0.685353] ETA: 2:00:40.813941\n",
      "[Epoch 0/200] [Batch 426/476] [D loss: 0.205483] [G loss: 10.276451, adv: 0.494473, cycle: 0.652215, identity: 0.651965] ETA: 2:08:15.976784\n",
      "[Epoch 0/200] [Batch 427/476] [D loss: 0.237957] [G loss: 10.858306, adv: 0.401550, cycle: 0.693191, identity: 0.704970] ETA: 2:02:50.405329\n",
      "[Epoch 0/200] [Batch 428/476] [D loss: 0.206210] [G loss: 11.363679, adv: 0.832868, cycle: 0.699841, identity: 0.706481] ETA: 2:00:04.771023\n",
      "[Epoch 0/200] [Batch 429/476] [D loss: 0.204762] [G loss: 10.978002, adv: 0.568119, cycle: 0.693252, identity: 0.695472] ETA: 2:02:49.233008\n",
      "[Epoch 0/200] [Batch 430/476] [D loss: 0.312201] [G loss: 11.054381, adv: 0.407628, cycle: 0.708393, identity: 0.712564] ETA: 1:58:30.465910\n",
      "[Epoch 0/200] [Batch 431/476] [D loss: 0.276851] [G loss: 11.531382, adv: 0.521391, cycle: 0.735938, identity: 0.730122] ETA: 1:54:47.290910\n",
      "[Epoch 0/200] [Batch 432/476] [D loss: 0.246705] [G loss: 10.831619, adv: 0.542779, cycle: 0.681464, identity: 0.694841] ETA: 2:00:00.061016\n",
      "[Epoch 0/200] [Batch 433/476] [D loss: 0.143117] [G loss: 10.959205, adv: 0.366641, cycle: 0.710184, identity: 0.698145] ETA: 1:57:22.616343\n",
      "[Epoch 0/200] [Batch 434/476] [D loss: 0.284580] [G loss: 11.428874, adv: 0.355254, cycle: 0.738998, identity: 0.736728] ETA: 1:55:53.544361\n",
      "[Epoch 0/200] [Batch 435/476] [D loss: 0.479513] [G loss: 10.735813, adv: 0.791019, cycle: 0.655798, identity: 0.677363] ETA: 2:01:56.348989\n",
      "[Epoch 0/200] [Batch 436/476] [D loss: 0.276803] [G loss: 11.867765, adv: 0.887773, cycle: 0.740006, identity: 0.715987] ETA: 2:05:33.643830\n",
      "[Epoch 0/200] [Batch 437/476] [D loss: 0.333171] [G loss: 11.270350, adv: 0.292544, cycle: 0.729287, identity: 0.736987] ETA: 2:04:16.769841\n",
      "[Epoch 0/200] [Batch 438/476] [D loss: 0.294731] [G loss: 10.917508, adv: 0.391180, cycle: 0.698536, identity: 0.708193] ETA: 1:50:24.680547\n",
      "[Epoch 0/200] [Batch 439/476] [D loss: 0.182456] [G loss: 11.290237, adv: 0.650109, cycle: 0.704232, identity: 0.719561] ETA: 2:03:09.150414\n",
      "[Epoch 0/200] [Batch 440/476] [D loss: 0.455963] [G loss: 12.319005, adv: 0.642384, cycle: 0.779705, identity: 0.775915] ETA: 1:58:59.944448\n",
      "[Epoch 0/200] [Batch 441/476] [D loss: 0.203867] [G loss: 10.834762, adv: 0.386419, cycle: 0.692789, identity: 0.704091] ETA: 1:57:06.975347\n",
      "[Epoch 0/200] [Batch 442/476] [D loss: 0.162679] [G loss: 10.314354, adv: 0.554454, cycle: 0.652630, identity: 0.646719] ETA: 2:04:54.105151\n",
      "[Epoch 0/200] [Batch 443/476] [D loss: 0.189511] [G loss: 11.080730, adv: 0.590399, cycle: 0.697203, identity: 0.703661] ETA: 2:12:34.560505\n",
      "[Epoch 0/200] [Batch 444/476] [D loss: 0.181948] [G loss: 10.625759, adv: 0.234386, cycle: 0.690160, identity: 0.697954] ETA: 2:11:52.953214\n",
      "[Epoch 0/200] [Batch 445/476] [D loss: 0.327786] [G loss: 10.681600, adv: 0.320459, cycle: 0.690311, identity: 0.691606] ETA: 1:55:27.660824\n",
      "[Epoch 0/200] [Batch 446/476] [D loss: 0.208446] [G loss: 10.943689, adv: 0.362132, cycle: 0.702540, identity: 0.711231] ETA: 2:11:13.342111\n",
      "[Epoch 0/200] [Batch 447/476] [D loss: 0.164429] [G loss: 11.186316, adv: 0.659793, cycle: 0.704564, identity: 0.696177] ETA: 2:09:28.459947\n",
      "[Epoch 0/200] [Batch 448/476] [D loss: 0.228976] [G loss: 11.082464, adv: 0.731214, cycle: 0.689076, identity: 0.692098] ETA: 2:12:28.447929\n",
      "[Epoch 0/200] [Batch 449/476] [D loss: 0.170679] [G loss: 11.579422, adv: 0.511854, cycle: 0.733659, identity: 0.746195] ETA: 2:08:43.160357\n",
      "[Epoch 0/200] [Batch 450/476] [D loss: 0.357526] [G loss: 10.825659, adv: 0.316728, cycle: 0.693249, identity: 0.715288] ETA: 2:06:59.999945\n",
      "[Epoch 0/200] [Batch 451/476] [D loss: 0.245265] [G loss: 10.338739, adv: 0.360811, cycle: 0.654749, identity: 0.686087] ETA: 2:10:17.265081\n",
      "[Epoch 0/200] [Batch 452/476] [D loss: 0.259270] [G loss: 11.977098, adv: 0.970267, cycle: 0.733947, identity: 0.733473] ETA: 2:14:48.642803\n",
      "[Epoch 0/200] [Batch 453/476] [D loss: 0.163012] [G loss: 11.458535, adv: 0.544232, cycle: 0.726725, identity: 0.729411] ETA: 2:05:14.853296\n",
      "[Epoch 0/200] [Batch 454/476] [D loss: 0.355637] [G loss: 10.786112, adv: 0.129123, cycle: 0.706598, identity: 0.718203] ETA: 1:55:22.620516\n",
      "[Epoch 0/200] [Batch 455/476] [D loss: 0.168187] [G loss: 11.552454, adv: 0.493027, cycle: 0.737482, identity: 0.736921] ETA: 2:18:58.130326\n",
      "[Epoch 0/200] [Batch 456/476] [D loss: 0.264793] [G loss: 12.186487, adv: 0.713699, cycle: 0.761310, identity: 0.771937] ETA: 2:07:45.553244\n",
      "[Epoch 0/200] [Batch 457/476] [D loss: 0.129068] [G loss: 10.686367, adv: 0.644383, cycle: 0.669180, identity: 0.670036] ETA: 2:05:44.285079\n",
      "[Epoch 0/200] [Batch 458/476] [D loss: 0.114823] [G loss: 10.782911, adv: 0.394163, cycle: 0.690795, identity: 0.696160] ETA: 2:06:27.394190\n",
      "[Epoch 0/200] [Batch 459/476] [D loss: 0.337850] [G loss: 11.713698, adv: 0.414971, cycle: 0.751763, identity: 0.756219] ETA: 2:00:50.775275\n",
      "[Epoch 0/200] [Batch 460/476] [D loss: 0.213959] [G loss: 11.331641, adv: 0.269428, cycle: 0.733508, identity: 0.745426] ETA: 1:56:35.434284\n",
      "[Epoch 0/200] [Batch 461/476] [D loss: 0.322413] [G loss: 10.990541, adv: 0.306504, cycle: 0.711652, identity: 0.713504] ETA: 2:00:16.153628\n",
      "[Epoch 0/200] [Batch 462/476] [D loss: 0.092820] [G loss: 11.414638, adv: 0.720220, cycle: 0.711170, identity: 0.716542] ETA: 2:25:59.332099\n",
      "[Epoch 0/200] [Batch 463/476] [D loss: 0.448752] [G loss: 10.770329, adv: 0.565905, cycle: 0.680548, identity: 0.679789] ETA: 2:19:16.783389\n",
      "[Epoch 0/200] [Batch 464/476] [D loss: 0.150367] [G loss: 10.674850, adv: 0.287853, cycle: 0.693854, identity: 0.689691] ETA: 2:12:12.853455\n",
      "[Epoch 0/200] [Batch 465/476] [D loss: 0.306532] [G loss: 10.745655, adv: 0.267284, cycle: 0.695936, identity: 0.703801] ETA: 2:17:00.206587\n",
      "[Epoch 0/200] [Batch 466/476] [D loss: 0.155988] [G loss: 11.271024, adv: 0.995251, cycle: 0.685499, identity: 0.684156] ETA: 2:29:55.689754\n",
      "[Epoch 0/200] [Batch 467/476] [D loss: 0.211919] [G loss: 11.706978, adv: 0.711356, cycle: 0.729009, identity: 0.741107] ETA: 2:19:20.563806\n",
      "[Epoch 0/200] [Batch 468/476] [D loss: 0.231598] [G loss: 11.521106, adv: 0.762916, cycle: 0.720679, identity: 0.710280] ETA: 2:06:54.690163\n",
      "[Epoch 0/200] [Batch 469/476] [D loss: 0.164980] [G loss: 10.459463, adv: 0.078001, cycle: 0.692754, identity: 0.690785] ETA: 2:02:23.943586\n",
      "[Epoch 0/200] [Batch 470/476] [D loss: 0.402213] [G loss: 11.153650, adv: 0.257263, cycle: 0.728416, identity: 0.722445] ETA: 1:48:47.562239\n",
      "[Epoch 0/200] [Batch 471/476] [D loss: 0.249681] [G loss: 11.434561, adv: 0.938531, cycle: 0.701478, identity: 0.696250] ETA: 2:05:28.128561\n",
      "[Epoch 0/200] [Batch 472/476] [D loss: 0.238543] [G loss: 11.459264, adv: 0.533194, cycle: 0.726139, identity: 0.732935] ETA: 2:00:53.168089\n",
      "[Epoch 0/200] [Batch 473/476] [D loss: 0.187842] [G loss: 11.043468, adv: 0.397951, cycle: 0.708070, identity: 0.712963] ETA: 2:04:27.984720\n",
      "[Epoch 0/200] [Batch 474/476] [D loss: 0.131511] [G loss: 11.160864, adv: 0.230542, cycle: 0.729567, identity: 0.726931] ETA: 2:02:09.418107\n",
      "[Epoch 0/200] [Batch 475/476] [D loss: 0.268557] [G loss: 10.652887, adv: 0.548138, cycle: 0.668899, identity: 0.683152] ETA: 1:57:53.326242\n",
      "[Epoch 1/200] [Batch 0/476] [D loss: 0.275468] [G loss: 10.681162, adv: 0.287902, cycle: 0.691837, identity: 0.694977] ETA: 8:35:17.014180\n",
      "[Epoch 1/200] [Batch 1/476] [D loss: 0.176812] [G loss: 10.945470, adv: 0.540808, cycle: 0.690927, identity: 0.699078] ETA: 2:05:56.942829\n",
      "[Epoch 1/200] [Batch 2/476] [D loss: 0.317102] [G loss: 11.526953, adv: 1.017262, cycle: 0.692351, identity: 0.717237] ETA: 2:02:31.308173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 3/476] [D loss: 0.271287] [G loss: 10.942940, adv: 0.623739, cycle: 0.684465, identity: 0.694910] ETA: 2:05:56.331606\n",
      "[Epoch 1/200] [Batch 4/476] [D loss: 0.207908] [G loss: 10.546795, adv: 0.220155, cycle: 0.686736, identity: 0.691855] ETA: 1:58:20.662231\n",
      "[Epoch 1/200] [Batch 5/476] [D loss: 0.318623] [G loss: 12.110813, adv: 0.531215, cycle: 0.770575, identity: 0.774770] ETA: 1:46:18.909705\n",
      "[Epoch 1/200] [Batch 6/476] [D loss: 0.182738] [G loss: 11.824814, adv: 0.569916, cycle: 0.746407, identity: 0.758165] ETA: 1:58:42.304444\n",
      "[Epoch 1/200] [Batch 7/476] [D loss: 0.165158] [G loss: 10.313436, adv: 0.376495, cycle: 0.659626, identity: 0.668136] ETA: 2:00:41.079855\n",
      "[Epoch 1/200] [Batch 8/476] [D loss: 0.132941] [G loss: 11.402140, adv: 0.512134, cycle: 0.724687, identity: 0.728627] ETA: 1:52:27.088717\n",
      "[Epoch 1/200] [Batch 9/476] [D loss: 0.320939] [G loss: 11.748655, adv: 0.942275, cycle: 0.720522, identity: 0.720232] ETA: 1:55:44.359970\n",
      "[Epoch 1/200] [Batch 10/476] [D loss: 0.212672] [G loss: 10.849552, adv: 0.272605, cycle: 0.701230, identity: 0.712930] ETA: 1:59:06.211116\n",
      "[Epoch 1/200] [Batch 11/476] [D loss: 0.132018] [G loss: 10.809773, adv: 0.202164, cycle: 0.706697, identity: 0.708128] ETA: 1:47:26.678694\n",
      "[Epoch 1/200] [Batch 12/476] [D loss: 0.175661] [G loss: 11.195374, adv: 0.451923, cycle: 0.711474, identity: 0.725743] ETA: 2:01:50.834505\n",
      "[Epoch 1/200] [Batch 13/476] [D loss: 0.115269] [G loss: 10.814896, adv: 0.614677, cycle: 0.683150, identity: 0.673744] ETA: 2:01:03.969769\n",
      "[Epoch 1/200] [Batch 14/476] [D loss: 0.263678] [G loss: 10.057192, adv: 0.357047, cycle: 0.645406, identity: 0.649216] ETA: 1:57:45.341649\n",
      "[Epoch 1/200] [Batch 15/476] [D loss: 0.171212] [G loss: 11.286439, adv: 0.621772, cycle: 0.710348, identity: 0.712238] ETA: 1:57:47.931535\n",
      "[Epoch 1/200] [Batch 16/476] [D loss: 0.236206] [G loss: 9.800833, adv: 0.404678, cycle: 0.623445, identity: 0.632340] ETA: 1:57:27.625095\n",
      "[Epoch 1/200] [Batch 17/476] [D loss: 0.182068] [G loss: 10.186093, adv: 0.484438, cycle: 0.646095, identity: 0.648140] ETA: 1:57:01.945065\n",
      "[Epoch 1/200] [Batch 18/476] [D loss: 0.310374] [G loss: 11.417948, adv: 0.762934, cycle: 0.708481, identity: 0.714041] ETA: 1:55:40.516370\n",
      "[Epoch 1/200] [Batch 19/476] [D loss: 0.319667] [G loss: 11.895044, adv: 0.530910, cycle: 0.752011, identity: 0.768804] ETA: 1:55:15.808926\n",
      "[Epoch 1/200] [Batch 20/476] [D loss: 0.291681] [G loss: 10.835181, adv: 0.413300, cycle: 0.689542, identity: 0.705291] ETA: 2:02:09.702827\n",
      "[Epoch 1/200] [Batch 21/476] [D loss: 0.143589] [G loss: 11.030641, adv: 0.765613, cycle: 0.682456, identity: 0.688093] ETA: 1:50:27.871520\n",
      "[Epoch 1/200] [Batch 22/476] [D loss: 0.190430] [G loss: 10.368350, adv: 0.215574, cycle: 0.679739, identity: 0.671076] ETA: 2:03:53.229501\n",
      "[Epoch 1/200] [Batch 23/476] [D loss: 0.252468] [G loss: 11.188457, adv: 0.334717, cycle: 0.716539, identity: 0.737670] ETA: 1:53:23.279213\n",
      "[Epoch 1/200] [Batch 24/476] [D loss: 0.368167] [G loss: 11.787649, adv: 1.001163, cycle: 0.719301, identity: 0.718695] ETA: 2:05:50.027800\n",
      "[Epoch 1/200] [Batch 25/476] [D loss: 0.440949] [G loss: 11.656601, adv: 0.754764, cycle: 0.719203, identity: 0.741960] ETA: 5:55:55.199065\n",
      "[Epoch 1/200] [Batch 26/476] [D loss: 0.455209] [G loss: 11.046795, adv: 0.304636, cycle: 0.716437, identity: 0.715558] ETA: 2:22:56.659836\n",
      "[Epoch 1/200] [Batch 27/476] [D loss: 0.168872] [G loss: 11.277281, adv: 0.547749, cycle: 0.713310, identity: 0.719287] ETA: 2:08:19.522763\n",
      "[Epoch 1/200] [Batch 28/476] [D loss: 0.261272] [G loss: 10.527010, adv: 0.852198, cycle: 0.642193, identity: 0.650577] ETA: 2:00:31.798136\n",
      "[Epoch 1/200] [Batch 29/476] [D loss: 0.149716] [G loss: 11.148438, adv: 0.576511, cycle: 0.701226, identity: 0.711932] ETA: 2:01:57.717741\n",
      "[Epoch 1/200] [Batch 30/476] [D loss: 0.150658] [G loss: 10.868248, adv: 0.509500, cycle: 0.686982, identity: 0.697785] ETA: 1:55:30.222423\n",
      "[Epoch 1/200] [Batch 31/476] [D loss: 0.215133] [G loss: 11.025707, adv: 0.450246, cycle: 0.699758, identity: 0.715576] ETA: 1:53:00.060196\n",
      "[Epoch 1/200] [Batch 32/476] [D loss: 0.288586] [G loss: 10.838638, adv: 0.694552, cycle: 0.671209, identity: 0.686400] ETA: 2:03:33.638509\n",
      "[Epoch 1/200] [Batch 33/476] [D loss: 0.277303] [G loss: 10.601505, adv: 0.441274, cycle: 0.675432, identity: 0.681181] ETA: 1:55:07.133283\n",
      "[Epoch 1/200] [Batch 34/476] [D loss: 0.124141] [G loss: 11.274992, adv: 1.081523, cycle: 0.674901, identity: 0.688892] ETA: 2:08:38.730063\n",
      "[Epoch 1/200] [Batch 35/476] [D loss: 0.189355] [G loss: 11.544971, adv: 0.772407, cycle: 0.713516, identity: 0.727480] ETA: 1:52:35.369553\n",
      "[Epoch 1/200] [Batch 36/476] [D loss: 0.315387] [G loss: 10.829052, adv: 0.167851, cycle: 0.711623, identity: 0.708993] ETA: 1:55:31.318436\n",
      "[Epoch 1/200] [Batch 37/476] [D loss: 0.346464] [G loss: 11.343922, adv: 0.903247, cycle: 0.694691, identity: 0.698753] ETA: 2:04:38.556928\n",
      "[Epoch 1/200] [Batch 38/476] [D loss: 0.266061] [G loss: 11.450215, adv: 0.531098, cycle: 0.723009, identity: 0.737806] ETA: 2:10:49.722203\n",
      "[Epoch 1/200] [Batch 39/476] [D loss: 0.093037] [G loss: 10.338154, adv: 0.259051, cycle: 0.672377, identity: 0.671066] ETA: 2:02:27.781963\n",
      "[Epoch 1/200] [Batch 40/476] [D loss: 0.148025] [G loss: 10.836159, adv: 0.685371, cycle: 0.675729, identity: 0.678699] ETA: 1:59:17.559981\n",
      "[Epoch 1/200] [Batch 41/476] [D loss: 0.080613] [G loss: 11.045782, adv: 0.578533, cycle: 0.694920, identity: 0.703610] ETA: 1:58:18.091703\n",
      "[Epoch 1/200] [Batch 42/476] [D loss: 0.160012] [G loss: 10.910693, adv: 0.239181, cycle: 0.707991, identity: 0.718320] ETA: 1:59:31.946415\n",
      "[Epoch 1/200] [Batch 43/476] [D loss: 0.181089] [G loss: 10.429379, adv: 0.389887, cycle: 0.668020, identity: 0.671859] ETA: 2:03:20.045723\n",
      "[Epoch 1/200] [Batch 44/476] [D loss: 0.339048] [G loss: 11.110643, adv: 0.485043, cycle: 0.702487, identity: 0.720146] ETA: 2:03:06.649218\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a7e95c480b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a7e95c480b19>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(dataloader, epoch)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mloss_GAN_AB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mfake_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_BA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mloss_GAN_BA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mloss_GAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_GAN_AB\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_GAN_BA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6b30b968a5b4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m         return F.instance_norm(\n\u001b[1;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   1943\u001b[0m     return torch.instance_norm(\n\u001b[1;32m   1944\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m         \u001b[0muse_input_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m     )\n\u001b[1;32m   1947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "\n",
    "# if epoch != 0:\n",
    "#     # Load pretrained models\n",
    "#     G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "#     G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "#     D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "#     D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "# else:\n",
    "    # Initialize weights\n",
    "    \n",
    "G_AB.apply(weights_init_normal)\n",
    "G_BA.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "epoch = 0\n",
    "img_grids = []\n",
    "images = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "def train_gan(dataloader, epoch):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "            \n",
    "#             if i%5 == 0:\n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            print(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    n_epochs,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    loss_D.item(),\n",
    "                    loss_G.item(),\n",
    "                    loss_GAN.item(),\n",
    "                    loss_cycle.item(),\n",
    "                    loss_identity.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, images)\n",
    "                \n",
    "            \n",
    "            G_losses.append(loss_G.item())\n",
    "            D_losses.append(loss_D.item())\n",
    "            \n",
    "            \n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))\n",
    "            \n",
    "            \n",
    "train_gan(dataloader3, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
