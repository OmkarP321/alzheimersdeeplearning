{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = torch.load(\"../datasets/64skulldataset.pt\")\n",
    "\n",
    "dataset = [sample for sample in dataset if sample != None]\n",
    "\n",
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "def process_gan(dataset, s):\n",
    "    \n",
    "    output = []\n",
    "    dataset = [sample[0] for sample in dataset]\n",
    "    for sample in dataset:\n",
    "        sample = sample[s][0]\n",
    "        sample /= torch.max(sample)\n",
    "        output.append(torch.unsqueeze(sample, 0))\n",
    "    return output\n",
    "\n",
    "        \n",
    "NCgan1 = process_gan(NC, 0)\n",
    "NCgan2 = process_gan(NC, 1)\n",
    "NCgan3 = process_gan(NC, 2)\n",
    "\n",
    "ADgan1 = process_gan(AD, 0)\n",
    "ADgan2 = process_gan(AD, 1)\n",
    "ADgan3 = process_gan(AD, 2)\n",
    "\n",
    "gan1 = []\n",
    "for i in range(len(ADgan1)):\n",
    "    gan1.append({\"A\": NCgan1[i], \"B\": ADgan1[i]})\n",
    "\n",
    "gan2 = []\n",
    "for i in range(len(ADgan2)):\n",
    "    gan2.append({\"A\": NCgan2[i], \"B\": ADgan2[i]})\n",
    "    \n",
    "gan3 = []\n",
    "for i in range(len(ADgan3)):\n",
    "    gan3.append({\"A\": NCgan3[i], \"B\": ADgan3[i]})\n",
    "\n",
    "batch_size = 1\n",
    "dataloader1 = DataLoader(gan1, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader2 = DataLoader(gan2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader3 = DataLoader(gan3, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))\n",
    "\n",
    "\n",
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "##############################\n",
    "#           RESNET\n",
    "##############################\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.ReflectionPad2d(3), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "##############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 100\n",
    "dataset_name = 'CycleGAN2'\n",
    "batch_size = 1\n",
    "lr=0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "n_cpu = 8\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "channels = 1\n",
    "sample_interval = 500\n",
    "checkpoint_interval = 25\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "\n",
    "\n",
    "def sample_images(batches_done, dataloader, images):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=4, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=4, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=4, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=4, normalize=True)\n",
    "\n",
    "    image_grid = torch.stack((real_A, fake_B, real_B, fake_A), 0)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "        \n",
    "def train_gan(dataloader, epoch):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "                \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "            \n",
    "           \n",
    "                \n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "            \n",
    "                print(\n",
    "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        n_epochs,\n",
    "                        i,\n",
    "                        len(dataloader),\n",
    "                        loss_D.item(),\n",
    "                        loss_G.item(),\n",
    "                        loss_GAN.item(),\n",
    "                        loss_cycle.item(),\n",
    "                        loss_identity.item(),\n",
    "                        time_left,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, dataloader, images)\n",
    "                \n",
    "            \n",
    "            G_losses.append(loss_G.item())\n",
    "            D_losses.append(loss_D.item())\n",
    "            \n",
    "            \n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/476] [D loss: 0.453004] [G loss: 2.251709, adv: 1.445777, cycle: 0.059891, identity: 0.041404] ETA: 4:04:21.734772\n",
      "[Epoch 0/200] [Batch 100/476] [D loss: 0.590417] [G loss: 2.922217, adv: 1.870370, cycle: 0.073319, identity: 0.063732] ETA: 1:48:32.766838\n",
      "[Epoch 0/200] [Batch 200/476] [D loss: 0.428210] [G loss: 1.888819, adv: 1.187153, cycle: 0.048301, identity: 0.043732] ETA: 1:40:32.062769\n",
      "[Epoch 0/200] [Batch 300/476] [D loss: 0.508606] [G loss: 2.310069, adv: 1.484732, cycle: 0.057868, identity: 0.049332] ETA: 1:52:17.253261\n",
      "[Epoch 0/200] [Batch 400/476] [D loss: 0.550286] [G loss: 2.099478, adv: 1.475994, cycle: 0.042640, identity: 0.039417] ETA: 1:41:39.013424\n",
      "[Epoch 1/200] [Batch 0/476] [D loss: 0.514739] [G loss: 2.200931, adv: 1.498567, cycle: 0.046734, identity: 0.047005] ETA: 6:12:26.310460\n",
      "[Epoch 1/200] [Batch 100/476] [D loss: 0.453647] [G loss: 2.516351, adv: 1.583667, cycle: 0.058232, identity: 0.070074] ETA: 1:52:44.561630\n",
      "[Epoch 1/200] [Batch 200/476] [D loss: 0.520705] [G loss: 2.151593, adv: 1.390085, cycle: 0.054014, identity: 0.044273] ETA: 1:46:42.714269\n",
      "[Epoch 1/200] [Batch 300/476] [D loss: 0.466130] [G loss: 2.611612, adv: 1.607205, cycle: 0.064671, identity: 0.071539] ETA: 1:49:33.045963\n",
      "[Epoch 1/200] [Batch 400/476] [D loss: 0.521388] [G loss: 2.744388, adv: 1.682283, cycle: 0.077444, identity: 0.057534] ETA: 1:38:01.149652\n",
      "[Epoch 2/200] [Batch 0/476] [D loss: 0.495469] [G loss: 2.563469, adv: 1.637079, cycle: 0.064815, identity: 0.055648] ETA: 5:01:41.000645\n",
      "[Epoch 2/200] [Batch 100/476] [D loss: 0.515187] [G loss: 2.099478, adv: 1.475994, cycle: 0.042640, identity: 0.039417] ETA: 1:48:18.075609\n",
      "[Epoch 2/200] [Batch 200/476] [D loss: 0.466006] [G loss: 2.282609, adv: 1.521542, cycle: 0.051177, identity: 0.049859] ETA: 1:48:02.652969\n",
      "[Epoch 2/200] [Batch 300/476] [D loss: 0.483298] [G loss: 2.617160, adv: 1.645809, cycle: 0.063164, identity: 0.067942] ETA: 1:52:56.779521\n",
      "[Epoch 2/200] [Batch 400/476] [D loss: 0.542348] [G loss: 2.159594, adv: 1.441028, cycle: 0.047961, identity: 0.047791] ETA: 1:40:42.308094\n",
      "[Epoch 3/200] [Batch 0/476] [D loss: 0.461985] [G loss: 1.791506, adv: 1.139291, cycle: 0.047670, identity: 0.035103] ETA: 5:00:39.875167\n",
      "[Epoch 3/200] [Batch 100/476] [D loss: 0.477335] [G loss: 2.353031, adv: 1.625188, cycle: 0.050178, identity: 0.045212] ETA: 1:42:01.805414\n",
      "[Epoch 3/200] [Batch 200/476] [D loss: 0.492327] [G loss: 2.215408, adv: 1.332260, cycle: 0.055356, identity: 0.065918] ETA: 1:42:23.848269\n",
      "[Epoch 3/200] [Batch 300/476] [D loss: 0.448524] [G loss: 2.489860, adv: 1.693252, cycle: 0.059662, identity: 0.039998] ETA: 1:40:57.010132\n",
      "[Epoch 3/200] [Batch 400/476] [D loss: 0.420092] [G loss: 2.230638, adv: 1.338627, cycle: 0.061377, identity: 0.055649] ETA: 1:47:09.022161\n",
      "[Epoch 4/200] [Batch 0/476] [D loss: 0.511159] [G loss: 2.229731, adv: 1.357040, cycle: 0.063098, identity: 0.048342] ETA: 4:51:06.018635\n",
      "[Epoch 4/200] [Batch 100/476] [D loss: 0.455038] [G loss: 2.021915, adv: 1.325482, cycle: 0.046784, identity: 0.045719] ETA: 1:44:11.078576\n",
      "[Epoch 4/200] [Batch 200/476] [D loss: 0.433876] [G loss: 2.094182, adv: 1.338816, cycle: 0.051251, identity: 0.048572] ETA: 1:29:58.999077\n",
      "[Epoch 4/200] [Batch 300/476] [D loss: 0.496366] [G loss: 2.393486, adv: 1.620865, cycle: 0.052398, identity: 0.049728] ETA: 1:40:19.225379\n",
      "[Epoch 4/200] [Batch 400/476] [D loss: 0.440725] [G loss: 2.031786, adv: 1.278511, cycle: 0.051146, identity: 0.048362] ETA: 2:02:50.300438\n",
      "[Epoch 5/200] [Batch 0/476] [D loss: 0.387834] [G loss: 2.406246, adv: 1.321210, cycle: 0.080019, identity: 0.056970] ETA: 5:06:54.184241\n",
      "[Epoch 5/200] [Batch 100/476] [D loss: 0.441338] [G loss: 2.051573, adv: 1.334707, cycle: 0.045958, identity: 0.051457] ETA: 2:02:09.941959\n",
      "[Epoch 5/200] [Batch 200/476] [D loss: 0.416937] [G loss: 2.454028, adv: 1.481028, cycle: 0.070319, identity: 0.053962] ETA: 1:41:10.763421\n",
      "[Epoch 5/200] [Batch 300/476] [D loss: 0.502093] [G loss: 2.845007, adv: 1.689590, cycle: 0.087226, identity: 0.056632] ETA: 1:35:16.258364\n",
      "[Epoch 5/200] [Batch 400/476] [D loss: 0.470164] [G loss: 2.773001, adv: 1.545475, cycle: 0.086790, identity: 0.071926] ETA: 1:40:28.017855\n",
      "[Epoch 6/200] [Batch 0/476] [D loss: 0.554850] [G loss: 2.435584, adv: 1.566130, cycle: 0.056341, identity: 0.061208] ETA: 4:46:29.820417\n",
      "[Epoch 6/200] [Batch 100/476] [D loss: 0.578533] [G loss: 2.165452, adv: 1.316214, cycle: 0.057697, identity: 0.054454] ETA: 1:46:20.671260\n",
      "[Epoch 6/200] [Batch 200/476] [D loss: 0.521295] [G loss: 2.409647, adv: 1.493519, cycle: 0.062272, identity: 0.058682] ETA: 1:51:36.256687\n",
      "[Epoch 6/200] [Batch 300/476] [D loss: 0.420389] [G loss: 2.434808, adv: 1.205119, cycle: 0.084907, identity: 0.076123] ETA: 1:45:34.643612\n",
      "[Epoch 6/200] [Batch 400/476] [D loss: 0.440130] [G loss: 2.599561, adv: 1.573478, cycle: 0.065873, identity: 0.073471] ETA: 1:44:50.605059\n",
      "[Epoch 7/200] [Batch 0/476] [D loss: 0.450088] [G loss: 2.051573, adv: 1.334707, cycle: 0.045958, identity: 0.051457] ETA: 4:33:21.016776\n",
      "[Epoch 7/200] [Batch 100/476] [D loss: 0.544950] [G loss: 2.001246, adv: 1.409512, cycle: 0.040465, identity: 0.037417] ETA: 1:27:47.372719\n",
      "[Epoch 7/200] [Batch 200/476] [D loss: 0.465900] [G loss: 2.310787, adv: 1.439049, cycle: 0.063863, identity: 0.046622] ETA: 1:44:14.412309\n",
      "[Epoch 7/200] [Batch 300/476] [D loss: 0.459738] [G loss: 2.602907, adv: 1.487067, cycle: 0.078391, identity: 0.066385] ETA: 1:33:49.779442\n",
      "[Epoch 7/200] [Batch 400/476] [D loss: 0.543446] [G loss: 2.145318, adv: 1.276611, cycle: 0.055441, identity: 0.062859] ETA: 1:35:07.917893\n",
      "[Epoch 8/200] [Batch 0/476] [D loss: 0.490324] [G loss: 2.218308, adv: 1.247061, cycle: 0.065196, identity: 0.063857] ETA: 4:57:51.833130\n",
      "[Epoch 8/200] [Batch 100/476] [D loss: 0.508963] [G loss: 2.256607, adv: 1.630401, cycle: 0.042342, identity: 0.040557] ETA: 1:36:46.590529\n",
      "[Epoch 8/200] [Batch 200/476] [D loss: 0.530840] [G loss: 2.634786, adv: 1.571888, cycle: 0.082098, identity: 0.048384] ETA: 1:32:10.739628\n",
      "[Epoch 8/200] [Batch 300/476] [D loss: 0.372991] [G loss: 1.955230, adv: 1.102427, cycle: 0.057206, identity: 0.056148] ETA: 1:38:32.124258\n",
      "[Epoch 8/200] [Batch 400/476] [D loss: 0.583160] [G loss: 2.922217, adv: 1.870370, cycle: 0.073319, identity: 0.063732] ETA: 1:31:28.932106\n",
      "[Epoch 9/200] [Batch 0/476] [D loss: 0.503272] [G loss: 2.393486, adv: 1.620865, cycle: 0.052398, identity: 0.049728] ETA: 4:51:41.036940\n",
      "[Epoch 9/200] [Batch 100/476] [D loss: 0.516134] [G loss: 2.200931, adv: 1.498567, cycle: 0.046734, identity: 0.047005] ETA: 1:39:06.501205\n",
      "[Epoch 9/200] [Batch 200/476] [D loss: 0.457989] [G loss: 1.946346, adv: 1.295732, cycle: 0.044255, identity: 0.041612] ETA: 1:46:46.628901\n",
      "[Epoch 9/200] [Batch 300/476] [D loss: 0.445996] [G loss: 1.908765, adv: 1.198684, cycle: 0.046678, identity: 0.048660] ETA: 1:39:07.729301\n",
      "[Epoch 9/200] [Batch 400/476] [D loss: 0.476089] [G loss: 2.046082, adv: 1.279859, cycle: 0.050284, identity: 0.052676] ETA: 1:24:29.219193\n",
      "[Epoch 10/200] [Batch 0/476] [D loss: 0.492308] [G loss: 2.395535, adv: 1.597222, cycle: 0.054551, identity: 0.050561] ETA: 4:46:22.527046\n",
      "[Epoch 10/200] [Batch 100/476] [D loss: 0.483892] [G loss: 2.316168, adv: 1.295808, cycle: 0.070420, identity: 0.063231] ETA: 1:36:20.759397\n",
      "[Epoch 10/200] [Batch 200/476] [D loss: 0.457987] [G loss: 1.809701, adv: 1.047339, cycle: 0.051646, identity: 0.049181] ETA: 1:52:59.062958\n",
      "[Epoch 10/200] [Batch 300/476] [D loss: 0.419204] [G loss: 2.100599, adv: 1.195139, cycle: 0.059777, identity: 0.061538] ETA: 1:38:36.292844\n",
      "[Epoch 10/200] [Batch 400/476] [D loss: 0.407487] [G loss: 2.590157, adv: 1.551183, cycle: 0.075913, identity: 0.055969] ETA: 1:38:48.191195\n",
      "[Epoch 11/200] [Batch 0/476] [D loss: 0.536363] [G loss: 2.505126, adv: 1.615615, cycle: 0.055594, identity: 0.066715] ETA: 4:56:43.387384\n",
      "[Epoch 11/200] [Batch 100/476] [D loss: 0.514157] [G loss: 2.491184, adv: 1.477674, cycle: 0.071114, identity: 0.060473] ETA: 1:29:19.996941\n",
      "[Epoch 11/200] [Batch 200/476] [D loss: 0.458827] [G loss: 2.393739, adv: 1.504540, cycle: 0.065130, identity: 0.047581] ETA: 1:37:07.945095\n",
      "[Epoch 11/200] [Batch 300/476] [D loss: 0.457607] [G loss: 2.928397, adv: 1.851648, cycle: 0.079415, identity: 0.056520] ETA: 1:37:12.141357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/200] [Batch 400/476] [D loss: 0.423341] [G loss: 1.869408, adv: 1.205441, cycle: 0.049268, identity: 0.034257] ETA: 1:46:09.388084\n",
      "[Epoch 12/200] [Batch 0/476] [D loss: 0.397314] [G loss: 2.362360, adv: 1.563859, cycle: 0.053647, identity: 0.052407] ETA: 4:37:15.603420\n",
      "[Epoch 12/200] [Batch 100/476] [D loss: 0.498287] [G loss: 2.436456, adv: 1.630832, cycle: 0.054671, identity: 0.051782] ETA: 1:31:27.245393\n",
      "[Epoch 12/200] [Batch 200/476] [D loss: 0.459675] [G loss: 2.342281, adv: 1.328782, cycle: 0.064769, identity: 0.073162] ETA: 1:36:37.764492\n",
      "[Epoch 12/200] [Batch 300/476] [D loss: 0.436274] [G loss: 2.448567, adv: 1.519252, cycle: 0.067952, identity: 0.049958] ETA: 1:32:36.834722\n",
      "[Epoch 12/200] [Batch 400/476] [D loss: 0.544095] [G loss: 2.589240, adv: 1.566826, cycle: 0.071475, identity: 0.061532] ETA: 1:30:05.915771\n",
      "[Epoch 13/200] [Batch 0/476] [D loss: 0.543753] [G loss: 3.367979, adv: 2.084400, cycle: 0.093810, identity: 0.069096] ETA: 4:52:14.857046\n",
      "[Epoch 13/200] [Batch 100/476] [D loss: 0.358984] [G loss: 2.091136, adv: 1.092890, cycle: 0.071293, identity: 0.057063] ETA: 1:33:01.441608\n",
      "[Epoch 13/200] [Batch 200/476] [D loss: 0.528039] [G loss: 2.139818, adv: 1.304358, cycle: 0.064331, identity: 0.038431] ETA: 1:32:25.731660\n",
      "[Epoch 13/200] [Batch 300/476] [D loss: 0.357443] [G loss: 2.115832, adv: 1.269764, cycle: 0.055656, identity: 0.057902] ETA: 1:35:38.197094\n",
      "[Epoch 13/200] [Batch 400/476] [D loss: 0.504750] [G loss: 2.059899, adv: 1.416952, cycle: 0.040081, identity: 0.048428] ETA: 1:29:28.602221\n",
      "[Epoch 14/200] [Batch 0/476] [D loss: 0.432427] [G loss: 1.785925, adv: 0.924386, cycle: 0.058981, identity: 0.054346] ETA: 4:54:48.670847\n",
      "[Epoch 14/200] [Batch 100/476] [D loss: 0.451203] [G loss: 2.242674, adv: 1.432576, cycle: 0.053751, identity: 0.054517] ETA: 1:38:22.432673\n",
      "[Epoch 14/200] [Batch 200/476] [D loss: 0.534413] [G loss: 2.514939, adv: 1.509026, cycle: 0.074931, identity: 0.051321] ETA: 1:32:22.692772\n",
      "[Epoch 14/200] [Batch 300/476] [D loss: 0.511259] [G loss: 2.646130, adv: 1.650948, cycle: 0.069940, identity: 0.059156] ETA: 1:32:31.543891\n",
      "[Epoch 14/200] [Batch 400/476] [D loss: 0.479979] [G loss: 2.392483, adv: 1.319600, cycle: 0.079935, identity: 0.054708] ETA: 1:33:24.257425\n",
      "[Epoch 15/200] [Batch 0/476] [D loss: 0.503094] [G loss: 2.200931, adv: 1.498567, cycle: 0.046734, identity: 0.047005] ETA: 4:37:30.426784\n",
      "[Epoch 15/200] [Batch 100/476] [D loss: 0.482356] [G loss: 2.602794, adv: 1.597283, cycle: 0.070106, identity: 0.060891] ETA: 1:33:11.346560\n",
      "[Epoch 15/200] [Batch 200/476] [D loss: 0.490427] [G loss: 2.368036, adv: 1.407190, cycle: 0.067905, identity: 0.056360] ETA: 1:40:07.646694\n",
      "[Epoch 15/200] [Batch 300/476] [D loss: 0.475064] [G loss: 2.369983, adv: 1.359801, cycle: 0.074200, identity: 0.053637] ETA: 1:36:28.245945\n",
      "[Epoch 15/200] [Batch 400/476] [D loss: 0.462371] [G loss: 2.310787, adv: 1.439049, cycle: 0.063863, identity: 0.046622] ETA: 1:42:39.915390\n",
      "[Epoch 16/200] [Batch 0/476] [D loss: 0.537509] [G loss: 2.409564, adv: 1.369384, cycle: 0.074442, identity: 0.059153] ETA: 4:33:39.013924\n",
      "[Epoch 16/200] [Batch 100/476] [D loss: 0.408551] [G loss: 2.014589, adv: 1.163932, cycle: 0.057237, identity: 0.055657] ETA: 1:33:35.110435\n",
      "[Epoch 16/200] [Batch 200/476] [D loss: 0.506054] [G loss: 2.310733, adv: 1.583776, cycle: 0.046568, identity: 0.052256] ETA: 1:28:35.495544\n",
      "[Epoch 16/200] [Batch 300/476] [D loss: 0.320710] [G loss: 2.361276, adv: 1.481163, cycle: 0.059838, identity: 0.056347] ETA: 1:32:52.785599\n",
      "[Epoch 16/200] [Batch 400/476] [D loss: 0.515810] [G loss: 2.002154, adv: 1.168095, cycle: 0.057724, identity: 0.051363] ETA: 1:31:34.002308\n",
      "[Epoch 17/200] [Batch 0/476] [D loss: 0.472344] [G loss: 2.128031, adv: 1.213057, cycle: 0.062121, identity: 0.058753] ETA: 4:31:22.262589\n",
      "[Epoch 17/200] [Batch 100/476] [D loss: 0.486407] [G loss: 1.890059, adv: 0.981964, cycle: 0.060691, identity: 0.060238] ETA: 1:30:14.185516\n",
      "[Epoch 17/200] [Batch 200/476] [D loss: 0.493351] [G loss: 2.122748, adv: 1.330571, cycle: 0.054739, identity: 0.048958] ETA: 1:35:16.159334\n",
      "[Epoch 17/200] [Batch 300/476] [D loss: 0.492208] [G loss: 2.683531, adv: 1.806323, cycle: 0.063855, identity: 0.047731] ETA: 1:37:57.204168\n",
      "[Epoch 17/200] [Batch 400/476] [D loss: 0.534862] [G loss: 2.486782, adv: 1.604007, cycle: 0.065002, identity: 0.046551] ETA: 1:33:22.162918\n",
      "[Epoch 18/200] [Batch 0/476] [D loss: 0.576431] [G loss: 2.196598, adv: 1.384143, cycle: 0.058595, identity: 0.045301] ETA: 4:24:55.324087\n",
      "[Epoch 18/200] [Batch 100/476] [D loss: 0.526400] [G loss: 2.145318, adv: 1.276611, cycle: 0.055441, identity: 0.062859] ETA: 1:39:40.549404\n",
      "[Epoch 18/200] [Batch 200/476] [D loss: 0.422688] [G loss: 2.152011, adv: 1.320026, cycle: 0.062931, identity: 0.040536] ETA: 1:26:34.528778\n",
      "[Epoch 18/200] [Batch 300/476] [D loss: 0.462589] [G loss: 2.352597, adv: 1.506616, cycle: 0.060196, identity: 0.048804] ETA: 1:33:00.916042\n",
      "[Epoch 18/200] [Batch 400/476] [D loss: 0.427790] [G loss: 2.643789, adv: 1.411753, cycle: 0.084688, identity: 0.077032] ETA: 1:28:29.154213\n",
      "[Epoch 19/200] [Batch 0/476] [D loss: 0.467760] [G loss: 2.316168, adv: 1.295808, cycle: 0.070420, identity: 0.063231] ETA: 4:29:02.582603\n",
      "[Epoch 19/200] [Batch 100/476] [D loss: 0.478413] [G loss: 2.173418, adv: 1.452359, cycle: 0.045452, identity: 0.053307] ETA: 1:31:43.019793\n",
      "[Epoch 19/200] [Batch 200/476] [D loss: 0.444643] [G loss: 2.251709, adv: 1.445777, cycle: 0.059891, identity: 0.041404] ETA: 1:34:35.492429\n",
      "[Epoch 19/200] [Batch 300/476] [D loss: 0.517019] [G loss: 2.379993, adv: 1.478881, cycle: 0.062542, identity: 0.055139] ETA: 1:32:51.331215\n",
      "[Epoch 19/200] [Batch 400/476] [D loss: 0.450258] [G loss: 2.223009, adv: 1.277307, cycle: 0.067214, identity: 0.054712] ETA: 1:35:08.617089\n",
      "[Epoch 20/200] [Batch 0/476] [D loss: 0.427851] [G loss: 2.055321, adv: 1.270918, cycle: 0.056015, identity: 0.044851] ETA: 4:29:34.267788\n",
      "[Epoch 20/200] [Batch 100/476] [D loss: 0.396330] [G loss: 2.014589, adv: 1.163932, cycle: 0.057237, identity: 0.055657] ETA: 1:27:23.833342\n",
      "[Epoch 20/200] [Batch 200/476] [D loss: 0.462225] [G loss: 2.246890, adv: 1.365961, cycle: 0.061361, identity: 0.053465] ETA: 1:26:00.139580\n",
      "[Epoch 20/200] [Batch 300/476] [D loss: 0.486352] [G loss: 2.122530, adv: 1.315389, cycle: 0.057889, identity: 0.045650] ETA: 1:31:08.443022\n",
      "[Epoch 20/200] [Batch 400/476] [D loss: 0.509627] [G loss: 2.379993, adv: 1.478881, cycle: 0.062542, identity: 0.055139] ETA: 1:31:17.653427\n",
      "[Epoch 21/200] [Batch 0/476] [D loss: 0.521132] [G loss: 2.229731, adv: 1.357040, cycle: 0.063098, identity: 0.048342] ETA: 4:19:43.340278\n",
      "[Epoch 21/200] [Batch 100/476] [D loss: 0.436365] [G loss: 2.528434, adv: 1.495501, cycle: 0.074188, identity: 0.058211] ETA: 1:38:16.829292\n",
      "[Epoch 21/200] [Batch 200/476] [D loss: 0.532045] [G loss: 2.635903, adv: 1.714867, cycle: 0.065217, identity: 0.053772] ETA: 1:30:30.458023\n",
      "[Epoch 21/200] [Batch 300/476] [D loss: 0.438504] [G loss: 2.250058, adv: 1.480783, cycle: 0.051913, identity: 0.050030] ETA: 1:30:53.725092\n",
      "[Epoch 21/200] [Batch 400/476] [D loss: 0.474439] [G loss: 2.008260, adv: 1.201681, cycle: 0.057555, identity: 0.046205] ETA: 1:31:09.845707\n",
      "[Epoch 22/200] [Batch 0/476] [D loss: 0.514157] [G loss: 2.491184, adv: 1.477674, cycle: 0.071114, identity: 0.060473] ETA: 4:21:46.046885\n",
      "[Epoch 22/200] [Batch 100/476] [D loss: 0.495593] [G loss: 1.853250, adv: 1.248315, cycle: 0.037172, identity: 0.046644] ETA: 1:33:49.795507\n",
      "[Epoch 22/200] [Batch 200/476] [D loss: 0.447038] [G loss: 2.223009, adv: 1.277307, cycle: 0.067214, identity: 0.054712] ETA: 1:26:40.150829\n",
      "[Epoch 22/200] [Batch 300/476] [D loss: 0.515843] [G loss: 2.646130, adv: 1.650948, cycle: 0.069940, identity: 0.059156] ETA: 1:31:45.135946\n",
      "[Epoch 22/200] [Batch 400/476] [D loss: 0.569522] [G loss: 1.976728, adv: 1.086671, cycle: 0.057895, identity: 0.062220] ETA: 1:25:35.412073\n",
      "[Epoch 23/200] [Batch 0/476] [D loss: 0.528376] [G loss: 2.456340, adv: 1.556754, cycle: 0.067091, identity: 0.045735] ETA: 4:21:52.120268\n",
      "[Epoch 23/200] [Batch 100/476] [D loss: 0.519185] [G loss: 2.059718, adv: 1.279248, cycle: 0.050456, identity: 0.055181] ETA: 1:24:30.101501\n",
      "[Epoch 23/200] [Batch 200/476] [D loss: 0.446521] [G loss: 1.869408, adv: 1.205441, cycle: 0.049268, identity: 0.034257] ETA: 1:29:21.724130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/200] [Batch 300/476] [D loss: 0.477721] [G loss: 2.617160, adv: 1.645809, cycle: 0.063164, identity: 0.067942] ETA: 1:23:11.099064\n",
      "[Epoch 23/200] [Batch 400/476] [D loss: 0.503884] [G loss: 2.935109, adv: 1.830579, cycle: 0.075980, identity: 0.068947] ETA: 1:28:59.489869\n",
      "[Epoch 24/200] [Batch 0/476] [D loss: 0.461209] [G loss: 2.046082, adv: 1.279859, cycle: 0.050284, identity: 0.052676] ETA: 4:11:23.081131\n",
      "[Epoch 24/200] [Batch 100/476] [D loss: 0.448921] [G loss: 2.094182, adv: 1.338816, cycle: 0.051251, identity: 0.048572] ETA: 1:28:02.118522\n",
      "[Epoch 24/200] [Batch 200/476] [D loss: 0.502411] [G loss: 2.086637, adv: 1.237586, cycle: 0.054390, identity: 0.061030] ETA: 1:31:52.706997\n",
      "[Epoch 24/200] [Batch 300/476] [D loss: 0.426006] [G loss: 2.239276, adv: 1.256577, cycle: 0.068099, identity: 0.060343] ETA: 1:32:04.281692\n",
      "[Epoch 24/200] [Batch 400/476] [D loss: 0.474662] [G loss: 2.013623, adv: 1.293030, cycle: 0.045923, identity: 0.052273] ETA: 1:33:32.682575\n",
      "[Epoch 25/200] [Batch 0/476] [D loss: 0.426013] [G loss: 2.432786, adv: 1.522872, cycle: 0.062980, identity: 0.056022] ETA: 4:12:11.537914\n",
      "[Epoch 25/200] [Batch 100/476] [D loss: 0.480286] [G loss: 2.199548, adv: 1.483389, cycle: 0.052139, identity: 0.038954] ETA: 1:27:18.462830\n",
      "[Epoch 25/200] [Batch 200/476] [D loss: 0.535600] [G loss: 2.505126, adv: 1.615615, cycle: 0.055594, identity: 0.066715] ETA: 1:32:26.968746\n",
      "[Epoch 25/200] [Batch 300/476] [D loss: 0.457925] [G loss: 2.073128, adv: 1.472404, cycle: 0.043514, identity: 0.033117] ETA: 1:25:58.746958\n",
      "[Epoch 25/200] [Batch 400/476] [D loss: 0.421582] [G loss: 2.105554, adv: 1.191653, cycle: 0.063138, identity: 0.056504] ETA: 1:38:26.325364\n",
      "[Epoch 26/200] [Batch 0/476] [D loss: 0.456677] [G loss: 2.310849, adv: 1.476599, cycle: 0.055479, identity: 0.055891] ETA: 5:33:23.172592\n",
      "[Epoch 26/200] [Batch 100/476] [D loss: 0.465600] [G loss: 2.261728, adv: 1.380173, cycle: 0.056567, identity: 0.063177] ETA: 1:20:50.048093\n",
      "[Epoch 26/200] [Batch 200/476] [D loss: 0.468106] [G loss: 2.935109, adv: 1.830579, cycle: 0.075980, identity: 0.068947] ETA: 1:30:34.389801\n",
      "[Epoch 26/200] [Batch 300/476] [D loss: 0.474516] [G loss: 2.393739, adv: 1.504540, cycle: 0.065130, identity: 0.047581] ETA: 1:24:40.465599\n",
      "[Epoch 26/200] [Batch 400/476] [D loss: 0.462170] [G loss: 2.599561, adv: 1.573478, cycle: 0.065873, identity: 0.073471] ETA: 1:35:19.366880\n",
      "[Epoch 27/200] [Batch 0/476] [D loss: 0.554277] [G loss: 2.105989, adv: 1.348222, cycle: 0.053812, identity: 0.043929] ETA: 4:16:25.689078\n",
      "[Epoch 27/200] [Batch 100/476] [D loss: 0.356486] [G loss: 2.115832, adv: 1.269764, cycle: 0.055656, identity: 0.057902] ETA: 1:23:49.804647\n",
      "[Epoch 27/200] [Batch 200/476] [D loss: 0.490203] [G loss: 2.269391, adv: 1.489643, cycle: 0.057162, identity: 0.041626] ETA: 1:23:57.810459\n",
      "[Epoch 27/200] [Batch 300/476] [D loss: 0.533732] [G loss: 2.456072, adv: 1.622720, cycle: 0.059636, identity: 0.047398] ETA: 1:21:57.613190\n",
      "[Epoch 27/200] [Batch 400/476] [D loss: 0.458781] [G loss: 2.291028, adv: 1.281720, cycle: 0.068780, identity: 0.064302] ETA: 1:27:27.828235\n",
      "[Epoch 28/200] [Batch 0/476] [D loss: 0.550363] [G loss: 2.426405, adv: 1.496229, cycle: 0.066043, identity: 0.053950] ETA: 4:13:35.064072\n",
      "[Epoch 28/200] [Batch 100/476] [D loss: 0.458659] [G loss: 1.791506, adv: 1.139291, cycle: 0.047670, identity: 0.035103] ETA: 1:33:30.918958\n",
      "[Epoch 28/200] [Batch 200/476] [D loss: 0.461391] [G loss: 2.211867, adv: 1.439206, cycle: 0.054068, identity: 0.046397] ETA: 1:28:24.342390\n",
      "[Epoch 28/200] [Batch 300/476] [D loss: 0.480486] [G loss: 1.859900, adv: 1.167159, cycle: 0.039463, identity: 0.059623] ETA: 1:21:06.426498\n",
      "[Epoch 28/200] [Batch 400/476] [D loss: 0.514376] [G loss: 2.456340, adv: 1.556754, cycle: 0.067091, identity: 0.045735] ETA: 1:32:07.087997\n",
      "[Epoch 29/200] [Batch 0/476] [D loss: 0.538170] [G loss: 2.446982, adv: 1.528831, cycle: 0.070414, identity: 0.042802] ETA: 4:03:22.633793\n",
      "[Epoch 29/200] [Batch 100/476] [D loss: 0.444422] [G loss: 1.943819, adv: 1.248048, cycle: 0.049544, identity: 0.040067] ETA: 1:18:19.766064\n",
      "[Epoch 29/200] [Batch 200/476] [D loss: 0.463434] [G loss: 2.274371, adv: 1.453143, cycle: 0.059991, identity: 0.044264] ETA: 1:23:43.623845\n",
      "[Epoch 29/200] [Batch 300/476] [D loss: 0.538813] [G loss: 2.509790, adv: 1.390874, cycle: 0.078983, identity: 0.065817] ETA: 1:26:50.185364\n",
      "[Epoch 29/200] [Batch 400/476] [D loss: 0.449387] [G loss: 1.863874, adv: 1.323571, cycle: 0.034798, identity: 0.038465] ETA: 1:32:30.257049\n",
      "[Epoch 30/200] [Batch 0/476] [D loss: 0.484378] [G loss: 2.251709, adv: 1.445777, cycle: 0.059891, identity: 0.041404] ETA: 4:23:45.157194\n",
      "[Epoch 30/200] [Batch 100/476] [D loss: 0.571705] [G loss: 2.105989, adv: 1.348222, cycle: 0.053812, identity: 0.043929] ETA: 1:21:52.127895\n",
      "[Epoch 30/200] [Batch 200/476] [D loss: 0.461044] [G loss: 2.122748, adv: 1.330571, cycle: 0.054739, identity: 0.048958] ETA: 1:22:02.620106\n",
      "[Epoch 30/200] [Batch 300/476] [D loss: 0.537396] [G loss: 2.145318, adv: 1.276611, cycle: 0.055441, identity: 0.062859] ETA: 1:21:09.871607\n",
      "[Epoch 30/200] [Batch 400/476] [D loss: 0.487010] [G loss: 2.845007, adv: 1.689590, cycle: 0.087226, identity: 0.056632] ETA: 1:22:34.020767\n",
      "[Epoch 31/200] [Batch 0/476] [D loss: 0.528644] [G loss: 2.059899, adv: 1.416952, cycle: 0.040081, identity: 0.048428] ETA: 4:30:52.250207\n",
      "[Epoch 31/200] [Batch 100/476] [D loss: 0.508995] [G loss: 2.173418, adv: 1.452359, cycle: 0.045452, identity: 0.053307] ETA: 1:20:45.824888\n",
      "[Epoch 31/200] [Batch 200/476] [D loss: 0.488079] [G loss: 1.841742, adv: 1.205466, cycle: 0.043214, identity: 0.040827] ETA: 1:22:34.200871\n",
      "[Epoch 31/200] [Batch 300/476] [D loss: 0.344534] [G loss: 2.194009, adv: 1.128836, cycle: 0.074868, identity: 0.063298] ETA: 1:20:40.392609\n",
      "[Epoch 31/200] [Batch 400/476] [D loss: 0.598055] [G loss: 2.234081, adv: 1.535463, cycle: 0.048566, identity: 0.042591] ETA: 1:29:27.864643\n",
      "[Epoch 32/200] [Batch 0/476] [D loss: 0.427218] [G loss: 2.699980, adv: 1.707792, cycle: 0.067019, identity: 0.064399] ETA: 4:02:05.361237\n",
      "[Epoch 32/200] [Batch 100/476] [D loss: 0.522889] [G loss: 2.225061, adv: 1.409520, cycle: 0.054571, identity: 0.053966] ETA: 1:28:31.122677\n",
      "[Epoch 32/200] [Batch 200/476] [D loss: 0.491192] [G loss: 2.228068, adv: 1.496339, cycle: 0.053349, identity: 0.039647] ETA: 1:22:11.488388\n",
      "[Epoch 32/200] [Batch 300/476] [D loss: 0.535604] [G loss: 2.486782, adv: 1.604007, cycle: 0.065002, identity: 0.046551] ETA: 1:22:26.788688\n",
      "[Epoch 32/200] [Batch 400/476] [D loss: 0.496230] [G loss: 2.057936, adv: 1.417429, cycle: 0.043302, identity: 0.041497] ETA: 1:17:26.878315\n",
      "[Epoch 33/200] [Batch 0/476] [D loss: 0.516138] [G loss: 1.858650, adv: 0.985118, cycle: 0.058834, identity: 0.057038] ETA: 4:02:23.423072\n",
      "[Epoch 33/200] [Batch 100/476] [D loss: 0.487439] [G loss: 2.456340, adv: 1.556754, cycle: 0.067091, identity: 0.045735] ETA: 1:25:02.525391\n",
      "[Epoch 33/200] [Batch 200/476] [D loss: 0.473599] [G loss: 1.996749, adv: 1.336204, cycle: 0.040643, identity: 0.050823] ETA: 1:20:15.193660\n",
      "[Epoch 33/200] [Batch 300/476] [D loss: 0.454477] [G loss: 2.525631, adv: 1.493240, cycle: 0.072651, identity: 0.061176] ETA: 1:15:05.365904\n",
      "[Epoch 33/200] [Batch 400/476] [D loss: 0.457253] [G loss: 2.364021, adv: 1.398483, cycle: 0.067953, identity: 0.057202] ETA: 1:26:56.601105\n",
      "[Epoch 34/200] [Batch 0/476] [D loss: 0.618969] [G loss: 2.757257, adv: 1.701081, cycle: 0.074965, identity: 0.061305] ETA: 4:00:50.421297\n",
      "[Epoch 34/200] [Batch 100/476] [D loss: 0.455988] [G loss: 2.467417, adv: 1.583200, cycle: 0.054889, identity: 0.067066] ETA: 1:26:45.331499\n",
      "[Epoch 34/200] [Batch 200/476] [D loss: 0.562437] [G loss: 2.285561, adv: 1.324404, cycle: 0.061952, identity: 0.068327] ETA: 1:17:35.143661\n",
      "[Epoch 34/200] [Batch 300/476] [D loss: 0.523247] [G loss: 2.213477, adv: 1.464651, cycle: 0.050661, identity: 0.048444] ETA: 1:24:35.875642\n",
      "[Epoch 34/200] [Batch 400/476] [D loss: 0.467203] [G loss: 2.496682, adv: 1.557470, cycle: 0.065608, identity: 0.056627] ETA: 1:13:55.765289\n",
      "[Epoch 35/200] [Batch 0/476] [D loss: 0.482778] [G loss: 2.489727, adv: 1.547325, cycle: 0.061473, identity: 0.065534] ETA: 4:06:13.026037\n",
      "[Epoch 35/200] [Batch 100/476] [D loss: 0.444224] [G loss: 2.619126, adv: 1.451706, cycle: 0.077948, identity: 0.077588] ETA: 1:19:26.876335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/200] [Batch 200/476] [D loss: 0.520717] [G loss: 1.944502, adv: 1.281647, cycle: 0.042490, identity: 0.047590] ETA: 1:19:24.964366\n",
      "[Epoch 35/200] [Batch 300/476] [D loss: 0.530924] [G loss: 2.504402, adv: 1.565632, cycle: 0.065958, identity: 0.055838] ETA: 1:21:55.350609\n",
      "[Epoch 35/200] [Batch 400/476] [D loss: 0.511608] [G loss: 2.564733, adv: 1.535026, cycle: 0.076718, identity: 0.052505] ETA: 1:23:43.568358\n",
      "[Epoch 36/200] [Batch 0/476] [D loss: 0.458950] [G loss: 2.109517, adv: 1.368830, cycle: 0.049977, identity: 0.048183] ETA: 3:55:27.443184\n",
      "[Epoch 36/200] [Batch 100/476] [D loss: 0.437432] [G loss: 1.869408, adv: 1.205441, cycle: 0.049268, identity: 0.034257] ETA: 1:21:19.311587\n",
      "[Epoch 36/200] [Batch 200/476] [D loss: 0.530694] [G loss: 2.257644, adv: 1.337681, cycle: 0.063516, identity: 0.056961] ETA: 1:22:10.732216\n",
      "[Epoch 36/200] [Batch 300/476] [D loss: 0.457076] [G loss: 2.957239, adv: 1.580116, cycle: 0.101603, identity: 0.072218] ETA: 1:16:53.570212\n",
      "[Epoch 36/200] [Batch 400/476] [D loss: 0.511351] [G loss: 2.007373, adv: 1.257823, cycle: 0.050448, identity: 0.049013] ETA: 1:27:24.106476\n",
      "[Epoch 37/200] [Batch 0/476] [D loss: 0.452968] [G loss: 2.617160, adv: 1.645809, cycle: 0.063164, identity: 0.067942] ETA: 3:54:04.019506\n",
      "[Epoch 37/200] [Batch 100/476] [D loss: 0.451313] [G loss: 2.127079, adv: 1.257352, cycle: 0.058327, identity: 0.057292] ETA: 1:23:58.756638\n",
      "[Epoch 37/200] [Batch 200/476] [D loss: 0.468433] [G loss: 2.051573, adv: 1.334707, cycle: 0.045958, identity: 0.051457] ETA: 1:20:23.170258\n",
      "[Epoch 37/200] [Batch 300/476] [D loss: 0.565471] [G loss: 2.435584, adv: 1.566130, cycle: 0.056341, identity: 0.061208] ETA: 1:16:52.380842\n",
      "[Epoch 37/200] [Batch 400/476] [D loss: 0.435697] [G loss: 1.780614, adv: 1.074807, cycle: 0.043017, identity: 0.055128] ETA: 1:23:36.359473\n",
      "[Epoch 38/200] [Batch 0/476] [D loss: 0.518509] [G loss: 2.782522, adv: 1.642116, cycle: 0.080334, identity: 0.067414] ETA: 4:06:07.973585\n",
      "[Epoch 38/200] [Batch 100/476] [D loss: 0.471028] [G loss: 1.983853, adv: 1.202811, cycle: 0.055378, identity: 0.045453] ETA: 1:18:16.198041\n",
      "[Epoch 38/200] [Batch 200/476] [D loss: 0.414164] [G loss: 2.643789, adv: 1.411753, cycle: 0.084688, identity: 0.077032] ETA: 1:26:29.936787\n",
      "[Epoch 38/200] [Batch 300/476] [D loss: 0.480164] [G loss: 2.957239, adv: 1.580116, cycle: 0.101603, identity: 0.072218] ETA: 1:24:28.528650\n",
      "[Epoch 38/200] [Batch 400/476] [D loss: 0.438022] [G loss: 2.418401, adv: 1.402461, cycle: 0.072881, identity: 0.057425] ETA: 1:20:32.743483\n",
      "[Epoch 39/200] [Batch 0/476] [D loss: 0.439761] [G loss: 2.526968, adv: 1.638079, cycle: 0.066195, identity: 0.045388] ETA: 4:00:31.153660\n",
      "[Epoch 39/200] [Batch 100/476] [D loss: 0.519904] [G loss: 2.782522, adv: 1.642116, cycle: 0.080334, identity: 0.067414] ETA: 1:10:06.182293\n",
      "[Epoch 39/200] [Batch 200/476] [D loss: 0.369257] [G loss: 2.091136, adv: 1.092890, cycle: 0.071293, identity: 0.057063] ETA: 1:17:42.567863\n",
      "[Epoch 39/200] [Batch 300/476] [D loss: 0.392932] [G loss: 2.016722, adv: 1.153472, cycle: 0.057410, identity: 0.057831] ETA: 1:18:16.471325\n",
      "[Epoch 39/200] [Batch 400/476] [D loss: 0.543749] [G loss: 1.847723, adv: 1.183617, cycle: 0.043438, identity: 0.045946] ETA: 1:21:17.478041\n",
      "[Epoch 40/200] [Batch 0/476] [D loss: 0.428932] [G loss: 2.290693, adv: 1.409449, cycle: 0.055644, identity: 0.064960] ETA: 3:52:39.584656\n",
      "[Epoch 40/200] [Batch 100/476] [D loss: 0.451078] [G loss: 2.528434, adv: 1.495501, cycle: 0.074188, identity: 0.058211] ETA: 1:22:08.236475\n",
      "[Epoch 40/200] [Batch 200/476] [D loss: 0.453847] [G loss: 2.251988, adv: 1.419134, cycle: 0.059353, identity: 0.047865] ETA: 1:18:07.428217\n",
      "[Epoch 40/200] [Batch 300/476] [D loss: 0.464404] [G loss: 1.953194, adv: 1.062913, cycle: 0.059989, identity: 0.058079] ETA: 1:23:36.489344\n",
      "[Epoch 40/200] [Batch 400/476] [D loss: 0.540774] [G loss: 2.198469, adv: 1.262367, cycle: 0.065502, identity: 0.056217] ETA: 1:22:53.642960\n",
      "[Epoch 41/200] [Batch 0/476] [D loss: 0.530745] [G loss: 2.139818, adv: 1.304358, cycle: 0.064331, identity: 0.038431] ETA: 4:03:30.410279\n",
      "[Epoch 41/200] [Batch 100/476] [D loss: 0.458949] [G loss: 2.291028, adv: 1.281720, cycle: 0.068780, identity: 0.064302] ETA: 1:29:08.558990\n",
      "[Epoch 41/200] [Batch 200/476] [D loss: 0.430493] [G loss: 2.041738, adv: 1.327783, cycle: 0.048264, identity: 0.046263] ETA: 1:23:18.409909\n",
      "[Epoch 41/200] [Batch 300/476] [D loss: 0.387945] [G loss: 2.723047, adv: 1.576984, cycle: 0.078133, identity: 0.072947] ETA: 1:19:19.865198\n",
      "[Epoch 41/200] [Batch 400/476] [D loss: 0.431176] [G loss: 2.290693, adv: 1.409449, cycle: 0.055644, identity: 0.064960] ETA: 1:26:57.355895\n",
      "[Epoch 42/200] [Batch 0/476] [D loss: 0.421181] [G loss: 1.983845, adv: 1.199709, cycle: 0.050792, identity: 0.055243] ETA: 3:57:35.652674\n",
      "[Epoch 42/200] [Batch 100/476] [D loss: 0.508893] [G loss: 2.218308, adv: 1.247061, cycle: 0.065196, identity: 0.063857] ETA: 1:16:56.962772\n",
      "[Epoch 42/200] [Batch 200/476] [D loss: 0.507160] [G loss: 2.514939, adv: 1.509026, cycle: 0.074931, identity: 0.051321] ETA: 1:13:50.927551\n",
      "[Epoch 42/200] [Batch 300/476] [D loss: 0.492143] [G loss: 2.128031, adv: 1.213057, cycle: 0.062121, identity: 0.058753] ETA: 1:16:04.681247\n",
      "[Epoch 42/200] [Batch 400/476] [D loss: 0.428823] [G loss: 2.418133, adv: 1.573149, cycle: 0.067940, identity: 0.033116] ETA: 1:18:28.388878\n",
      "[Epoch 43/200] [Batch 0/476] [D loss: 0.457445] [G loss: 2.467417, adv: 1.583200, cycle: 0.054889, identity: 0.067066] ETA: 3:47:33.441240\n",
      "[Epoch 43/200] [Batch 100/476] [D loss: 0.559897] [G loss: 2.975591, adv: 1.760759, cycle: 0.087086, identity: 0.068794] ETA: 1:13:24.694838\n",
      "[Epoch 43/200] [Batch 200/476] [D loss: 0.456896] [G loss: 2.526959, adv: 1.597671, cycle: 0.066869, identity: 0.052119] ETA: 1:16:11.302302\n",
      "[Epoch 43/200] [Batch 300/476] [D loss: 0.501860] [G loss: 2.496989, adv: 1.594411, cycle: 0.058681, identity: 0.063153] ETA: 1:13:34.257217\n",
      "[Epoch 43/200] [Batch 400/476] [D loss: 0.460857] [G loss: 2.413334, adv: 1.534391, cycle: 0.060389, identity: 0.055011] ETA: 1:15:02.821023\n",
      "[Epoch 44/200] [Batch 0/476] [D loss: 0.403515] [G loss: 2.590157, adv: 1.551183, cycle: 0.075913, identity: 0.055969] ETA: 3:57:55.823215\n",
      "[Epoch 44/200] [Batch 100/476] [D loss: 0.456655] [G loss: 2.467417, adv: 1.583200, cycle: 0.054889, identity: 0.067066] ETA: 1:11:40.064417\n",
      "[Epoch 44/200] [Batch 200/476] [D loss: 0.387134] [G loss: 2.338269, adv: 1.509497, cycle: 0.055474, identity: 0.054807] ETA: 1:15:33.738497\n",
      "[Epoch 44/200] [Batch 300/476] [D loss: 0.505868] [G loss: 1.960250, adv: 1.180164, cycle: 0.052053, identity: 0.051911] ETA: 1:16:36.453675\n",
      "[Epoch 44/200] [Batch 400/476] [D loss: 0.435517] [G loss: 2.341740, adv: 1.485346, cycle: 0.065915, identity: 0.039449] ETA: 1:16:50.048279\n",
      "[Epoch 45/200] [Batch 0/476] [D loss: 0.454109] [G loss: 2.018765, adv: 1.168164, cycle: 0.050804, identity: 0.068513] ETA: 3:57:03.731890\n",
      "[Epoch 45/200] [Batch 100/476] [D loss: 0.449101] [G loss: 1.791506, adv: 1.139291, cycle: 0.047670, identity: 0.035103] ETA: 1:16:53.115807\n",
      "[Epoch 45/200] [Batch 200/476] [D loss: 0.422645] [G loss: 1.920685, adv: 0.978263, cycle: 0.060755, identity: 0.066974] ETA: 1:11:19.663301\n",
      "[Epoch 45/200] [Batch 300/476] [D loss: 0.424077] [G loss: 2.364021, adv: 1.398483, cycle: 0.067953, identity: 0.057202] ETA: 1:17:11.637440\n",
      "[Epoch 45/200] [Batch 400/476] [D loss: 0.493440] [G loss: 2.038283, adv: 1.288794, cycle: 0.050174, identity: 0.049550] ETA: 1:19:01.432028\n",
      "[Epoch 46/200] [Batch 0/476] [D loss: 0.484214] [G loss: 2.313894, adv: 1.329237, cycle: 0.061916, identity: 0.073099] ETA: 3:44:49.912024\n",
      "[Epoch 46/200] [Batch 100/476] [D loss: 0.518799] [G loss: 2.328814, adv: 1.503891, cycle: 0.060620, identity: 0.043744] ETA: 1:14:30.705370\n",
      "[Epoch 46/200] [Batch 200/476] [D loss: 0.434717] [G loss: 2.223009, adv: 1.277307, cycle: 0.067214, identity: 0.054712] ETA: 1:22:44.193123\n",
      "[Epoch 46/200] [Batch 300/476] [D loss: 0.330938] [G loss: 2.361276, adv: 1.481163, cycle: 0.059838, identity: 0.056347] ETA: 1:18:20.183907\n",
      "[Epoch 46/200] [Batch 400/476] [D loss: 0.353504] [G loss: 2.406246, adv: 1.321210, cycle: 0.080019, identity: 0.056970] ETA: 1:19:34.518263\n",
      "[Epoch 47/200] [Batch 0/476] [D loss: 0.436125] [G loss: 2.125718, adv: 1.191031, cycle: 0.067435, identity: 0.052067] ETA: 3:53:49.729646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/200] [Batch 100/476] [D loss: 0.472666] [G loss: 2.353031, adv: 1.625188, cycle: 0.050178, identity: 0.045212] ETA: 1:19:17.391212\n",
      "[Epoch 47/200] [Batch 200/476] [D loss: 0.434577] [G loss: 2.364021, adv: 1.398483, cycle: 0.067953, identity: 0.057202] ETA: 1:25:23.002426\n",
      "[Epoch 47/200] [Batch 300/476] [D loss: 0.472487] [G loss: 2.352597, adv: 1.506616, cycle: 0.060196, identity: 0.048804] ETA: 1:19:08.562355\n",
      "[Epoch 47/200] [Batch 400/476] [D loss: 0.488543] [G loss: 2.368036, adv: 1.407190, cycle: 0.067905, identity: 0.056360] ETA: 1:15:41.859658\n",
      "[Epoch 48/200] [Batch 0/476] [D loss: 0.450627] [G loss: 2.694856, adv: 1.512193, cycle: 0.085152, identity: 0.066229] ETA: 3:50:49.832504\n",
      "[Epoch 48/200] [Batch 100/476] [D loss: 0.442542] [G loss: 1.953194, adv: 1.062913, cycle: 0.059989, identity: 0.058079] ETA: 1:18:29.476061\n",
      "[Epoch 48/200] [Batch 200/476] [D loss: 0.444010] [G loss: 1.780614, adv: 1.074807, cycle: 0.043017, identity: 0.055128] ETA: 1:20:53.943201\n",
      "[Epoch 48/200] [Batch 300/476] [D loss: 0.510877] [G loss: 2.486782, adv: 1.604007, cycle: 0.065002, identity: 0.046551] ETA: 1:14:30.576426\n",
      "[Epoch 48/200] [Batch 400/476] [D loss: 0.538170] [G loss: 2.446982, adv: 1.528831, cycle: 0.070414, identity: 0.042802] ETA: 1:15:40.967480\n",
      "[Epoch 49/200] [Batch 0/476] [D loss: 0.373091] [G loss: 2.270460, adv: 1.275941, cycle: 0.066550, identity: 0.065803] ETA: 3:47:31.680145\n",
      "[Epoch 49/200] [Batch 100/476] [D loss: 0.476163] [G loss: 2.244753, adv: 1.244814, cycle: 0.071010, identity: 0.057967] ETA: 1:20:01.558777\n",
      "[Epoch 49/200] [Batch 200/476] [D loss: 0.418003] [G loss: 2.054097, adv: 1.206301, cycle: 0.058257, identity: 0.053045] ETA: 1:11:32.336143\n",
      "[Epoch 49/200] [Batch 300/476] [D loss: 0.488283] [G loss: 2.365867, adv: 1.439713, cycle: 0.060229, identity: 0.064773] ETA: 1:11:52.883766\n",
      "[Epoch 49/200] [Batch 400/476] [D loss: 0.437610] [G loss: 1.888819, adv: 1.187153, cycle: 0.048301, identity: 0.043732] ETA: 1:20:34.590244\n",
      "[Epoch 50/200] [Batch 0/476] [D loss: 0.518677] [G loss: 2.264605, adv: 1.461463, cycle: 0.058146, identity: 0.044337] ETA: 3:44:38.658628\n",
      "[Epoch 50/200] [Batch 100/476] [D loss: 0.466984] [G loss: 2.051573, adv: 1.334707, cycle: 0.045958, identity: 0.051457] ETA: 1:18:48.492904\n",
      "[Epoch 50/200] [Batch 200/476] [D loss: 0.424141] [G loss: 2.127306, adv: 1.299839, cycle: 0.062751, identity: 0.039992] ETA: 1:13:27.867050\n",
      "[Epoch 50/200] [Batch 300/476] [D loss: 0.481036] [G loss: 2.244753, adv: 1.244814, cycle: 0.071010, identity: 0.057967] ETA: 1:20:55.961108\n",
      "[Epoch 50/200] [Batch 400/476] [D loss: 0.475836] [G loss: 2.310069, adv: 1.484732, cycle: 0.057868, identity: 0.049332] ETA: 1:06:36.922255\n",
      "[Epoch 51/200] [Batch 0/476] [D loss: 0.353504] [G loss: 2.406246, adv: 1.321210, cycle: 0.080019, identity: 0.056970] ETA: 4:55:37.408738\n",
      "[Epoch 51/200] [Batch 100/476] [D loss: 0.542271] [G loss: 2.159594, adv: 1.441028, cycle: 0.047961, identity: 0.047791] ETA: 1:09:20.498663\n",
      "[Epoch 51/200] [Batch 200/476] [D loss: 0.465798] [G loss: 2.291028, adv: 1.281720, cycle: 0.068780, identity: 0.064302] ETA: 1:13:55.139071\n",
      "[Epoch 51/200] [Batch 300/476] [D loss: 0.565840] [G loss: 2.330975, adv: 1.307719, cycle: 0.070896, identity: 0.062860] ETA: 1:15:44.293015\n",
      "[Epoch 51/200] [Batch 400/476] [D loss: 0.466799] [G loss: 2.246890, adv: 1.365961, cycle: 0.061361, identity: 0.053465] ETA: 1:08:28.002688\n",
      "[Epoch 52/200] [Batch 0/476] [D loss: 0.470467] [G loss: 2.039775, adv: 1.414795, cycle: 0.046327, identity: 0.032342] ETA: 3:33:14.690304\n",
      "[Epoch 52/200] [Batch 100/476] [D loss: 0.418090] [G loss: 1.996193, adv: 1.352956, cycle: 0.043430, identity: 0.041787] ETA: 1:13:07.022083\n",
      "[Epoch 52/200] [Batch 200/476] [D loss: 0.576246] [G loss: 2.775151, adv: 1.590730, cycle: 0.083823, identity: 0.069239] ETA: 1:12:12.751419\n",
      "[Epoch 52/200] [Batch 300/476] [D loss: 0.556836] [G loss: 2.257644, adv: 1.337681, cycle: 0.063516, identity: 0.056961] ETA: 1:14:36.268675\n",
      "[Epoch 52/200] [Batch 400/476] [D loss: 0.459653] [G loss: 2.433824, adv: 1.546793, cycle: 0.061401, identity: 0.054604] ETA: 1:08:31.923737\n",
      "[Epoch 53/200] [Batch 0/476] [D loss: 0.486333] [G loss: 2.099793, adv: 1.243287, cycle: 0.060637, identity: 0.050027] ETA: 3:44:49.687205\n",
      "[Epoch 53/200] [Batch 100/476] [D loss: 0.523915] [G loss: 1.990427, adv: 1.387174, cycle: 0.041790, identity: 0.037071] ETA: 1:16:20.099152\n",
      "[Epoch 53/200] [Batch 200/476] [D loss: 0.537667] [G loss: 2.652725, adv: 1.582434, cycle: 0.082199, identity: 0.049661] ETA: 1:18:47.849880\n",
      "[Epoch 53/200] [Batch 300/476] [D loss: 0.503847] [G loss: 2.148015, adv: 1.229686, cycle: 0.062827, identity: 0.058012] ETA: 1:11:46.726479\n",
      "[Epoch 53/200] [Batch 400/476] [D loss: 0.533938] [G loss: 2.256607, adv: 1.630401, cycle: 0.042342, identity: 0.040557] ETA: 1:17:56.279602\n",
      "[Epoch 54/200] [Batch 0/476] [D loss: 0.497919] [G loss: 2.185464, adv: 1.288807, cycle: 0.063398, identity: 0.052535] ETA: 3:27:25.443752\n",
      "[Epoch 54/200] [Batch 100/476] [D loss: 0.520788] [G loss: 2.089520, adv: 1.316489, cycle: 0.054106, identity: 0.046394] ETA: 1:17:33.513328\n",
      "[Epoch 54/200] [Batch 200/476] [D loss: 0.540312] [G loss: 2.145318, adv: 1.276611, cycle: 0.055441, identity: 0.062859] ETA: 1:17:51.275864\n",
      "[Epoch 54/200] [Batch 300/476] [D loss: 0.480338] [G loss: 2.151468, adv: 1.291510, cycle: 0.057531, identity: 0.056929] ETA: 1:05:00.777869\n",
      "[Epoch 54/200] [Batch 400/476] [D loss: 0.392623] [G loss: 2.234166, adv: 1.379622, cycle: 0.058999, identity: 0.052911] ETA: 1:08:00.289284\n",
      "[Epoch 55/200] [Batch 0/476] [D loss: 0.447893] [G loss: 2.282609, adv: 1.521542, cycle: 0.051177, identity: 0.049859] ETA: 3:22:35.690165\n",
      "[Epoch 55/200] [Batch 100/476] [D loss: 0.460928] [G loss: 2.114483, adv: 1.134428, cycle: 0.067007, identity: 0.061997] ETA: 1:16:09.488754\n",
      "[Epoch 55/200] [Batch 200/476] [D loss: 0.401720] [G loss: 2.328885, adv: 1.326483, cycle: 0.071826, identity: 0.056829] ETA: 1:09:57.961850\n",
      "[Epoch 55/200] [Batch 300/476] [D loss: 0.535448] [G loss: 2.184822, adv: 1.346958, cycle: 0.055865, identity: 0.055843] ETA: 1:10:52.974720\n",
      "[Epoch 55/200] [Batch 400/476] [D loss: 0.576065] [G loss: 2.384294, adv: 1.342232, cycle: 0.068529, identity: 0.071355] ETA: 1:14:53.384418\n",
      "[Epoch 56/200] [Batch 0/476] [D loss: 0.430259] [G loss: 2.142757, adv: 1.293353, cycle: 0.057772, identity: 0.054336] ETA: 3:25:09.603607\n",
      "[Epoch 56/200] [Batch 100/476] [D loss: 0.425572] [G loss: 2.364021, adv: 1.398483, cycle: 0.067953, identity: 0.057202] ETA: 1:05:53.276501\n",
      "[Epoch 56/200] [Batch 200/476] [D loss: 0.499233] [G loss: 1.983853, adv: 1.202811, cycle: 0.055378, identity: 0.045453] ETA: 1:16:21.730593\n",
      "[Epoch 56/200] [Batch 300/476] [D loss: 0.531584] [G loss: 2.913275, adv: 1.847785, cycle: 0.068814, identity: 0.075471] ETA: 1:13:09.118363\n",
      "[Epoch 56/200] [Batch 400/476] [D loss: 0.467157] [G loss: 2.526968, adv: 1.638079, cycle: 0.066195, identity: 0.045388] ETA: 1:09:58.756882\n",
      "[Epoch 57/200] [Batch 0/476] [D loss: 0.440593] [G loss: 1.901979, adv: 1.217584, cycle: 0.049368, identity: 0.038143] ETA: 3:19:23.244285\n",
      "[Epoch 57/200] [Batch 100/476] [D loss: 0.478357] [G loss: 2.369983, adv: 1.359801, cycle: 0.074200, identity: 0.053637] ETA: 1:09:44.088135\n",
      "[Epoch 57/200] [Batch 200/476] [D loss: 0.421748] [G loss: 2.418401, adv: 1.402461, cycle: 0.072881, identity: 0.057425] ETA: 1:11:10.260908\n",
      "[Epoch 57/200] [Batch 300/476] [D loss: 0.467144] [G loss: 1.987475, adv: 1.143401, cycle: 0.059231, identity: 0.050352] ETA: 1:08:41.414362\n",
      "[Epoch 57/200] [Batch 400/476] [D loss: 0.433238] [G loss: 2.230638, adv: 1.338627, cycle: 0.061377, identity: 0.055649] ETA: 1:02:08.052641\n",
      "[Epoch 58/200] [Batch 0/476] [D loss: 0.459040] [G loss: 2.021915, adv: 1.325482, cycle: 0.046784, identity: 0.045719] ETA: 3:14:55.243591\n",
      "[Epoch 58/200] [Batch 100/476] [D loss: 0.476163] [G loss: 2.244753, adv: 1.244814, cycle: 0.071010, identity: 0.057967] ETA: 1:11:20.089046\n",
      "[Epoch 58/200] [Batch 200/476] [D loss: 0.374561] [G loss: 2.150054, adv: 1.400335, cycle: 0.056348, identity: 0.037247] ETA: 1:13:55.627533\n",
      "[Epoch 58/200] [Batch 300/476] [D loss: 0.622048] [G loss: 2.499663, adv: 1.330463, cycle: 0.080958, identity: 0.071924] ETA: 1:12:56.743361\n",
      "[Epoch 58/200] [Batch 400/476] [D loss: 0.403145] [G loss: 2.248066, adv: 1.227369, cycle: 0.068980, identity: 0.066180] ETA: 1:11:28.073591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 59/200] [Batch 0/476] [D loss: 0.406364] [G loss: 1.996421, adv: 1.268855, cycle: 0.053459, identity: 0.038596] ETA: 3:24:53.243059\n",
      "[Epoch 59/200] [Batch 100/476] [D loss: 0.508097] [G loss: 2.002154, adv: 1.168095, cycle: 0.057724, identity: 0.051363] ETA: 1:07:16.582512\n",
      "[Epoch 59/200] [Batch 200/476] [D loss: 0.432177] [G loss: 2.142757, adv: 1.293353, cycle: 0.057772, identity: 0.054336] ETA: 1:04:06.960365\n",
      "[Epoch 59/200] [Batch 300/476] [D loss: 0.483189] [G loss: 2.018765, adv: 1.168164, cycle: 0.050804, identity: 0.068513] ETA: 1:11:21.218811\n",
      "[Epoch 59/200] [Batch 400/476] [D loss: 0.575558] [G loss: 2.775151, adv: 1.590730, cycle: 0.083823, identity: 0.069239] ETA: 1:06:23.725425\n",
      "[Epoch 60/200] [Batch 0/476] [D loss: 0.422798] [G loss: 2.230638, adv: 1.338627, cycle: 0.061377, identity: 0.055649] ETA: 3:26:45.025043\n",
      "[Epoch 60/200] [Batch 100/476] [D loss: 0.496897] [G loss: 1.797628, adv: 1.095371, cycle: 0.043381, identity: 0.053689] ETA: 1:07:26.620617\n",
      "[Epoch 60/200] [Batch 200/476] [D loss: 0.526868] [G loss: 2.396019, adv: 1.630453, cycle: 0.051886, identity: 0.049342] ETA: 1:14:22.926874\n",
      "[Epoch 60/200] [Batch 300/476] [D loss: 0.389888] [G loss: 2.723047, adv: 1.576984, cycle: 0.078133, identity: 0.072947] ETA: 1:10:51.367712\n",
      "[Epoch 60/200] [Batch 400/476] [D loss: 0.507058] [G loss: 2.509790, adv: 1.390874, cycle: 0.078983, identity: 0.065817] ETA: 1:06:22.513733\n",
      "[Epoch 61/200] [Batch 0/476] [D loss: 0.409156] [G loss: 2.234166, adv: 1.379622, cycle: 0.058999, identity: 0.052911] ETA: 3:16:51.058572\n",
      "[Epoch 61/200] [Batch 100/476] [D loss: 0.480006] [G loss: 2.526959, adv: 1.597671, cycle: 0.066869, identity: 0.052119] ETA: 1:11:21.437065\n",
      "[Epoch 61/200] [Batch 200/476] [D loss: 0.492878] [G loss: 1.962372, adv: 1.344926, cycle: 0.041459, identity: 0.040570] ETA: 1:06:10.889671\n",
      "[Epoch 61/200] [Batch 300/476] [D loss: 0.525493] [G loss: 2.646130, adv: 1.650948, cycle: 0.069940, identity: 0.059156] ETA: 1:04:44.516607\n",
      "[Epoch 61/200] [Batch 400/476] [D loss: 0.576793] [G loss: 2.780258, adv: 1.841271, cycle: 0.063669, identity: 0.060459] ETA: 1:07:32.220691\n",
      "[Epoch 62/200] [Batch 0/476] [D loss: 0.596333] [G loss: 2.499663, adv: 1.330463, cycle: 0.080958, identity: 0.071924] ETA: 3:19:13.378498\n",
      "[Epoch 62/200] [Batch 100/476] [D loss: 0.422600] [G loss: 2.230638, adv: 1.338627, cycle: 0.061377, identity: 0.055649] ETA: 1:02:28.112233\n",
      "[Epoch 62/200] [Batch 200/476] [D loss: 0.495969] [G loss: 2.059718, adv: 1.279248, cycle: 0.050456, identity: 0.055181] ETA: 1:11:13.680069\n",
      "[Epoch 62/200] [Batch 300/476] [D loss: 0.470331] [G loss: 1.900802, adv: 1.048982, cycle: 0.057959, identity: 0.054447] ETA: 1:06:50.687785\n",
      "[Epoch 62/200] [Batch 400/476] [D loss: 0.520636] [G loss: 2.458335, adv: 1.557137, cycle: 0.059434, identity: 0.061372] ETA: 1:09:56.792631\n",
      "[Epoch 63/200] [Batch 0/476] [D loss: 0.424141] [G loss: 2.127306, adv: 1.299839, cycle: 0.062751, identity: 0.039992] ETA: 3:22:43.348738\n",
      "[Epoch 63/200] [Batch 100/476] [D loss: 0.423882] [G loss: 2.416150, adv: 1.485373, cycle: 0.064622, identity: 0.056912] ETA: 1:14:04.992348\n",
      "[Epoch 63/200] [Batch 200/476] [D loss: 0.565985] [G loss: 2.167902, adv: 1.514742, cycle: 0.048090, identity: 0.034452] ETA: 1:11:03.185386\n",
      "[Epoch 63/200] [Batch 300/476] [D loss: 0.436989] [G loss: 2.223009, adv: 1.277307, cycle: 0.067214, identity: 0.054712] ETA: 1:11:09.983833\n",
      "[Epoch 63/200] [Batch 400/476] [D loss: 0.455941] [G loss: 2.008260, adv: 1.201681, cycle: 0.057555, identity: 0.046205] ETA: 1:09:31.494936\n",
      "[Epoch 64/200] [Batch 0/476] [D loss: 0.450397] [G loss: 2.364021, adv: 1.398483, cycle: 0.067953, identity: 0.057202] ETA: 3:24:04.434692\n",
      "[Epoch 64/200] [Batch 100/476] [D loss: 0.507587] [G loss: 2.081748, adv: 1.325472, cycle: 0.051265, identity: 0.048724] ETA: 1:07:41.124257\n",
      "[Epoch 64/200] [Batch 200/476] [D loss: 0.456840] [G loss: 1.953194, adv: 1.062913, cycle: 0.059989, identity: 0.058079] ETA: 1:06:59.159706\n",
      "[Epoch 64/200] [Batch 300/476] [D loss: 0.424231] [G loss: 2.239276, adv: 1.256577, cycle: 0.068099, identity: 0.060343] ETA: 1:06:21.760928\n",
      "[Epoch 64/200] [Batch 400/476] [D loss: 0.487946] [G loss: 1.983853, adv: 1.202811, cycle: 0.055378, identity: 0.045453] ETA: 1:10:37.907352\n",
      "[Epoch 65/200] [Batch 0/476] [D loss: 0.408458] [G loss: 1.786817, adv: 0.910424, cycle: 0.059092, identity: 0.057094] ETA: 3:16:52.273793\n",
      "[Epoch 65/200] [Batch 100/476] [D loss: 0.449101] [G loss: 1.791506, adv: 1.139291, cycle: 0.047670, identity: 0.035103] ETA: 1:10:19.644508\n",
      "[Epoch 65/200] [Batch 200/476] [D loss: 0.359249] [G loss: 1.942484, adv: 0.926391, cycle: 0.066111, identity: 0.070997] ETA: 1:01:34.561481\n",
      "[Epoch 65/200] [Batch 300/476] [D loss: 0.539578] [G loss: 2.744388, adv: 1.682283, cycle: 0.077444, identity: 0.057534] ETA: 1:06:10.310583\n",
      "[Epoch 65/200] [Batch 400/476] [D loss: 0.459039] [G loss: 2.176545, adv: 1.503977, cycle: 0.046893, identity: 0.040727] ETA: 1:02:05.125051\n",
      "[Epoch 66/200] [Batch 0/476] [D loss: 0.468369] [G loss: 2.004832, adv: 1.127823, cycle: 0.064513, identity: 0.046377] ETA: 3:17:16.244909\n",
      "[Epoch 66/200] [Batch 100/476] [D loss: 0.508339] [G loss: 2.446982, adv: 1.528831, cycle: 0.070414, identity: 0.042802] ETA: 1:07:36.850499\n",
      "[Epoch 66/200] [Batch 200/476] [D loss: 0.513706] [G loss: 2.652725, adv: 1.582434, cycle: 0.082199, identity: 0.049661] ETA: 1:05:42.452858\n",
      "[Epoch 66/200] [Batch 300/476] [D loss: 0.611675] [G loss: 2.948584, adv: 1.964054, cycle: 0.074383, identity: 0.048140] ETA: 1:02:33.503245\n",
      "[Epoch 66/200] [Batch 400/476] [D loss: 0.475178] [G loss: 2.291028, adv: 1.281720, cycle: 0.068780, identity: 0.064302] ETA: 1:05:38.953011\n",
      "[Epoch 67/200] [Batch 0/476] [D loss: 0.494701] [G loss: 2.215408, adv: 1.332260, cycle: 0.055356, identity: 0.065918] ETA: 3:24:01.482095\n",
      "[Epoch 67/200] [Batch 100/476] [D loss: 0.443575] [G loss: 1.849933, adv: 1.171103, cycle: 0.047260, identity: 0.041245] ETA: 1:04:46.482803\n",
      "[Epoch 67/200] [Batch 200/476] [D loss: 0.470352] [G loss: 2.352597, adv: 1.506616, cycle: 0.060196, identity: 0.048804] ETA: 1:03:10.373334\n",
      "[Epoch 67/200] [Batch 300/476] [D loss: 0.496827] [G loss: 2.353031, adv: 1.625188, cycle: 0.050178, identity: 0.045212] ETA: 1:07:16.726410\n",
      "[Epoch 67/200] [Batch 400/476] [D loss: 0.479741] [G loss: 2.269437, adv: 1.306096, cycle: 0.061106, identity: 0.070456] ETA: 1:07:00.795714\n",
      "[Epoch 68/200] [Batch 0/476] [D loss: 0.448784] [G loss: 2.094810, adv: 1.033891, cycle: 0.074122, identity: 0.063939] ETA: 3:16:40.923820\n",
      "[Epoch 68/200] [Batch 100/476] [D loss: 0.520879] [G loss: 2.509790, adv: 1.390874, cycle: 0.078983, identity: 0.065817] ETA: 1:06:48.634243\n",
      "[Epoch 68/200] [Batch 200/476] [D loss: 0.410289] [G loss: 2.356679, adv: 1.517141, cycle: 0.051601, identity: 0.064705] ETA: 1:01:56.403698\n",
      "[Epoch 68/200] [Batch 300/476] [D loss: 0.532313] [G loss: 2.409564, adv: 1.369384, cycle: 0.074442, identity: 0.059153] ETA: 1:03:48.189795\n",
      "[Epoch 68/200] [Batch 400/476] [D loss: 0.443509] [G loss: 2.320311, adv: 1.534579, cycle: 0.055520, identity: 0.046106] ETA: 1:00:29.501244\n",
      "[Epoch 69/200] [Batch 0/476] [D loss: 0.449156] [G loss: 2.155714, adv: 1.198486, cycle: 0.063564, identity: 0.064317] ETA: 3:07:43.570460\n",
      "[Epoch 69/200] [Batch 100/476] [D loss: 0.488329] [G loss: 1.944502, adv: 1.281647, cycle: 0.042490, identity: 0.047590] ETA: 1:01:38.382557\n",
      "[Epoch 69/200] [Batch 200/476] [D loss: 0.437158] [G loss: 2.051573, adv: 1.334707, cycle: 0.045958, identity: 0.051457] ETA: 1:03:24.563605\n",
      "[Epoch 69/200] [Batch 300/476] [D loss: 0.484142] [G loss: 2.461199, adv: 1.425035, cycle: 0.074959, identity: 0.057315] ETA: 1:01:16.751244\n",
      "[Epoch 69/200] [Batch 400/476] [D loss: 0.459998] [G loss: 1.953194, adv: 1.062913, cycle: 0.059989, identity: 0.058079] ETA: 1:02:27.416379\n",
      "[Epoch 70/200] [Batch 0/476] [D loss: 0.473193] [G loss: 2.508993, adv: 1.451480, cycle: 0.070351, identity: 0.070800] ETA: 3:22:15.892315\n",
      "[Epoch 70/200] [Batch 100/476] [D loss: 0.559751] [G loss: 3.065910, adv: 1.975285, cycle: 0.081740, identity: 0.054644] ETA: 1:03:32.451167\n",
      "[Epoch 70/200] [Batch 200/476] [D loss: 0.374653] [G loss: 2.257720, adv: 1.217046, cycle: 0.071071, identity: 0.065993] ETA: 1:09:12.951336\n",
      "[Epoch 70/200] [Batch 300/476] [D loss: 0.581376] [G loss: 1.860778, adv: 1.252312, cycle: 0.042992, identity: 0.035709] ETA: 1:03:54.449911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 70/200] [Batch 400/476] [D loss: 0.444049] [G loss: 2.528434, adv: 1.495501, cycle: 0.074188, identity: 0.058211] ETA: 1:06:43.107424\n",
      "[Epoch 71/200] [Batch 0/476] [D loss: 0.536739] [G loss: 2.456072, adv: 1.622720, cycle: 0.059636, identity: 0.047398] ETA: 3:14:06.326277\n",
      "[Epoch 71/200] [Batch 100/476] [D loss: 0.442771] [G loss: 2.310849, adv: 1.476599, cycle: 0.055479, identity: 0.055891] ETA: 1:06:41.571922\n",
      "[Epoch 71/200] [Batch 200/476] [D loss: 0.540985] [G loss: 2.198469, adv: 1.262367, cycle: 0.065502, identity: 0.056217] ETA: 1:05:46.540123\n",
      "[Epoch 71/200] [Batch 300/476] [D loss: 0.499525] [G loss: 2.775151, adv: 1.590730, cycle: 0.083823, identity: 0.069239] ETA: 1:01:33.566689\n",
      "[Epoch 71/200] [Batch 400/476] [D loss: 0.396071] [G loss: 2.152011, adv: 1.320026, cycle: 0.062931, identity: 0.040536] ETA: 1:02:27.008919\n",
      "[Epoch 72/200] [Batch 0/476] [D loss: 0.559914] [G loss: 3.367979, adv: 2.084400, cycle: 0.093810, identity: 0.069096] ETA: 3:26:02.519531\n",
      "[Epoch 72/200] [Batch 100/476] [D loss: 0.429244] [G loss: 1.850726, adv: 1.106014, cycle: 0.050546, identity: 0.047851] ETA: 1:05:09.996849\n",
      "[Epoch 72/200] [Batch 200/476] [D loss: 0.399363] [G loss: 2.147632, adv: 1.241480, cycle: 0.065632, identity: 0.049967] ETA: 1:04:56.401945\n",
      "[Epoch 72/200] [Batch 300/476] [D loss: 0.506087] [G loss: 2.158411, adv: 1.321825, cycle: 0.057549, identity: 0.052218] ETA: 1:08:35.380136\n",
      "[Epoch 72/200] [Batch 400/476] [D loss: 0.480970] [G loss: 1.944502, adv: 1.281647, cycle: 0.042490, identity: 0.047590] ETA: 1:01:43.109985\n",
      "[Epoch 73/200] [Batch 0/476] [D loss: 0.535366] [G loss: 2.311523, adv: 1.547882, cycle: 0.048474, identity: 0.055780] ETA: 3:14:25.294989\n",
      "[Epoch 73/200] [Batch 100/476] [D loss: 0.477721] [G loss: 2.617160, adv: 1.645809, cycle: 0.063164, identity: 0.067942] ETA: 1:05:02.666855\n",
      "[Epoch 73/200] [Batch 200/476] [D loss: 0.494627] [G loss: 2.328814, adv: 1.503891, cycle: 0.060620, identity: 0.043744] ETA: 1:02:14.132203\n",
      "[Epoch 73/200] [Batch 300/476] [D loss: 0.511428] [G loss: 2.647243, adv: 1.702749, cycle: 0.068927, identity: 0.051044] ETA: 1:08:47.083254\n",
      "[Epoch 73/200] [Batch 400/476] [D loss: 0.439361] [G loss: 2.155714, adv: 1.198486, cycle: 0.063564, identity: 0.064317] ETA: 1:02:32.734570\n",
      "[Epoch 74/200] [Batch 0/476] [D loss: 0.535174] [G loss: 2.628171, adv: 1.782511, cycle: 0.059421, identity: 0.050291] ETA: 3:06:56.515131\n",
      "[Epoch 74/200] [Batch 100/476] [D loss: 0.547882] [G loss: 2.435584, adv: 1.566130, cycle: 0.056341, identity: 0.061208] ETA: 1:03:42.364391\n",
      "[Epoch 74/200] [Batch 200/476] [D loss: 0.488464] [G loss: 2.063762, adv: 1.478104, cycle: 0.037275, identity: 0.042581] ETA: 1:01:23.254425\n",
      "[Epoch 74/200] [Batch 300/476] [D loss: 0.423728] [G loss: 2.032938, adv: 1.302269, cycle: 0.053434, identity: 0.039267] ETA: 1:07:44.432118\n",
      "[Epoch 74/200] [Batch 400/476] [D loss: 0.645215] [G loss: 2.502337, adv: 1.598099, cycle: 0.067263, identity: 0.046322] ETA: 1:13:46.215742\n",
      "[Epoch 75/200] [Batch 0/476] [D loss: 0.502493] [G loss: 2.005474, adv: 1.149911, cycle: 0.056850, identity: 0.057412] ETA: 3:59:55.856857\n",
      "[Epoch 75/200] [Batch 100/476] [D loss: 0.463407] [G loss: 1.952140, adv: 1.264181, cycle: 0.044827, identity: 0.047937] ETA: 1:01:26.215210\n",
      "[Epoch 75/200] [Batch 200/476] [D loss: 0.408904] [G loss: 2.290693, adv: 1.409449, cycle: 0.055644, identity: 0.064960] ETA: 1:09:48.801646\n",
      "[Epoch 75/200] [Batch 300/476] [D loss: 0.478135] [G loss: 2.468946, adv: 1.583031, cycle: 0.060785, identity: 0.055612] ETA: 1:07:31.476288\n",
      "[Epoch 75/200] [Batch 400/476] [D loss: 0.515375] [G loss: 2.935109, adv: 1.830579, cycle: 0.075980, identity: 0.068947] ETA: 1:07:58.830314\n",
      "[Epoch 76/200] [Batch 0/476] [D loss: 0.459846] [G loss: 2.302169, adv: 1.429222, cycle: 0.061492, identity: 0.051605] ETA: 4:22:59.191456\n",
      "[Epoch 76/200] [Batch 100/476] [D loss: 0.373033] [G loss: 1.955230, adv: 1.102427, cycle: 0.057206, identity: 0.056148] ETA: 1:05:23.654987\n",
      "[Epoch 76/200] [Batch 200/476] [D loss: 0.498903] [G loss: 2.720193, adv: 1.649156, cycle: 0.077645, identity: 0.058918] ETA: 1:06:39.475611\n",
      "[Epoch 76/200] [Batch 300/476] [D loss: 0.569011] [G loss: 2.780258, adv: 1.841271, cycle: 0.063669, identity: 0.060459] ETA: 1:10:44.664622\n",
      "[Epoch 76/200] [Batch 400/476] [D loss: 0.511713] [G loss: 2.473211, adv: 1.575305, cycle: 0.065404, identity: 0.048774] ETA: 1:12:52.384888\n",
      "[Epoch 77/200] [Batch 0/476] [D loss: 0.500191] [G loss: 2.002054, adv: 1.117199, cycle: 0.065912, identity: 0.045148] ETA: 3:35:20.609848\n",
      "[Epoch 77/200] [Batch 100/476] [D loss: 0.467203] [G loss: 2.496682, adv: 1.557470, cycle: 0.065608, identity: 0.056627] ETA: 1:16:32.071720\n",
      "[Epoch 77/200] [Batch 200/476] [D loss: 0.454447] [G loss: 2.282609, adv: 1.521542, cycle: 0.051177, identity: 0.049859] ETA: 1:06:10.895972\n",
      "[Epoch 77/200] [Batch 300/476] [D loss: 0.423728] [G loss: 2.032938, adv: 1.302269, cycle: 0.053434, identity: 0.039267] ETA: 1:05:09.054646\n",
      "[Epoch 77/200] [Batch 400/476] [D loss: 0.482364] [G loss: 3.019581, adv: 1.607929, cycle: 0.102416, identity: 0.077498] ETA: 1:05:14.751481\n",
      "[Epoch 78/200] [Batch 0/476] [D loss: 0.514692] [G loss: 2.413334, adv: 1.534391, cycle: 0.060389, identity: 0.055011] ETA: 2:58:32.787474\n",
      "[Epoch 78/200] [Batch 100/476] [D loss: 0.548326] [G loss: 2.545255, adv: 1.624832, cycle: 0.064002, identity: 0.056081] ETA: 1:11:40.964604\n",
      "[Epoch 78/200] [Batch 200/476] [D loss: 0.409756] [G loss: 2.270924, adv: 1.460717, cycle: 0.054741, identity: 0.052560] ETA: 1:00:20.435642\n",
      "[Epoch 78/200] [Batch 300/476] [D loss: 0.411108] [G loss: 2.311862, adv: 1.490903, cycle: 0.061010, identity: 0.042172] ETA: 1:02:15.720759\n",
      "[Epoch 78/200] [Batch 400/476] [D loss: 0.504151] [G loss: 2.185464, adv: 1.288807, cycle: 0.063398, identity: 0.052535] ETA: 1:03:17.826073\n",
      "[Epoch 79/200] [Batch 0/476] [D loss: 0.478344] [G loss: 2.130276, adv: 1.414868, cycle: 0.043776, identity: 0.055529] ETA: 3:08:52.503066\n",
      "[Epoch 79/200] [Batch 100/476] [D loss: 0.394480] [G loss: 2.234166, adv: 1.379622, cycle: 0.058999, identity: 0.052911] ETA: 1:07:03.921091\n",
      "[Epoch 79/200] [Batch 200/476] [D loss: 0.384914] [G loss: 2.416150, adv: 1.485373, cycle: 0.064622, identity: 0.056912] ETA: 1:03:07.820386\n",
      "[Epoch 79/200] [Batch 300/476] [D loss: 0.485792] [G loss: 2.294908, adv: 1.667724, cycle: 0.041280, identity: 0.042878] ETA: 0:59:36.874550\n",
      "[Epoch 79/200] [Batch 400/476] [D loss: 0.525711] [G loss: 2.486782, adv: 1.604007, cycle: 0.065002, identity: 0.046551] ETA: 1:03:58.140718\n",
      "[Epoch 80/200] [Batch 0/476] [D loss: 0.513154] [G loss: 2.379993, adv: 1.478881, cycle: 0.062542, identity: 0.055139] ETA: 2:58:50.019150\n",
      "[Epoch 80/200] [Batch 100/476] [D loss: 0.465279] [G loss: 2.567541, adv: 1.563485, cycle: 0.075203, identity: 0.050406] ETA: 1:00:13.533525\n",
      "[Epoch 80/200] [Batch 200/476] [D loss: 0.500347] [G loss: 2.598090, adv: 1.658128, cycle: 0.058217, identity: 0.071559] ETA: 1:00:29.031610\n",
      "[Epoch 80/200] [Batch 300/476] [D loss: 0.517455] [G loss: 2.200931, adv: 1.498567, cycle: 0.046734, identity: 0.047005] ETA: 1:03:28.641930\n",
      "[Epoch 80/200] [Batch 400/476] [D loss: 0.583134] [G loss: 2.975591, adv: 1.760759, cycle: 0.087086, identity: 0.068794] ETA: 1:06:35.021782\n",
      "[Epoch 81/200] [Batch 0/476] [D loss: 0.492772] [G loss: 2.243138, adv: 1.398440, cycle: 0.058245, identity: 0.052450] ETA: 2:57:50.569879\n",
      "[Epoch 81/200] [Batch 100/476] [D loss: 0.414609] [G loss: 2.180667, adv: 1.282064, cycle: 0.056596, identity: 0.066529] ETA: 1:09:20.171997\n",
      "[Epoch 81/200] [Batch 200/476] [D loss: 0.504120] [G loss: 1.962372, adv: 1.344926, cycle: 0.041459, identity: 0.040570] ETA: 1:03:01.783420\n",
      "[Epoch 81/200] [Batch 300/476] [D loss: 0.457073] [G loss: 2.567541, adv: 1.563485, cycle: 0.075203, identity: 0.050406] ETA: 0:58:22.317024\n",
      "[Epoch 81/200] [Batch 400/476] [D loss: 0.462555] [G loss: 2.489727, adv: 1.547325, cycle: 0.061473, identity: 0.065534] ETA: 1:13:00.626066\n",
      "[Epoch 82/200] [Batch 0/476] [D loss: 0.408302] [G loss: 2.028725, adv: 1.075782, cycle: 0.063908, identity: 0.062774] ETA: 3:18:25.520927\n",
      "[Epoch 82/200] [Batch 100/476] [D loss: 0.500366] [G loss: 2.423726, adv: 1.633190, cycle: 0.054478, identity: 0.049151] ETA: 1:07:12.018304\n",
      "[Epoch 82/200] [Batch 200/476] [D loss: 0.437379] [G loss: 2.009917, adv: 1.377174, cycle: 0.042712, identity: 0.041125] ETA: 1:05:43.403069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 82/200] [Batch 300/476] [D loss: 0.464028] [G loss: 1.940802, adv: 1.163587, cycle: 0.048516, identity: 0.058410] ETA: 1:11:12.872952\n",
      "[Epoch 82/200] [Batch 400/476] [D loss: 0.443429] [G loss: 2.274064, adv: 1.301475, cycle: 0.065743, identity: 0.063032] ETA: 0:58:40.495407\n",
      "[Epoch 83/200] [Batch 0/476] [D loss: 0.554077] [G loss: 2.257644, adv: 1.337681, cycle: 0.063516, identity: 0.056961] ETA: 3:08:09.612605\n",
      "[Epoch 83/200] [Batch 100/476] [D loss: 0.399049] [G loss: 2.328885, adv: 1.326483, cycle: 0.071826, identity: 0.056829] ETA: 1:03:53.091452\n",
      "[Epoch 83/200] [Batch 200/476] [D loss: 0.449289] [G loss: 2.567541, adv: 1.563485, cycle: 0.075203, identity: 0.050406] ETA: 1:06:31.085936\n",
      "[Epoch 83/200] [Batch 300/476] [D loss: 0.513862] [G loss: 3.367979, adv: 2.084400, cycle: 0.093810, identity: 0.069096] ETA: 1:07:18.727066\n",
      "[Epoch 83/200] [Batch 400/476] [D loss: 0.471028] [G loss: 1.983853, adv: 1.202811, cycle: 0.055378, identity: 0.045453] ETA: 1:08:17.955494\n",
      "[Epoch 84/200] [Batch 0/476] [D loss: 0.453789] [G loss: 2.024423, adv: 1.178500, cycle: 0.058482, identity: 0.052220] ETA: 2:58:55.126530\n",
      "[Epoch 84/200] [Batch 100/476] [D loss: 0.457605] [G loss: 2.310787, adv: 1.439049, cycle: 0.063863, identity: 0.046622] ETA: 0:58:46.156503\n",
      "[Epoch 84/200] [Batch 200/476] [D loss: 0.407216] [G loss: 2.454028, adv: 1.481028, cycle: 0.070319, identity: 0.053962] ETA: 0:58:51.983694\n",
      "[Epoch 84/200] [Batch 300/476] [D loss: 0.433133] [G loss: 2.528434, adv: 1.495501, cycle: 0.074188, identity: 0.058211] ETA: 1:09:18.557694\n",
      "[Epoch 84/200] [Batch 400/476] [D loss: 0.401624] [G loss: 2.347727, adv: 1.366198, cycle: 0.060877, identity: 0.074551] ETA: 1:03:06.342682\n",
      "[Epoch 85/200] [Batch 0/476] [D loss: 0.415182] [G loss: 1.868934, adv: 0.967535, cycle: 0.054471, identity: 0.071338] ETA: 3:09:04.806218\n",
      "[Epoch 85/200] [Batch 100/476] [D loss: 0.440595] [G loss: 2.155714, adv: 1.198486, cycle: 0.063564, identity: 0.064317] ETA: 0:58:41.992321\n",
      "[Epoch 85/200] [Batch 200/476] [D loss: 0.442771] [G loss: 2.310849, adv: 1.476599, cycle: 0.055479, identity: 0.055891] ETA: 1:02:03.314023\n",
      "[Epoch 85/200] [Batch 300/476] [D loss: 0.503352] [G loss: 2.327392, adv: 1.456007, cycle: 0.064168, identity: 0.045940] ETA: 1:12:59.766054\n",
      "[Epoch 85/200] [Batch 400/476] [D loss: 0.528773] [G loss: 2.913275, adv: 1.847785, cycle: 0.068814, identity: 0.075471] ETA: 1:04:47.204947\n",
      "[Epoch 86/200] [Batch 0/476] [D loss: 0.516083] [G loss: 2.861422, adv: 1.764996, cycle: 0.080606, identity: 0.058073] ETA: 2:50:30.558386\n",
      "[Epoch 86/200] [Batch 100/476] [D loss: 0.542271] [G loss: 2.159594, adv: 1.441028, cycle: 0.047961, identity: 0.047791] ETA: 1:01:24.796276\n",
      "[Epoch 86/200] [Batch 200/476] [D loss: 0.431900] [G loss: 2.073778, adv: 1.169058, cycle: 0.060300, identity: 0.060344] ETA: 1:03:51.215031\n",
      "[Epoch 86/200] [Batch 300/476] [D loss: 0.413374] [G loss: 2.356679, adv: 1.517141, cycle: 0.051601, identity: 0.064705] ETA: 1:00:40.774933\n",
      "[Epoch 86/200] [Batch 400/476] [D loss: 0.397691] [G loss: 2.016722, adv: 1.153472, cycle: 0.057410, identity: 0.057831] ETA: 1:03:07.633554\n",
      "[Epoch 87/200] [Batch 0/476] [D loss: 0.471512] [G loss: 2.127079, adv: 1.257352, cycle: 0.058327, identity: 0.057292] ETA: 3:02:03.020096\n",
      "[Epoch 87/200] [Batch 100/476] [D loss: 0.485619] [G loss: 2.269437, adv: 1.306096, cycle: 0.061106, identity: 0.070456] ETA: 1:07:01.124067\n",
      "[Epoch 87/200] [Batch 200/476] [D loss: 0.508320] [G loss: 2.886940, adv: 1.757607, cycle: 0.084112, identity: 0.057642] ETA: 1:04:28.098581\n",
      "[Epoch 87/200] [Batch 300/476] [D loss: 0.608652] [G loss: 2.554825, adv: 1.457564, cycle: 0.080720, identity: 0.058012] ETA: 1:04:20.013191\n",
      "[Epoch 87/200] [Batch 400/476] [D loss: 0.449383] [G loss: 1.780614, adv: 1.074807, cycle: 0.043017, identity: 0.055128] ETA: 1:01:23.874135\n",
      "[Epoch 88/200] [Batch 0/476] [D loss: 0.536739] [G loss: 2.456072, adv: 1.622720, cycle: 0.059636, identity: 0.047398] ETA: 2:47:12.326813\n",
      "[Epoch 88/200] [Batch 100/476] [D loss: 0.498963] [G loss: 2.044822, adv: 1.295385, cycle: 0.047093, identity: 0.055701] ETA: 1:01:15.726118\n",
      "[Epoch 88/200] [Batch 200/476] [D loss: 0.436719] [G loss: 1.901604, adv: 1.163033, cycle: 0.051229, identity: 0.045256] ETA: 0:57:02.234007\n",
      "[Epoch 88/200] [Batch 300/476] [D loss: 0.492950] [G loss: 2.142757, adv: 1.293353, cycle: 0.057772, identity: 0.054336] ETA: 0:59:34.815051\n",
      "[Epoch 88/200] [Batch 400/476] [D loss: 0.358465] [G loss: 2.115832, adv: 1.269764, cycle: 0.055656, identity: 0.057902] ETA: 0:56:38.195309\n",
      "[Epoch 89/200] [Batch 0/476] [D loss: 0.490979] [G loss: 2.312995, adv: 1.449428, cycle: 0.068637, identity: 0.035439] ETA: 3:04:20.365763\n",
      "[Epoch 89/200] [Batch 100/476] [D loss: 0.551645] [G loss: 2.133602, adv: 1.435760, cycle: 0.047251, identity: 0.045067] ETA: 1:01:57.341919\n",
      "[Epoch 89/200] [Batch 200/476] [D loss: 0.492878] [G loss: 1.962372, adv: 1.344926, cycle: 0.041459, identity: 0.040570] ETA: 1:03:47.767891\n",
      "[Epoch 89/200] [Batch 300/476] [D loss: 0.539754] [G loss: 2.417598, adv: 1.417439, cycle: 0.077390, identity: 0.045252] ETA: 1:03:30.187208\n",
      "[Epoch 89/200] [Batch 400/476] [D loss: 0.467130] [G loss: 2.424533, adv: 1.326057, cycle: 0.088574, identity: 0.042548] ETA: 1:02:25.914352\n",
      "[Epoch 90/200] [Batch 0/476] [D loss: 0.474714] [G loss: 2.316168, adv: 1.295808, cycle: 0.070420, identity: 0.063231] ETA: 2:46:29.399128\n",
      "[Epoch 90/200] [Batch 100/476] [D loss: 0.494197] [G loss: 2.294908, adv: 1.667724, cycle: 0.041280, identity: 0.042878] ETA: 1:03:57.205811\n",
      "[Epoch 90/200] [Batch 200/476] [D loss: 0.539943] [G loss: 2.409564, adv: 1.369384, cycle: 0.074442, identity: 0.059153] ETA: 1:05:01.096191\n",
      "[Epoch 90/200] [Batch 300/476] [D loss: 0.428013] [G loss: 2.270924, adv: 1.460717, cycle: 0.054741, identity: 0.052560] ETA: 1:00:32.032695\n",
      "[Epoch 90/200] [Batch 400/476] [D loss: 0.460816] [G loss: 2.291028, adv: 1.281720, cycle: 0.068780, identity: 0.064302] ETA: 1:03:11.739693\n",
      "[Epoch 91/200] [Batch 0/476] [D loss: 0.326586] [G loss: 2.033234, adv: 1.282640, cycle: 0.054911, identity: 0.040297] ETA: 2:42:51.693825\n",
      "[Epoch 91/200] [Batch 100/476] [D loss: 0.554829] [G loss: 2.589240, adv: 1.566826, cycle: 0.071475, identity: 0.061532] ETA: 0:59:26.787354\n",
      "[Epoch 91/200] [Batch 200/476] [D loss: 0.492653] [G loss: 2.310733, adv: 1.583776, cycle: 0.046568, identity: 0.052256] ETA: 1:03:22.971708\n",
      "[Epoch 91/200] [Batch 300/476] [D loss: 0.567098] [G loss: 2.384294, adv: 1.342232, cycle: 0.068529, identity: 0.071355] ETA: 1:04:49.525574\n",
      "[Epoch 91/200] [Batch 400/476] [D loss: 0.541712] [G loss: 2.437944, adv: 1.569844, cycle: 0.054584, identity: 0.064451] ETA: 0:57:21.493999\n",
      "[Epoch 92/200] [Batch 0/476] [D loss: 0.491210] [G loss: 2.218308, adv: 1.247061, cycle: 0.065196, identity: 0.063857] ETA: 2:53:18.873814\n",
      "[Epoch 92/200] [Batch 100/476] [D loss: 0.496117] [G loss: 2.413334, adv: 1.534391, cycle: 0.060389, identity: 0.055011] ETA: 1:05:26.343310\n",
      "[Epoch 92/200] [Batch 200/476] [D loss: 0.477642] [G loss: 1.973574, adv: 1.264193, cycle: 0.048858, identity: 0.044160] ETA: 0:52:19.736113\n",
      "[Epoch 92/200] [Batch 300/476] [D loss: 0.447723] [G loss: 2.516351, adv: 1.583667, cycle: 0.058232, identity: 0.070074] ETA: 0:57:01.173057\n",
      "[Epoch 92/200] [Batch 400/476] [D loss: 0.443845] [G loss: 2.084309, adv: 1.286596, cycle: 0.053713, identity: 0.052117] ETA: 0:54:12.235748\n",
      "[Epoch 93/200] [Batch 0/476] [D loss: 0.507825] [G loss: 2.413334, adv: 1.534391, cycle: 0.060389, identity: 0.055011] ETA: 2:40:28.923241\n",
      "[Epoch 93/200] [Batch 100/476] [D loss: 0.528994] [G loss: 2.159594, adv: 1.441028, cycle: 0.047961, identity: 0.047791] ETA: 1:01:47.982594\n",
      "[Epoch 93/200] [Batch 200/476] [D loss: 0.449521] [G loss: 2.316340, adv: 1.379874, cycle: 0.060645, identity: 0.066004] ETA: 0:54:54.559040\n",
      "[Epoch 93/200] [Batch 300/476] [D loss: 0.418912] [G loss: 2.248316, adv: 1.336284, cycle: 0.060125, identity: 0.062156] ETA: 0:54:02.579170\n",
      "[Epoch 93/200] [Batch 400/476] [D loss: 0.401179] [G loss: 2.442231, adv: 1.507453, cycle: 0.059985, identity: 0.066985] ETA: 0:53:18.838929\n",
      "[Epoch 94/200] [Batch 0/476] [D loss: 0.566498] [G loss: 2.352597, adv: 1.506616, cycle: 0.060196, identity: 0.048804] ETA: 2:39:45.151211\n",
      "[Epoch 94/200] [Batch 100/476] [D loss: 0.503989] [G loss: 2.563469, adv: 1.637079, cycle: 0.064815, identity: 0.055648] ETA: 0:53:28.239494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 94/200] [Batch 200/476] [D loss: 0.555611] [G loss: 2.285561, adv: 1.324404, cycle: 0.061952, identity: 0.068327] ETA: 0:59:52.001129\n",
      "[Epoch 94/200] [Batch 300/476] [D loss: 0.590813] [G loss: 2.780258, adv: 1.841271, cycle: 0.063669, identity: 0.060459] ETA: 0:53:46.181803\n",
      "[Epoch 94/200] [Batch 400/476] [D loss: 0.498287] [G loss: 2.436456, adv: 1.630832, cycle: 0.054671, identity: 0.051782] ETA: 0:56:17.962263\n",
      "[Epoch 95/200] [Batch 0/476] [D loss: 0.388284] [G loss: 2.441398, adv: 1.515830, cycle: 0.061607, identity: 0.061899] ETA: 2:36:18.435445\n",
      "[Epoch 95/200] [Batch 100/476] [D loss: 0.514386] [G loss: 2.340749, adv: 1.413212, cycle: 0.063346, identity: 0.058815] ETA: 0:51:38.044167\n",
      "[Epoch 95/200] [Batch 200/476] [D loss: 0.541244] [G loss: 3.193427, adv: 2.039233, cycle: 0.085453, identity: 0.059933] ETA: 0:54:49.894314\n",
      "[Epoch 95/200] [Batch 300/476] [D loss: 0.533777] [G loss: 2.466260, adv: 1.542658, cycle: 0.060142, identity: 0.064437] ETA: 0:48:35.402927\n",
      "[Epoch 95/200] [Batch 400/476] [D loss: 0.428932] [G loss: 2.290693, adv: 1.409449, cycle: 0.055644, identity: 0.064960] ETA: 0:55:04.951916\n",
      "[Epoch 96/200] [Batch 0/476] [D loss: 0.520788] [G loss: 2.089520, adv: 1.316489, cycle: 0.054106, identity: 0.046394] ETA: 2:31:37.406189\n",
      "[Epoch 96/200] [Batch 100/476] [D loss: 0.437625] [G loss: 1.908765, adv: 1.198684, cycle: 0.046678, identity: 0.048660] ETA: 0:59:40.894338\n",
      "[Epoch 96/200] [Batch 200/476] [D loss: 0.477207] [G loss: 1.900802, adv: 1.048982, cycle: 0.057959, identity: 0.054447] ETA: 0:52:58.690254\n",
      "[Epoch 96/200] [Batch 300/476] [D loss: 0.446478] [G loss: 2.433824, adv: 1.546793, cycle: 0.061401, identity: 0.054604] ETA: 1:01:32.953586\n",
      "[Epoch 96/200] [Batch 400/476] [D loss: 0.339252] [G loss: 2.194009, adv: 1.128836, cycle: 0.074868, identity: 0.063298] ETA: 1:00:27.660931\n",
      "[Epoch 97/200] [Batch 0/476] [D loss: 0.545925] [G loss: 2.461415, adv: 1.493788, cycle: 0.071140, identity: 0.051246] ETA: 2:38:34.740320\n",
      "[Epoch 97/200] [Batch 100/476] [D loss: 0.485874] [G loss: 2.602794, adv: 1.597283, cycle: 0.070106, identity: 0.060891] ETA: 0:54:37.775085\n",
      "[Epoch 97/200] [Batch 200/476] [D loss: 0.430202] [G loss: 2.370250, adv: 1.298361, cycle: 0.074361, identity: 0.065656] ETA: 0:57:19.318897\n",
      "[Epoch 97/200] [Batch 300/476] [D loss: 0.516599] [G loss: 2.385800, adv: 1.351493, cycle: 0.068651, identity: 0.069560] ETA: 0:49:52.337585\n",
      "[Epoch 97/200] [Batch 400/476] [D loss: 0.538459] [G loss: 2.103090, adv: 1.268348, cycle: 0.055164, identity: 0.056621] ETA: 0:52:18.353962\n",
      "[Epoch 98/200] [Batch 0/476] [D loss: 0.606564] [G loss: 2.502558, adv: 1.714641, cycle: 0.053171, identity: 0.051242] ETA: 2:36:42.685513\n",
      "[Epoch 98/200] [Batch 100/476] [D loss: 0.499514] [G loss: 2.456340, adv: 1.556754, cycle: 0.067091, identity: 0.045735] ETA: 0:52:33.703167\n",
      "[Epoch 98/200] [Batch 200/476] [D loss: 0.574480] [G loss: 2.384294, adv: 1.342232, cycle: 0.068529, identity: 0.071355] ETA: 0:52:59.196014\n",
      "[Epoch 98/200] [Batch 300/476] [D loss: 0.457203] [G loss: 2.183160, adv: 1.344865, cycle: 0.052029, identity: 0.063601] ETA: 0:49:02.077274\n",
      "[Epoch 98/200] [Batch 400/476] [D loss: 0.529268] [G loss: 2.458335, adv: 1.557137, cycle: 0.059434, identity: 0.061372] ETA: 0:53:55.788813\n",
      "[Epoch 99/200] [Batch 0/476] [D loss: 0.540326] [G loss: 2.683531, adv: 1.806323, cycle: 0.063855, identity: 0.047731] ETA: 2:26:16.982220\n",
      "[Epoch 99/200] [Batch 100/476] [D loss: 0.430399] [G loss: 1.947931, adv: 1.199450, cycle: 0.053087, identity: 0.043523] ETA: 0:49:43.504297\n",
      "[Epoch 99/200] [Batch 200/476] [D loss: 0.498405] [G loss: 2.173418, adv: 1.452359, cycle: 0.045452, identity: 0.053307] ETA: 0:56:15.743894\n",
      "[Epoch 99/200] [Batch 300/476] [D loss: 0.503020] [G loss: 2.004832, adv: 1.127823, cycle: 0.064513, identity: 0.046377] ETA: 0:51:22.331032\n",
      "[Epoch 99/200] [Batch 400/476] [D loss: 0.422325] [G loss: 2.023232, adv: 1.174749, cycle: 0.055626, identity: 0.058445] ETA: 0:49:46.240468\n",
      "[Epoch 100/200] [Batch 0/476] [D loss: 0.525689] [G loss: 2.099478, adv: 1.475994, cycle: 0.042640, identity: 0.039417] ETA: 2:28:19.680996\n",
      "[Epoch 100/200] [Batch 100/476] [D loss: 0.461209] [G loss: 2.046082, adv: 1.279859, cycle: 0.050284, identity: 0.052676] ETA: 0:51:46.268048\n",
      "[Epoch 100/200] [Batch 200/476] [D loss: 0.539094] [G loss: 2.545255, adv: 1.624832, cycle: 0.064002, identity: 0.056081] ETA: 0:55:17.861223\n",
      "[Epoch 100/200] [Batch 300/476] [D loss: 0.523287] [G loss: 2.638178, adv: 1.807305, cycle: 0.061011, identity: 0.044152] ETA: 0:52:34.198670\n",
      "[Epoch 100/200] [Batch 400/476] [D loss: 0.497741] [G loss: 1.859900, adv: 1.167159, cycle: 0.039463, identity: 0.059623] ETA: 0:48:03.740234\n",
      "[Epoch 101/200] [Batch 0/476] [D loss: 0.544054] [G loss: 2.108617, adv: 1.222574, cycle: 0.062110, identity: 0.052989] ETA: 3:16:12.247750\n",
      "[Epoch 101/200] [Batch 100/476] [D loss: 0.340407] [G loss: 2.150200, adv: 1.404260, cycle: 0.050588, identity: 0.048013] ETA: 1:03:13.622238\n",
      "[Epoch 101/200] [Batch 200/476] [D loss: 0.465735] [G loss: 2.198771, adv: 1.445328, cycle: 0.046982, identity: 0.056725] ETA: 0:50:58.363850\n",
      "[Epoch 101/200] [Batch 300/476] [D loss: 0.460023] [G loss: 2.525631, adv: 1.493240, cycle: 0.072651, identity: 0.061176] ETA: 0:48:23.067369\n",
      "[Epoch 101/200] [Batch 400/476] [D loss: 0.458004] [G loss: 2.102903, adv: 1.301455, cycle: 0.053685, identity: 0.052920] ETA: 1:01:24.077445\n",
      "[Epoch 102/200] [Batch 0/476] [D loss: 0.408489] [G loss: 2.055321, adv: 1.270918, cycle: 0.056015, identity: 0.044851] ETA: 2:28:07.657249\n",
      "[Epoch 102/200] [Batch 100/476] [D loss: 0.447137] [G loss: 2.269391, adv: 1.489643, cycle: 0.057162, identity: 0.041626] ETA: 0:54:18.345795\n",
      "[Epoch 102/200] [Batch 200/476] [D loss: 0.524314] [G loss: 2.213477, adv: 1.464651, cycle: 0.050661, identity: 0.048444] ETA: 0:52:25.422382\n",
      "[Epoch 102/200] [Batch 300/476] [D loss: 0.555205] [G loss: 2.151593, adv: 1.390085, cycle: 0.054014, identity: 0.044273] ETA: 0:47:19.874496\n",
      "[Epoch 102/200] [Batch 400/476] [D loss: 0.397346] [G loss: 2.234166, adv: 1.379622, cycle: 0.058999, identity: 0.052911] ETA: 0:49:00.570803\n",
      "[Epoch 103/200] [Batch 0/476] [D loss: 0.425420] [G loss: 2.458329, adv: 1.436162, cycle: 0.066956, identity: 0.070521] ETA: 2:29:47.090573\n",
      "[Epoch 103/200] [Batch 100/476] [D loss: 0.444376] [G loss: 2.274064, adv: 1.301475, cycle: 0.065743, identity: 0.063032] ETA: 0:50:33.677330\n",
      "[Epoch 103/200] [Batch 200/476] [D loss: 0.551793] [G loss: 2.589240, adv: 1.566826, cycle: 0.071475, identity: 0.061532] ETA: 0:50:31.498838\n",
      "[Epoch 103/200] [Batch 300/476] [D loss: 0.475055] [G loss: 2.109517, adv: 1.368830, cycle: 0.049977, identity: 0.048183] ETA: 0:50:38.006821\n",
      "[Epoch 103/200] [Batch 400/476] [D loss: 0.461166] [G loss: 2.468946, adv: 1.583031, cycle: 0.060785, identity: 0.055612] ETA: 0:47:44.307604\n",
      "[Epoch 104/200] [Batch 0/476] [D loss: 0.423312] [G loss: 2.054097, adv: 1.206301, cycle: 0.058257, identity: 0.053045] ETA: 2:27:10.378876\n",
      "[Epoch 104/200] [Batch 100/476] [D loss: 0.504409] [G loss: 2.544926, adv: 1.271648, cycle: 0.087698, identity: 0.079260] ETA: 0:54:45.772273\n",
      "[Epoch 104/200] [Batch 200/476] [D loss: 0.499736] [G loss: 2.200931, adv: 1.498567, cycle: 0.046734, identity: 0.047005] ETA: 0:50:58.185638\n",
      "[Epoch 104/200] [Batch 300/476] [D loss: 0.555489] [G loss: 2.269437, adv: 1.306096, cycle: 0.061106, identity: 0.070456] ETA: 0:46:51.977712\n",
      "[Epoch 104/200] [Batch 400/476] [D loss: 0.431019] [G loss: 2.125718, adv: 1.191031, cycle: 0.067435, identity: 0.052067] ETA: 0:48:19.230659\n",
      "[Epoch 105/200] [Batch 0/476] [D loss: 0.450019] [G loss: 1.895426, adv: 1.316664, cycle: 0.040566, identity: 0.034620] ETA: 2:25:23.765554\n",
      "[Epoch 105/200] [Batch 100/476] [D loss: 0.451079] [G loss: 1.809701, adv: 1.047339, cycle: 0.051646, identity: 0.049181] ETA: 0:49:22.557678\n",
      "[Epoch 105/200] [Batch 200/476] [D loss: 0.507475] [G loss: 2.757257, adv: 1.701081, cycle: 0.074965, identity: 0.061305] ETA: 0:49:25.448031\n",
      "[Epoch 105/200] [Batch 300/476] [D loss: 0.540388] [G loss: 2.886940, adv: 1.757607, cycle: 0.084112, identity: 0.057642] ETA: 0:50:56.951790\n",
      "[Epoch 105/200] [Batch 400/476] [D loss: 0.412837] [G loss: 2.228353, adv: 1.358038, cycle: 0.058203, identity: 0.057657] ETA: 0:43:36.180539\n",
      "[Epoch 106/200] [Batch 0/476] [D loss: 0.495163] [G loss: 2.261092, adv: 1.426424, cycle: 0.062236, identity: 0.042461] ETA: 2:29:38.021240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 106/200] [Batch 100/476] [D loss: 0.452207] [G loss: 2.021915, adv: 1.325482, cycle: 0.046784, identity: 0.045719] ETA: 0:55:58.594837\n",
      "[Epoch 106/200] [Batch 200/476] [D loss: 0.468238] [G loss: 2.374851, adv: 1.493929, cycle: 0.059964, identity: 0.056256] ETA: 0:56:16.538818\n",
      "[Epoch 106/200] [Batch 300/476] [D loss: 0.489644] [G loss: 1.841742, adv: 1.205466, cycle: 0.043214, identity: 0.040827] ETA: 0:59:07.421055\n",
      "[Epoch 106/200] [Batch 400/476] [D loss: 0.512845] [G loss: 2.065351, adv: 1.299897, cycle: 0.049964, identity: 0.053162] ETA: 0:50:39.965803\n",
      "[Epoch 107/200] [Batch 0/476] [D loss: 0.479084] [G loss: 2.496989, adv: 1.594411, cycle: 0.058681, identity: 0.063153] ETA: 2:28:12.093693\n",
      "[Epoch 107/200] [Batch 100/476] [D loss: 0.494778] [G loss: 1.973574, adv: 1.264193, cycle: 0.048858, identity: 0.044160] ETA: 0:57:01.087027\n",
      "[Epoch 107/200] [Batch 200/476] [D loss: 0.392010] [G loss: 2.359776, adv: 1.458809, cycle: 0.064439, identity: 0.051316] ETA: 0:53:44.621316\n",
      "[Epoch 107/200] [Batch 300/476] [D loss: 0.517019] [G loss: 2.379993, adv: 1.478881, cycle: 0.062542, identity: 0.055139] ETA: 0:50:20.992767\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "dataset_name = 'CycleGAN3'\n",
    "\n",
    "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "train_gan(dataloader3, epoch)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'CycleGAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcab592ce4f4bef9a64bd02a48f4a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=476.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ADgan1 = []\n",
    "NCgan1 = []\n",
    "\n",
    "ADgan2 = []\n",
    "NCgan2 = []\n",
    "\n",
    "ADgan3 = []\n",
    "NCgan3 = []\n",
    "\n",
    "\n",
    "epoch = 25\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "\n",
    "for imgs in tqdm(dataloader1):\n",
    "\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = torch.squeeze(G_AB(real_A))\n",
    "    fake_B = fake_B.detach().cpu()\n",
    "\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = torch.squeeze(G_BA(real_B))\n",
    "    fake_A = fake_A.detach().cpu()\n",
    "    \n",
    "    B = torch.stack([fake_B, fake_B, fake_B], 0)\n",
    "    ADgan1.append((B.reshape(3, 64, 64).double(), torch.tensor(1).long()))\n",
    "\n",
    "    A = torch.stack([fake_A, fake_A, fake_A], 0)\n",
    "    NCgan1.append((A.reshape(3, 64, 64).double(), torch.tensor(0).long()))\n",
    "    \n",
    "    \n",
    "torch.save(ADgan1, '../datasets/64ADgan1.pt')\n",
    "torch.save(NCgan1, '../datasets/64NCgan1.pt')\n",
    "\n",
    "    \n",
    "# for imgs in tqdm(dataloader2):\n",
    "\n",
    "#     G_AB.eval()\n",
    "#     G_BA.eval()\n",
    "    \n",
    "#     real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "#     fake_B = torch.squeeze(G_AB(real_A))\n",
    "#     fake_B = fake_B.detach().cpu()\n",
    "\n",
    "#     real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "#     fake_A = torch.squeeze(G_BA(real_B))\n",
    "#     fake_A = fake_A.detach().cpu()\n",
    "    \n",
    "#     ADgan2.append(fake_B)\n",
    "#     NCgan2.append(fake_A)\n",
    "    \n",
    "# for imgs in tqdm(dataloader3):\n",
    "\n",
    "#     G_AB.eval()\n",
    "#     G_BA.eval()\n",
    "\n",
    "#     real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "#     fake_B = torch.squeeze(G_AB(real_A))\n",
    "#     fake_B = fake_B.detach().cpu()\n",
    "\n",
    "#     real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "#     fake_A = torch.squeeze(G_BA(real_B))\n",
    "#     fake_A = fake_A.detach().cpu()\n",
    "    \n",
    "#     ADgan3.append(fake_B)\n",
    "#     NCgan3.append(fake_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
