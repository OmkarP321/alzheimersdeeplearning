{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(csvpath):\n",
    "    #     Create dataframe from CSV\n",
    "    table = pd.read_csv(csvpath)\n",
    "    return table[['Image Data ID', 'Subject', 'Group', 'Sex', 'Age']]\n",
    "\n",
    "\n",
    "def create_paths(datapath):\n",
    "    #     Create paths to all nested images\n",
    "    imagepaths = []\n",
    "    for root, dirs, files in os.walk(datapath, topdown=False):\n",
    "        for name in files:\n",
    "            imagepaths.append(os.path.join(root, name))\n",
    "    return imagepaths\n",
    "\n",
    "\n",
    "def prepare_data(datapath, csvpath):\n",
    "    #     Create training set\n",
    "    train_data = []\n",
    "    imagepaths = create_paths(datapath)\n",
    "    table = create_df(csvpath)\n",
    "\n",
    "    for imagepath in tqdm(imagepaths[:-1]):\n",
    "\n",
    "        #       Find ID of each image, then lookup values using it\n",
    "        #         13th index for linux, 10th for windows\n",
    "        path = imagepath.split('\\\\')[13]\n",
    "        img_id = path[path.find('_I') + 2:-4]\n",
    "        subject_group = table.loc[table['Image Data ID'] == int(\n",
    "            img_id)][\"Group\"].reset_index(drop=True)[0]\n",
    "        subject_age = table.loc[table['Image Data ID'] == int(\n",
    "            img_id)][\"Age\"].reset_index(drop=True)[0]\n",
    "        subject_sex = table.loc[table['Image Data ID'] == int(\n",
    "            img_id)][\"Sex\"].reset_index(drop=True)[0]\n",
    "        subject_id = table.loc[table['Image Data ID'] == int(\n",
    "            img_id)][\"Subject\"].reset_index(drop=True)[0]\n",
    "\n",
    "        #       Nibabel to load NIFTI Images\n",
    "        img = nib.load(imagepath)\n",
    "        imgdata = img.get_fdata()\n",
    "\n",
    "        train_data.append((imgdata, subject_group))\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNI(Dataset):\n",
    "    def __init__(self, datapath, csvpath, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            datapath (string): Directory with all the images.\n",
    "            csvpath (string): Path to CSV\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.table = create_df(csvpath)\n",
    "        self.imagepaths = create_paths(datapath)[:-1]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        #         Returns the length of the dataset\n",
    "        return len(self.table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #         Returns a tuple of the image and its group/label\n",
    "        imgsize = 224\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        imagepath = self.imagepaths[idx]\n",
    "        #         10th index for windows, 13th for linux\n",
    "        idpath = imagepath.split('/')[13]\n",
    "        img_id = idpath[idpath.find('_I') + 2:-4]\n",
    "        group = self.table.loc[self.table['Image Data ID'] == int(\n",
    "            img_id)][\"Group\"].reset_index(drop=True)[0]\n",
    "        group_to_id = {'CN': 0, 'MCI': 1, 'AD': 2}\n",
    "        group_id = group_to_id[group]\n",
    "\n",
    "        imgdata = nib.load(imagepath).get_fdata()\n",
    "        #         temporary, for only one slice\n",
    "        imgdata = cv2.resize(imgdata[100, :, :], (imgsize, imgsize))\n",
    "        imgdata = imgdata.reshape(1, imgsize, imgsize)\n",
    "        imgdata = torch.from_numpy(imgdata)\n",
    "        #         Apply transform to sample\n",
    "        if self.transform:\n",
    "            imgdata = self.transform(imgdata)\n",
    "\n",
    "        sample = (imgdata, torch.tensor(group_id))\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 5)\n",
    "        self.fc1 = nn.Linear(128*10*10, 224)\n",
    "        self.fc2 = nn.Linear(224,100)\n",
    "        self.fc3 = nn.Linear(100,3)\n",
    "        self.drop1 = nn.Dropout(p=0.1)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        x = torch.randn(224, 224).view(-1, 1, 224, 224)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.drop1(F.relu(self.conv1(x))))\n",
    "        x = self.pool(self.drop1(F.relu(self.conv2(x))))\n",
    "        x = self.pool(self.drop1(F.relu(self.conv3(x))))\n",
    "        x = self.pool(self.drop1(F.relu(self.conv4(x))))\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 128*10*10)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = r\"/media/swang/Windows/Users/swang/Downloads/ADNI1_Complete_1Yr_1.5T\"\n",
    "csvpath = r\"/media/swang/Windows/Users/swang/Downloads/ADNI1_Complete_1Yr_1.5T_7_08_2020.csv\"\n",
    "dataset = ADNI(datapath, csvpath)\n",
    "#  transform = transforms.Normalize(mean=[192.1213], std=[215.9763])\n",
    "\n",
    "lengths = [\n",
    "    int(len(dataset) * 0.8),\n",
    "    int(len(dataset) * 0.05),\n",
    "    int(len(dataset) * 0.15) + 1\n",
    "]\n",
    "\n",
    "\n",
    "trainset, valset, testset = random_split(dataset, lengths)\n",
    "image_datasets = {'train': trainset, 'val': valset, 'test': testset}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}  \n",
    "\n",
    "\n",
    "# train_loader = DataLoader(trainset, batch_size, shuffle=True, num_workers)\n",
    "# dev_loader = DataLoader(devset, batch_size, shuffle=True, num_workers)\n",
    "# test_loader = DataLoader(testset, batch_size, shuffle=True, num_workers)\n",
    "\n",
    "# for batch_x, batch_y in train_loader:\n",
    "#     batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "#     print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in tqdm(dataloaders['train'], total = dataset_sizes['train']//16):\n",
    "            batch_x, batch_y = data\n",
    "            batch_x, batch_y = batch_x.to(device, dtype=torch.float), batch_y.to(device)\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, Loss: {running_loss/(dataset_sizes['train']//16)}\")\n",
    "        print('---------------------------------------------------------')\n",
    "        running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, group in tqdm(dataloaders['val'], total = dataset_sizes['val']//16):\n",
    "            image, group = image.to(device, dtype=torch.float), group.to(device)\n",
    "            outputs = net(image)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += len(group)\n",
    "            correct += (predicted == group).sum().item()\n",
    "    #         print((predicted, group))  \n",
    "        print(f\"Accuracy = {100 * correct / total}\")\n",
    "\n",
    "# For finding optimal weights and preventing overfitting:\n",
    "#  - Set model to eval mode (and maybe to torch.no_grad()) with torch.no_grad():\n",
    "#  - Save the model (pytorch automatically pickles model and saves it I think look up how to do this)\n",
    "#  - Run the model on validation set and save its accuracy\n",
    "#  - If validation accuracy has been declining for like last 5 epochs or something just take the most accurate one and load weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7bad708ec94a4ab082f0d4904efbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Loss: 1.0723267223751336\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b503e021dee74346a718eb0635940c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, Loss: 1.0637481155102713\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ff455fc8464cb3921b17caa8c7bfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, Loss: 1.0552606906807214\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3710613a670e4b5a9b11e12bce39c50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4, Loss: 1.0573667219856329\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9724aa870e9545f8b1dd6378cf865eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5, Loss: 1.0599884108493203\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda04b0d1c8846e5a11b39b9255f181f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6, Loss: 1.0545184800499363\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198a048b820847cfa5470caee42c7f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7, Loss: 1.0512872866371221\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a9119f845d4e69bc9ffddbb52c5a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8, Loss: 1.0526386154325384\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736f56d5aa854d3ba8a8d4bd3ed90a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9, Loss: 1.0507566353730988\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9b6aa3eea547f3b17d560901f15c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10, Loss: 1.052500265732146\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af8f4928e9d4665ab6414a6d401575d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy = 42.98245614035088\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    net = Net().to(device)\n",
    "    train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 220, 220]             156\n",
      "           Dropout-2          [-1, 6, 220, 220]               0\n",
      "         MaxPool2d-3          [-1, 6, 110, 110]               0\n",
      "            Conv2d-4         [-1, 32, 106, 106]           4,832\n",
      "           Dropout-5         [-1, 32, 106, 106]               0\n",
      "         MaxPool2d-6           [-1, 32, 53, 53]               0\n",
      "            Conv2d-7           [-1, 64, 49, 49]          51,264\n",
      "           Dropout-8           [-1, 64, 49, 49]               0\n",
      "         MaxPool2d-9           [-1, 64, 24, 24]               0\n",
      "           Conv2d-10          [-1, 128, 20, 20]         204,928\n",
      "          Dropout-11          [-1, 128, 20, 20]               0\n",
      "        MaxPool2d-12          [-1, 128, 10, 10]               0\n",
      "           Linear-13                  [-1, 224]       2,867,424\n",
      "          Dropout-14                  [-1, 224]               0\n",
      "           Linear-15                  [-1, 100]          22,500\n",
      "          Dropout-16                  [-1, 100]               0\n",
      "           Linear-17                    [-1, 3]             303\n",
      "================================================================\n",
      "Total params: 3,151,407\n",
      "Trainable params: 3,151,407\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 14.67\n",
      "Params size (MB): 12.02\n",
      "Estimated Total Size (MB): 26.88\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net, (1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(1)\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(torch.argmax(net(dataset[i][0].reshape(1,1,224,224).to(device, dtype=torch.float))))\n",
    "    print(dataset[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874ca2c7757047ce85067140586bac6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy = 50.43478260869565\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for image, group in tqdm(dataloaders['test'], total = dataset_sizes['test']//16):\n",
    "            image, group = image.to(device, dtype=torch.float), group.to(device)\n",
    "            outputs = net(image)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += len(group)\n",
    "            correct += (predicted == group).sum().item()\n",
    "    #         print((predicted, group))  \n",
    "        print(f\"Accuracy = {100 * correct / total}\")\n",
    "\n",
    "test_accuracy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "for img in tqdm(dataset):\n",
    "    means.append(torch.mean(img[0]))\n",
    "    stds.append(torch.std(img[0]))\n",
    "\n",
    "mean = torch.mean(torch.tensor(means))\n",
    "std = torch.mean(torch.tensor(stds))\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 246,
   "position": {
    "height": "643.993px",
    "left": "1211.97px",
    "right": "20px",
    "top": "83.9757px",
    "width": "798.976px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
