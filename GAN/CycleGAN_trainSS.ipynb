{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from CycleGAN_utils import *\n",
    "from CycleGAN_models import *\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 51\n",
    "batch_size = 1\n",
    "lr=0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 50\n",
    "n_cpu = 8\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "channels = 1\n",
    "sample_interval = 100\n",
    "checkpoint_interval = 25\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"../datasets/SSdataset.pt\")\n",
    "\n",
    "dataset = [sample for sample in dataset if sample != None]\n",
    "\n",
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "        \n",
    "def process_gan(dataset, s):\n",
    "    output = []\n",
    "    dataset = [sample[0] for sample in dataset]\n",
    "    for sample in dataset:\n",
    "        sample = sample[s][0]\n",
    "        sample /= torch.max(sample)\n",
    "        output.append(torch.unsqueeze(sample, 0))\n",
    "    return output\n",
    "\n",
    "        \n",
    "NCgan1 = process_gan(NC, 0)\n",
    "NCgan2 = process_gan(NC, 1)\n",
    "NCgan3 = process_gan(NC, 2)\n",
    "\n",
    "ADgan1 = process_gan(AD, 0)\n",
    "ADgan2 = process_gan(AD, 1)\n",
    "ADgan3 = process_gan(AD, 2)\n",
    "\n",
    "gan1 = []\n",
    "for i in range(len(ADgan1)):\n",
    "    gan1.append({\"A\": NCgan1[i], \"B\": ADgan1[i]})\n",
    "\n",
    "gan2 = []\n",
    "for i in range(len(ADgan2)):\n",
    "    gan2.append({\"A\": NCgan2[i], \"B\": ADgan2[i]})\n",
    "    \n",
    "gan3 = []\n",
    "for i in range(len(ADgan3)):\n",
    "    gan3.append({\"A\": NCgan3[i], \"B\": ADgan3[i]})\n",
    "\n",
    "batch_size = 1\n",
    "dataloader1 = DataLoader(gan1, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader2 = DataLoader(gan2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader3 = DataLoader(gan3, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "        \n",
    "def sample_images(batches_done, dataloader):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=4, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=4, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=4, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=4, normalize=True)\n",
    "\n",
    "    image_grid = torch.stack((real_A, fake_B, real_B, fake_A), 0)\n",
    "    save_image(image_grid, \"ganimages/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n",
    "    \n",
    "def train_gan(dataloader, epoch):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "                \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "           \n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "            \n",
    "                print(\n",
    "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        n_epochs,\n",
    "                        i,\n",
    "                        len(dataloader),\n",
    "                        loss_D.item(),\n",
    "                        loss_G.item(),\n",
    "                        loss_GAN.item(),\n",
    "                        loss_cycle.item(),\n",
    "                        loss_identity.item(),\n",
    "                        time_left,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, dataloader)\n",
    "                \n",
    "            \n",
    "            G_losses.append(loss_G.item())\n",
    "            D_losses.append(loss_D.item())\n",
    "            \n",
    "            \n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(G_AB.state_dict(), \"ganmodels/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(G_BA.state_dict(), \"ganmodels/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_A.state_dict(), \"ganmodels/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_B.state_dict(), \"ganmodels/%s/D_B_%d.pth\" % (dataset_name, epoch))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"ganmodels/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"ganmodels/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"ganmodels/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"ganmodels/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "    \n",
    "epoch = 0\n",
    "dataset_name = 'CycleGANSS1'\n",
    "os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"ganmodels/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader1, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/51] [Batch 0/476] [D loss: 1.580269] [G loss: 8.021387, adv: 2.080192, cycle: 0.416175, identity: 0.355890] ETA: 4:37:32.626734\n",
      "[Epoch 0/51] [Batch 100/476] [D loss: 0.371254] [G loss: 1.103347, adv: 0.450504, cycle: 0.042420, identity: 0.045728] ETA: 2:03:48.883514\n",
      "[Epoch 0/51] [Batch 200/476] [D loss: 0.258947] [G loss: 0.804562, adv: 0.321134, cycle: 0.032208, identity: 0.032270] ETA: 2:03:23.671474\n",
      "[Epoch 0/51] [Batch 300/476] [D loss: 0.262186] [G loss: 0.580356, adv: 0.218900, cycle: 0.024181, identity: 0.023929] ETA: 2:03:03.461123\n",
      "[Epoch 0/51] [Batch 400/476] [D loss: 0.255153] [G loss: 0.611283, adv: 0.285652, cycle: 0.021744, identity: 0.021639] ETA: 2:02:58.185303\n",
      "[Epoch 1/51] [Batch 0/476] [D loss: 0.276027] [G loss: 0.753901, adv: 0.286148, cycle: 0.031233, identity: 0.031085] ETA: 3:50:00.911331\n",
      "[Epoch 1/51] [Batch 100/476] [D loss: 0.279211] [G loss: 0.696355, adv: 0.324402, cycle: 0.024967, identity: 0.024457] ETA: 2:03:11.541696\n",
      "[Epoch 1/51] [Batch 200/476] [D loss: 0.305796] [G loss: 0.636466, adv: 0.329079, cycle: 0.020130, identity: 0.021218] ETA: 2:02:25.955086\n",
      "[Epoch 1/51] [Batch 300/476] [D loss: 0.363573] [G loss: 0.844851, adv: 0.386830, cycle: 0.031381, identity: 0.028843] ETA: 2:01:53.298583\n",
      "[Epoch 1/51] [Batch 400/476] [D loss: 0.270949] [G loss: 0.577253, adv: 0.274507, cycle: 0.020558, identity: 0.019434] ETA: 2:02:24.305849\n",
      "[Epoch 2/51] [Batch 0/476] [D loss: 0.241756] [G loss: 0.663946, adv: 0.244638, cycle: 0.029254, identity: 0.025353] ETA: 3:20:36.541610\n",
      "[Epoch 2/51] [Batch 100/476] [D loss: 0.255550] [G loss: 0.550033, adv: 0.271454, cycle: 0.018976, identity: 0.017765] ETA: 2:02:12.825798\n",
      "[Epoch 2/51] [Batch 200/476] [D loss: 0.223099] [G loss: 0.463840, adv: 0.263620, cycle: 0.013511, identity: 0.013022] ETA: 2:02:14.429838\n",
      "[Epoch 2/51] [Batch 300/476] [D loss: 0.253143] [G loss: 0.590959, adv: 0.308842, cycle: 0.019130, identity: 0.018164] ETA: 1:59:47.649761\n",
      "[Epoch 2/51] [Batch 400/476] [D loss: 0.243227] [G loss: 0.520920, adv: 0.266134, cycle: 0.018162, identity: 0.014632] ETA: 2:01:18.039227\n",
      "[Epoch 3/51] [Batch 0/476] [D loss: 0.254965] [G loss: 0.537609, adv: 0.289373, cycle: 0.016382, identity: 0.016883] ETA: 3:16:35.947632\n",
      "[Epoch 3/51] [Batch 100/476] [D loss: 0.253238] [G loss: 0.584122, adv: 0.272853, cycle: 0.021315, identity: 0.019623] ETA: 2:00:18.164614\n",
      "[Epoch 3/51] [Batch 200/476] [D loss: 0.252978] [G loss: 0.720218, adv: 0.234295, cycle: 0.033278, identity: 0.030629] ETA: 1:58:19.698183\n",
      "[Epoch 3/51] [Batch 300/476] [D loss: 0.254244] [G loss: 0.495132, adv: 0.277322, cycle: 0.014524, identity: 0.014514] ETA: 1:57:06.520586\n",
      "[Epoch 3/51] [Batch 400/476] [D loss: 0.243442] [G loss: 0.593719, adv: 0.245129, cycle: 0.023950, identity: 0.021819] ETA: 1:59:52.339191\n",
      "[Epoch 4/51] [Batch 0/476] [D loss: 0.253229] [G loss: 0.668402, adv: 0.284543, cycle: 0.026093, identity: 0.024586] ETA: 3:14:16.028642\n",
      "[Epoch 4/51] [Batch 100/476] [D loss: 0.248402] [G loss: 0.441725, adv: 0.241492, cycle: 0.013280, identity: 0.013487] ETA: 1:58:20.493530\n",
      "[Epoch 4/51] [Batch 200/476] [D loss: 0.263849] [G loss: 0.474499, adv: 0.226591, cycle: 0.016715, identity: 0.016152] ETA: 1:57:44.378463\n",
      "[Epoch 4/51] [Batch 300/476] [D loss: 0.255517] [G loss: 0.436419, adv: 0.191607, cycle: 0.016446, identity: 0.016071] ETA: 1:55:41.798672\n",
      "[Epoch 4/51] [Batch 400/476] [D loss: 0.260257] [G loss: 0.485646, adv: 0.237608, cycle: 0.016594, identity: 0.016420] ETA: 1:56:15.085762\n",
      "[Epoch 5/51] [Batch 0/476] [D loss: 0.256057] [G loss: 0.613357, adv: 0.243401, cycle: 0.024945, identity: 0.024102] ETA: 3:07:47.082096\n",
      "[Epoch 5/51] [Batch 100/476] [D loss: 0.249708] [G loss: 0.514086, adv: 0.287976, cycle: 0.015105, identity: 0.015012] ETA: 1:55:14.957974\n",
      "[Epoch 5/51] [Batch 200/476] [D loss: 0.256907] [G loss: 0.490182, adv: 0.264046, cycle: 0.014827, identity: 0.015574] ETA: 1:55:31.964447\n",
      "[Epoch 5/51] [Batch 300/476] [D loss: 0.284715] [G loss: 0.496513, adv: 0.311811, cycle: 0.012253, identity: 0.012435] ETA: 1:54:46.838017\n",
      "[Epoch 5/51] [Batch 400/476] [D loss: 0.290335] [G loss: 0.796680, adv: 0.507643, cycle: 0.018410, identity: 0.020988] ETA: 1:53:40.323793\n",
      "[Epoch 6/51] [Batch 0/476] [D loss: 0.255169] [G loss: 0.542979, adv: 0.279825, cycle: 0.017841, identity: 0.016949] ETA: 3:02:58.915401\n",
      "[Epoch 6/51] [Batch 100/476] [D loss: 0.245340] [G loss: 0.811888, adv: 0.203798, cycle: 0.040473, identity: 0.040673] ETA: 1:52:14.720459\n",
      "[Epoch 6/51] [Batch 200/476] [D loss: 0.290154] [G loss: 0.749043, adv: 0.381806, cycle: 0.025215, identity: 0.023018] ETA: 1:52:57.699885\n",
      "[Epoch 6/51] [Batch 300/476] [D loss: 0.234753] [G loss: 0.734879, adv: 0.239231, cycle: 0.032614, identity: 0.033902] ETA: 1:51:59.923096\n",
      "[Epoch 6/51] [Batch 400/476] [D loss: 0.263636] [G loss: 0.623964, adv: 0.416949, cycle: 0.013605, identity: 0.014193] ETA: 1:51:20.853553\n",
      "[Epoch 7/51] [Batch 0/476] [D loss: 0.248831] [G loss: 0.456168, adv: 0.206878, cycle: 0.016671, identity: 0.016515] ETA: 3:02:01.943783\n",
      "[Epoch 7/51] [Batch 100/476] [D loss: 0.236860] [G loss: 0.490860, adv: 0.238064, cycle: 0.017592, identity: 0.015375] ETA: 1:50:20.551606\n",
      "[Epoch 7/51] [Batch 200/476] [D loss: 0.225233] [G loss: 0.526307, adv: 0.306592, cycle: 0.014596, identity: 0.014752] ETA: 1:49:49.001888\n",
      "[Epoch 7/51] [Batch 300/476] [D loss: 0.260926] [G loss: 0.519262, adv: 0.213708, cycle: 0.020468, identity: 0.020174] ETA: 1:48:49.444434\n",
      "[Epoch 7/51] [Batch 400/476] [D loss: 0.241453] [G loss: 0.438509, adv: 0.233151, cycle: 0.013221, identity: 0.014629] ETA: 1:48:26.470551\n",
      "[Epoch 8/51] [Batch 0/476] [D loss: 0.224944] [G loss: 0.602958, adv: 0.252586, cycle: 0.022492, identity: 0.025090] ETA: 2:58:58.338105\n",
      "[Epoch 8/51] [Batch 100/476] [D loss: 0.508801] [G loss: 1.362857, adv: 0.886766, cycle: 0.029719, identity: 0.035781] ETA: 1:48:31.363144\n",
      "[Epoch 8/51] [Batch 200/476] [D loss: 0.254661] [G loss: 0.599342, adv: 0.378954, cycle: 0.014543, identity: 0.014991] ETA: 1:48:12.282208\n",
      "[Epoch 8/51] [Batch 300/476] [D loss: 0.216514] [G loss: 0.814455, adv: 0.369135, cycle: 0.029731, identity: 0.029602] ETA: 1:47:08.524035\n",
      "[Epoch 8/51] [Batch 400/476] [D loss: 0.161545] [G loss: 1.002364, adv: 0.559847, cycle: 0.031515, identity: 0.025473] ETA: 1:46:48.735023\n",
      "[Epoch 9/51] [Batch 0/476] [D loss: 0.233089] [G loss: 1.970299, adv: 1.277992, cycle: 0.045579, identity: 0.047304] ETA: 2:54:54.595991\n",
      "[Epoch 9/51] [Batch 100/476] [D loss: 0.104864] [G loss: 0.837765, adv: 0.489276, cycle: 0.023870, identity: 0.021957] ETA: 1:45:44.665986\n",
      "[Epoch 9/51] [Batch 200/476] [D loss: 0.208990] [G loss: 0.752582, adv: 0.289656, cycle: 0.031545, identity: 0.029495] ETA: 1:44:53.961361\n",
      "[Epoch 9/51] [Batch 300/476] [D loss: 0.177983] [G loss: 0.895985, adv: 0.478392, cycle: 0.028199, identity: 0.027120] ETA: 1:44:38.682318\n",
      "[Epoch 9/51] [Batch 400/476] [D loss: 0.042720] [G loss: 1.131212, adv: 0.680279, cycle: 0.030118, identity: 0.029951] ETA: 1:44:07.713421\n",
      "[Epoch 10/51] [Batch 0/476] [D loss: 0.142548] [G loss: 1.173283, adv: 0.604964, cycle: 0.040379, identity: 0.032907] ETA: 2:48:00.800279\n",
      "[Epoch 10/51] [Batch 100/476] [D loss: 0.191715] [G loss: 0.724612, adv: 0.293145, cycle: 0.030809, identity: 0.024676] ETA: 1:42:55.858795\n",
      "[Epoch 10/51] [Batch 200/476] [D loss: 0.173345] [G loss: 1.273098, adv: 0.656163, cycle: 0.043394, identity: 0.036600] ETA: 1:42:49.025210\n",
      "[Epoch 10/51] [Batch 300/476] [D loss: 0.177528] [G loss: 0.824054, adv: 0.340172, cycle: 0.032786, identity: 0.031205] ETA: 1:42:17.417690\n",
      "[Epoch 10/51] [Batch 400/476] [D loss: 0.251628] [G loss: 0.603969, adv: 0.227276, cycle: 0.025692, identity: 0.023954] ETA: 1:41:44.813178\n",
      "[Epoch 11/51] [Batch 0/476] [D loss: 0.221278] [G loss: 1.151056, adv: 0.767133, cycle: 0.026567, identity: 0.023650] ETA: 2:42:13.496742\n",
      "[Epoch 11/51] [Batch 100/476] [D loss: 0.108330] [G loss: 0.956631, adv: 0.550178, cycle: 0.028103, identity: 0.025085] ETA: 1:40:38.780437\n",
      "[Epoch 11/51] [Batch 200/476] [D loss: 0.080500] [G loss: 1.167775, adv: 0.679259, cycle: 0.034174, identity: 0.029354] ETA: 1:39:49.320259\n",
      "[Epoch 11/51] [Batch 300/476] [D loss: 0.043140] [G loss: 1.552414, adv: 0.760126, cycle: 0.056528, identity: 0.045403] ETA: 1:39:51.115499\n",
      "[Epoch 11/51] [Batch 400/476] [D loss: 0.043898] [G loss: 1.438366, adv: 0.924712, cycle: 0.036565, identity: 0.029600] ETA: 1:38:45.832691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/51] [Batch 0/476] [D loss: 0.093608] [G loss: 0.935382, adv: 0.490386, cycle: 0.032354, identity: 0.024291] ETA: 2:38:37.494315\n",
      "[Epoch 12/51] [Batch 100/476] [D loss: 0.062582] [G loss: 1.205003, adv: 0.525138, cycle: 0.049649, identity: 0.036674] ETA: 1:37:49.405190\n",
      "[Epoch 12/51] [Batch 200/476] [D loss: 0.137043] [G loss: 1.948428, adv: 1.335271, cycle: 0.044096, identity: 0.034440] ETA: 1:37:46.211617\n",
      "[Epoch 12/51] [Batch 300/476] [D loss: 0.116116] [G loss: 1.182399, adv: 0.736453, cycle: 0.030536, identity: 0.028118] ETA: 1:37:31.772530\n",
      "[Epoch 12/51] [Batch 400/476] [D loss: 0.186018] [G loss: 1.363213, adv: 0.679356, cycle: 0.047107, identity: 0.042557] ETA: 1:36:19.782493\n",
      "[Epoch 13/51] [Batch 0/476] [D loss: 0.055949] [G loss: 1.760373, adv: 1.237137, cycle: 0.036051, identity: 0.032546] ETA: 2:38:34.788666\n",
      "[Epoch 13/51] [Batch 100/476] [D loss: 0.050663] [G loss: 1.531358, adv: 0.882743, cycle: 0.043808, identity: 0.042107] ETA: 1:35:45.874560\n",
      "[Epoch 13/51] [Batch 200/476] [D loss: 0.015590] [G loss: 1.248012, adv: 0.844837, cycle: 0.026566, identity: 0.027504] ETA: 1:34:50.505020\n",
      "[Epoch 13/51] [Batch 300/476] [D loss: 0.045804] [G loss: 1.154014, adv: 0.660115, cycle: 0.033808, identity: 0.031164] ETA: 1:35:09.657136\n",
      "[Epoch 13/51] [Batch 400/476] [D loss: 0.053456] [G loss: 1.224716, adv: 0.580929, cycle: 0.043049, identity: 0.042660] ETA: 1:33:47.391581\n",
      "[Epoch 14/51] [Batch 0/476] [D loss: 0.010794] [G loss: 1.273033, adv: 0.814074, cycle: 0.032095, identity: 0.027601] ETA: 2:33:42.316832\n",
      "[Epoch 14/51] [Batch 100/476] [D loss: 0.074417] [G loss: 1.579060, adv: 1.094721, cycle: 0.034206, identity: 0.028456] ETA: 1:31:45.154131\n",
      "[Epoch 14/51] [Batch 200/476] [D loss: 0.013550] [G loss: 1.534951, adv: 1.060243, cycle: 0.034448, identity: 0.026045] ETA: 1:32:41.389904\n",
      "[Epoch 14/51] [Batch 300/476] [D loss: 0.049084] [G loss: 0.837337, adv: 0.292492, cycle: 0.036503, identity: 0.035964] ETA: 1:32:08.488213\n",
      "[Epoch 14/51] [Batch 400/476] [D loss: 0.150802] [G loss: 0.980493, adv: 0.497069, cycle: 0.032857, identity: 0.030971] ETA: 1:31:03.662945\n",
      "[Epoch 15/51] [Batch 0/476] [D loss: 0.190468] [G loss: 0.600918, adv: 0.198510, cycle: 0.026810, identity: 0.026862] ETA: 2:29:27.226788\n",
      "[Epoch 15/51] [Batch 100/476] [D loss: 0.282555] [G loss: 1.336394, adv: 0.610482, cycle: 0.049209, identity: 0.046764] ETA: 1:30:44.849553\n",
      "[Epoch 15/51] [Batch 200/476] [D loss: 0.132163] [G loss: 1.111499, adv: 0.660093, cycle: 0.031051, identity: 0.028180] ETA: 1:30:15.145870\n",
      "[Epoch 15/51] [Batch 300/476] [D loss: 0.111634] [G loss: 1.108343, adv: 0.562646, cycle: 0.035695, identity: 0.037750] ETA: 1:29:28.247589\n",
      "[Epoch 15/51] [Batch 400/476] [D loss: 0.129630] [G loss: 0.966111, adv: 0.636579, cycle: 0.021557, identity: 0.022793] ETA: 1:28:38.386330\n",
      "[Epoch 16/51] [Batch 0/476] [D loss: 0.135339] [G loss: 1.113015, adv: 0.680168, cycle: 0.029314, identity: 0.027942] ETA: 2:25:34.458323\n",
      "[Epoch 16/51] [Batch 100/476] [D loss: 0.249912] [G loss: 0.736798, adv: 0.357748, cycle: 0.026230, identity: 0.023351] ETA: 1:27:53.140697\n",
      "[Epoch 16/51] [Batch 200/476] [D loss: 0.250318] [G loss: 0.484521, adv: 0.168766, cycle: 0.019672, identity: 0.023807] ETA: 1:27:28.267741\n",
      "[Epoch 16/51] [Batch 300/476] [D loss: 0.169855] [G loss: 0.830382, adv: 0.579910, cycle: 0.017557, identity: 0.014980] ETA: 1:26:38.861589\n",
      "[Epoch 16/51] [Batch 400/476] [D loss: 0.236414] [G loss: 0.798907, adv: 0.339239, cycle: 0.031246, identity: 0.029442] ETA: 1:26:15.383692\n",
      "[Epoch 17/51] [Batch 0/476] [D loss: 0.141971] [G loss: 0.710116, adv: 0.407022, cycle: 0.021024, identity: 0.018572] ETA: 2:22:42.363089\n",
      "[Epoch 17/51] [Batch 100/476] [D loss: 0.200346] [G loss: 0.875016, adv: 0.524197, cycle: 0.024099, identity: 0.021966] ETA: 1:25:18.908446\n",
      "[Epoch 17/51] [Batch 200/476] [D loss: 0.171288] [G loss: 0.488913, adv: 0.230241, cycle: 0.017680, identity: 0.016375] ETA: 1:24:36.438560\n",
      "[Epoch 17/51] [Batch 300/476] [D loss: 0.287095] [G loss: 0.565963, adv: 0.289121, cycle: 0.018046, identity: 0.019276] ETA: 1:24:14.377671\n",
      "[Epoch 17/51] [Batch 400/476] [D loss: 0.265442] [G loss: 0.472904, adv: 0.303688, cycle: 0.011170, identity: 0.011502] ETA: 1:23:30.729378\n",
      "[Epoch 18/51] [Batch 0/476] [D loss: 0.254909] [G loss: 0.409076, adv: 0.204679, cycle: 0.013691, identity: 0.013498] ETA: 2:13:51.861294\n",
      "[Epoch 18/51] [Batch 100/476] [D loss: 0.271484] [G loss: 0.600706, adv: 0.332458, cycle: 0.018078, identity: 0.017493] ETA: 1:23:09.940903\n",
      "[Epoch 18/51] [Batch 200/476] [D loss: 0.249073] [G loss: 0.570586, adv: 0.268316, cycle: 0.020615, identity: 0.019224] ETA: 1:22:15.823099\n",
      "[Epoch 18/51] [Batch 300/476] [D loss: 0.217068] [G loss: 0.581819, adv: 0.239393, cycle: 0.022344, identity: 0.023797] ETA: 1:21:19.276165\n",
      "[Epoch 18/51] [Batch 400/476] [D loss: 0.296162] [G loss: 0.537146, adv: 0.295988, cycle: 0.015340, identity: 0.017552] ETA: 1:21:26.916398\n",
      "[Epoch 19/51] [Batch 0/476] [D loss: 0.206883] [G loss: 0.562049, adv: 0.403994, cycle: 0.010202, identity: 0.011208] ETA: 2:14:28.213074\n",
      "[Epoch 19/51] [Batch 100/476] [D loss: 0.219093] [G loss: 0.704075, adv: 0.413343, cycle: 0.019862, identity: 0.018422] ETA: 1:20:15.011301\n",
      "[Epoch 19/51] [Batch 200/476] [D loss: 0.215720] [G loss: 0.531927, adv: 0.366694, cycle: 0.010734, identity: 0.011579] ETA: 1:19:22.641113\n",
      "[Epoch 19/51] [Batch 300/476] [D loss: 0.301945] [G loss: 0.438762, adv: 0.217324, cycle: 0.014765, identity: 0.014758] ETA: 1:18:23.580854\n",
      "[Epoch 19/51] [Batch 400/476] [D loss: 0.228059] [G loss: 0.589326, adv: 0.286990, cycle: 0.019735, identity: 0.020997] ETA: 1:17:53.640324\n",
      "[Epoch 20/51] [Batch 0/476] [D loss: 0.259209] [G loss: 0.557885, adv: 0.324642, cycle: 0.015960, identity: 0.014728] ETA: 2:08:30.527724\n",
      "[Epoch 20/51] [Batch 100/476] [D loss: 0.232565] [G loss: 0.631758, adv: 0.298405, cycle: 0.022404, identity: 0.021862] ETA: 1:17:53.765045\n",
      "[Epoch 20/51] [Batch 200/476] [D loss: 0.231633] [G loss: 0.484237, adv: 0.285180, cycle: 0.013518, identity: 0.012775] ETA: 1:17:25.998133\n",
      "[Epoch 20/51] [Batch 300/476] [D loss: 0.250432] [G loss: 0.690217, adv: 0.297093, cycle: 0.026101, identity: 0.026424] ETA: 1:16:46.711250\n",
      "[Epoch 20/51] [Batch 400/476] [D loss: 0.231995] [G loss: 0.601176, adv: 0.420931, cycle: 0.011699, identity: 0.012652] ETA: 1:16:07.704298\n",
      "[Epoch 21/51] [Batch 0/476] [D loss: 0.257988] [G loss: 0.375612, adv: 0.164092, cycle: 0.014278, identity: 0.013747] ETA: 2:05:50.498114\n",
      "[Epoch 21/51] [Batch 100/476] [D loss: 0.235473] [G loss: 0.700333, adv: 0.370624, cycle: 0.022631, identity: 0.020680] ETA: 1:14:58.037219\n",
      "[Epoch 21/51] [Batch 200/476] [D loss: 0.249496] [G loss: 0.564928, adv: 0.362548, cycle: 0.012987, identity: 0.014502] ETA: 1:14:54.517822\n",
      "[Epoch 21/51] [Batch 300/476] [D loss: 0.257102] [G loss: 0.564669, adv: 0.251166, cycle: 0.021020, identity: 0.020660] ETA: 1:13:44.632616\n",
      "[Epoch 21/51] [Batch 400/476] [D loss: 0.215833] [G loss: 0.574230, adv: 0.355848, cycle: 0.014225, identity: 0.015227] ETA: 1:13:53.223362\n",
      "[Epoch 22/51] [Batch 0/476] [D loss: 0.200860] [G loss: 0.555520, adv: 0.339541, cycle: 0.014766, identity: 0.013663] ETA: 2:00:19.814557\n",
      "[Epoch 22/51] [Batch 100/476] [D loss: 0.193094] [G loss: 0.620032, adv: 0.346340, cycle: 0.018824, identity: 0.017090] ETA: 1:12:36.069529\n",
      "[Epoch 22/51] [Batch 200/476] [D loss: 0.292481] [G loss: 0.802768, adv: 0.268312, cycle: 0.035054, identity: 0.036783] ETA: 1:11:59.232635\n",
      "[Epoch 22/51] [Batch 300/476] [D loss: 0.190407] [G loss: 0.547448, adv: 0.348206, cycle: 0.013414, identity: 0.013020] ETA: 1:10:55.760147\n",
      "[Epoch 22/51] [Batch 400/476] [D loss: 0.236022] [G loss: 0.680844, adv: 0.397365, cycle: 0.019295, identity: 0.018105] ETA: 1:11:17.183144\n",
      "[Epoch 23/51] [Batch 0/476] [D loss: 0.241342] [G loss: 0.577001, adv: 0.349929, cycle: 0.014907, identity: 0.015601] ETA: 1:54:37.912560\n",
      "[Epoch 23/51] [Batch 100/476] [D loss: 0.230256] [G loss: 0.499727, adv: 0.296427, cycle: 0.013707, identity: 0.013246] ETA: 1:09:30.444474\n",
      "[Epoch 23/51] [Batch 200/476] [D loss: 0.194418] [G loss: 0.565153, adv: 0.355761, cycle: 0.014201, identity: 0.013477] ETA: 1:09:29.258898\n",
      "[Epoch 23/51] [Batch 300/476] [D loss: 0.237201] [G loss: 0.667206, adv: 0.334998, cycle: 0.021597, identity: 0.023248] ETA: 1:09:02.296220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/51] [Batch 400/476] [D loss: 0.235582] [G loss: 0.560759, adv: 0.377396, cycle: 0.012043, identity: 0.012587] ETA: 1:09:25.396210\n",
      "[Epoch 24/51] [Batch 0/476] [D loss: 0.200885] [G loss: 0.526144, adv: 0.316353, cycle: 0.013837, identity: 0.014285] ETA: 1:48:49.265107\n",
      "[Epoch 24/51] [Batch 100/476] [D loss: 0.226464] [G loss: 0.662994, adv: 0.429286, cycle: 0.015225, identity: 0.016292] ETA: 1:07:36.727081\n",
      "[Epoch 24/51] [Batch 200/476] [D loss: 0.157333] [G loss: 0.813688, adv: 0.449056, cycle: 0.023575, identity: 0.025777] ETA: 1:07:05.614426\n",
      "[Epoch 24/51] [Batch 300/476] [D loss: 0.203987] [G loss: 0.536963, adv: 0.240686, cycle: 0.019394, identity: 0.020467] ETA: 1:06:44.985861\n",
      "[Epoch 24/51] [Batch 400/476] [D loss: 0.228026] [G loss: 0.719784, adv: 0.411275, cycle: 0.020730, identity: 0.020241] ETA: 1:05:50.937486\n",
      "[Epoch 25/51] [Batch 0/476] [D loss: 0.134003] [G loss: 0.840484, adv: 0.513192, cycle: 0.021560, identity: 0.022339] ETA: 1:48:23.969368\n",
      "[Epoch 25/51] [Batch 100/476] [D loss: 0.174936] [G loss: 0.554471, adv: 0.370127, cycle: 0.012029, identity: 0.012810] ETA: 1:05:04.333838\n",
      "[Epoch 25/51] [Batch 200/476] [D loss: 0.227358] [G loss: 0.740296, adv: 0.366636, cycle: 0.025802, identity: 0.023127] ETA: 1:05:08.230133\n",
      "[Epoch 25/51] [Batch 300/476] [D loss: 0.278206] [G loss: 0.717025, adv: 0.334488, cycle: 0.025339, identity: 0.025829] ETA: 1:03:54.937542\n",
      "[Epoch 25/51] [Batch 400/476] [D loss: 0.193442] [G loss: 0.937576, adv: 0.560941, cycle: 0.025094, identity: 0.025139] ETA: 1:03:49.055592\n",
      "[Epoch 26/51] [Batch 0/476] [D loss: 0.242132] [G loss: 0.550197, adv: 0.338751, cycle: 0.013627, identity: 0.015035] ETA: 1:56:15.331712\n",
      "[Epoch 26/51] [Batch 100/476] [D loss: 0.195812] [G loss: 0.838339, adv: 0.513714, cycle: 0.021402, identity: 0.022121] ETA: 1:02:45.165043\n",
      "[Epoch 26/51] [Batch 200/476] [D loss: 0.190127] [G loss: 0.660211, adv: 0.302698, cycle: 0.024228, identity: 0.023048] ETA: 1:02:10.405998\n",
      "[Epoch 26/51] [Batch 300/476] [D loss: 0.223817] [G loss: 0.751626, adv: 0.376153, cycle: 0.025388, identity: 0.024319] ETA: 1:01:38.500061\n",
      "[Epoch 26/51] [Batch 400/476] [D loss: 0.264410] [G loss: 0.546465, adv: 0.237840, cycle: 0.020112, identity: 0.021501] ETA: 1:01:15.494432\n",
      "[Epoch 27/51] [Batch 0/476] [D loss: 0.178458] [G loss: 0.627145, adv: 0.329851, cycle: 0.019446, identity: 0.020567] ETA: 1:39:09.560577\n",
      "[Epoch 27/51] [Batch 100/476] [D loss: 0.194605] [G loss: 0.597104, adv: 0.282748, cycle: 0.020717, identity: 0.021438] ETA: 1:00:00.590110\n",
      "[Epoch 27/51] [Batch 200/476] [D loss: 0.229058] [G loss: 0.591484, adv: 0.300791, cycle: 0.019231, identity: 0.019677] ETA: 0:59:40.223251\n",
      "[Epoch 27/51] [Batch 300/476] [D loss: 0.208268] [G loss: 0.714925, adv: 0.431542, cycle: 0.019295, identity: 0.018086] ETA: 0:59:05.598896\n",
      "[Epoch 27/51] [Batch 400/476] [D loss: 0.274107] [G loss: 0.774934, adv: 0.497489, cycle: 0.018086, identity: 0.019316] ETA: 0:57:14.710102\n",
      "[Epoch 28/51] [Batch 0/476] [D loss: 0.119577] [G loss: 0.672695, adv: 0.435358, cycle: 0.015709, identity: 0.016049] ETA: 1:36:03.725102\n",
      "[Epoch 28/51] [Batch 100/476] [D loss: 0.183120] [G loss: 0.758962, adv: 0.290474, cycle: 0.031327, identity: 0.031044] ETA: 0:57:39.022316\n",
      "[Epoch 28/51] [Batch 200/476] [D loss: 0.151751] [G loss: 0.886951, adv: 0.624697, cycle: 0.017589, identity: 0.017273] ETA: 0:57:14.167611\n",
      "[Epoch 28/51] [Batch 300/476] [D loss: 0.166229] [G loss: 0.686685, adv: 0.445527, cycle: 0.016474, identity: 0.015283] ETA: 0:56:39.007029\n",
      "[Epoch 28/51] [Batch 400/476] [D loss: 0.169062] [G loss: 1.091381, adv: 0.782543, cycle: 0.020670, identity: 0.020427] ETA: 0:55:56.480398\n",
      "[Epoch 29/51] [Batch 0/476] [D loss: 0.185511] [G loss: 0.856897, adv: 0.447951, cycle: 0.026687, identity: 0.028415] ETA: 1:29:45.895540\n",
      "[Epoch 29/51] [Batch 100/476] [D loss: 0.101832] [G loss: 0.679920, adv: 0.425533, cycle: 0.017361, identity: 0.016156] ETA: 0:54:37.007809\n",
      "[Epoch 29/51] [Batch 200/476] [D loss: 0.102848] [G loss: 0.650442, adv: 0.371666, cycle: 0.018682, identity: 0.018390] ETA: 0:54:30.951599\n",
      "[Epoch 29/51] [Batch 300/476] [D loss: 0.152100] [G loss: 0.907753, adv: 0.629691, cycle: 0.018470, identity: 0.018672] ETA: 0:53:32.731815\n",
      "[Epoch 29/51] [Batch 400/476] [D loss: 0.141692] [G loss: 0.710424, adv: 0.384065, cycle: 0.021960, identity: 0.021353] ETA: 0:52:18.569372\n",
      "[Epoch 30/51] [Batch 0/476] [D loss: 0.153634] [G loss: 0.796371, adv: 0.491341, cycle: 0.020739, identity: 0.019528] ETA: 1:26:43.646716\n",
      "[Epoch 30/51] [Batch 100/476] [D loss: 0.113969] [G loss: 1.017549, adv: 0.680926, cycle: 0.022675, identity: 0.021974] ETA: 0:52:35.200796\n",
      "[Epoch 30/51] [Batch 200/476] [D loss: 0.203150] [G loss: 0.881980, adv: 0.534154, cycle: 0.023424, identity: 0.022717] ETA: 0:52:01.810770\n",
      "[Epoch 30/51] [Batch 300/476] [D loss: 0.144970] [G loss: 0.929633, adv: 0.594264, cycle: 0.022593, identity: 0.021889] ETA: 0:51:28.673424\n",
      "[Epoch 30/51] [Batch 400/476] [D loss: 0.265708] [G loss: 0.546894, adv: 0.277425, cycle: 0.017632, identity: 0.018631] ETA: 0:50:50.739437\n",
      "[Epoch 31/51] [Batch 0/476] [D loss: 0.120152] [G loss: 0.867028, adv: 0.572454, cycle: 0.019019, identity: 0.020877] ETA: 1:22:14.902000\n",
      "[Epoch 31/51] [Batch 100/476] [D loss: 0.260396] [G loss: 0.631323, adv: 0.303029, cycle: 0.021972, identity: 0.021715] ETA: 0:50:18.513865\n",
      "[Epoch 31/51] [Batch 200/476] [D loss: 0.211989] [G loss: 0.520743, adv: 0.228002, cycle: 0.019358, identity: 0.019832] ETA: 0:49:36.226492\n",
      "[Epoch 31/51] [Batch 300/476] [D loss: 0.289569] [G loss: 0.784096, adv: 0.467339, cycle: 0.021282, identity: 0.020788] ETA: 0:49:03.358488\n",
      "[Epoch 31/51] [Batch 400/476] [D loss: 0.125999] [G loss: 1.233646, adv: 0.713266, cycle: 0.035167, identity: 0.033741] ETA: 0:48:31.584892\n",
      "[Epoch 32/51] [Batch 0/476] [D loss: 0.118249] [G loss: 1.005006, adv: 0.490546, cycle: 0.033107, identity: 0.036677] ETA: 1:20:24.816196\n",
      "[Epoch 32/51] [Batch 100/476] [D loss: 0.181175] [G loss: 0.756102, adv: 0.447259, cycle: 0.020707, identity: 0.020356] ETA: 0:47:48.043770\n",
      "[Epoch 32/51] [Batch 200/476] [D loss: 0.122803] [G loss: 0.707835, adv: 0.387435, cycle: 0.020156, identity: 0.023768] ETA: 0:47:01.423714\n",
      "[Epoch 32/51] [Batch 300/476] [D loss: 0.227788] [G loss: 0.637857, adv: 0.342124, cycle: 0.018793, identity: 0.021560] ETA: 0:45:57.299973\n",
      "[Epoch 32/51] [Batch 400/476] [D loss: 0.206664] [G loss: 0.674140, adv: 0.301348, cycle: 0.024088, identity: 0.026382] ETA: 0:45:57.244387\n",
      "[Epoch 33/51] [Batch 0/476] [D loss: 0.120700] [G loss: 0.689905, adv: 0.427684, cycle: 0.017149, identity: 0.018146] ETA: 1:12:32.682026\n",
      "[Epoch 33/51] [Batch 100/476] [D loss: 0.147312] [G loss: 0.667692, adv: 0.403011, cycle: 0.018020, identity: 0.016896] ETA: 0:44:51.148952\n",
      "[Epoch 33/51] [Batch 200/476] [D loss: 0.141878] [G loss: 0.887043, adv: 0.430415, cycle: 0.030035, identity: 0.031256] ETA: 0:43:38.838547\n",
      "[Epoch 33/51] [Batch 300/476] [D loss: 0.181051] [G loss: 0.774932, adv: 0.429957, cycle: 0.022844, identity: 0.023307] ETA: 0:43:57.748309\n",
      "[Epoch 33/51] [Batch 400/476] [D loss: 0.089046] [G loss: 1.098176, adv: 0.687076, cycle: 0.026864, identity: 0.028492] ETA: 0:43:09.210493\n",
      "[Epoch 34/51] [Batch 0/476] [D loss: 0.093473] [G loss: 1.069166, adv: 0.661155, cycle: 0.026178, identity: 0.029246] ETA: 1:10:35.633099\n",
      "[Epoch 34/51] [Batch 100/476] [D loss: 0.120092] [G loss: 0.812467, adv: 0.490705, cycle: 0.021479, identity: 0.021394] ETA: 0:42:32.710161\n",
      "[Epoch 34/51] [Batch 200/476] [D loss: 0.151007] [G loss: 1.065741, adv: 0.747530, cycle: 0.020589, identity: 0.022463] ETA: 0:41:48.354550\n",
      "[Epoch 34/51] [Batch 300/476] [D loss: 0.236064] [G loss: 0.814140, adv: 0.508480, cycle: 0.019889, identity: 0.021354] ETA: 0:41:24.864071\n",
      "[Epoch 34/51] [Batch 400/476] [D loss: 0.108723] [G loss: 0.738699, adv: 0.428008, cycle: 0.020107, identity: 0.021925] ETA: 0:40:17.981186\n",
      "[Epoch 35/51] [Batch 0/476] [D loss: 0.128909] [G loss: 0.755606, adv: 0.461773, cycle: 0.019404, identity: 0.019959] ETA: 1:06:51.719589\n",
      "[Epoch 35/51] [Batch 100/476] [D loss: 0.235626] [G loss: 0.821861, adv: 0.517716, cycle: 0.020510, identity: 0.019809] ETA: 0:39:41.687907\n",
      "[Epoch 35/51] [Batch 200/476] [D loss: 0.093310] [G loss: 0.662983, adv: 0.172272, cycle: 0.033158, identity: 0.031826] ETA: 0:39:27.636589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/51] [Batch 300/476] [D loss: 0.179419] [G loss: 0.630750, adv: 0.327631, cycle: 0.020135, identity: 0.020353] ETA: 0:38:50.380039\n",
      "[Epoch 35/51] [Batch 400/476] [D loss: 0.262331] [G loss: 0.620292, adv: 0.190471, cycle: 0.028262, identity: 0.029439] ETA: 0:38:21.257160\n",
      "[Epoch 36/51] [Batch 0/476] [D loss: 0.159952] [G loss: 0.701312, adv: 0.403291, cycle: 0.020120, identity: 0.019365] ETA: 1:00:53.147564\n",
      "[Epoch 36/51] [Batch 100/476] [D loss: 0.123536] [G loss: 0.822538, adv: 0.489320, cycle: 0.022138, identity: 0.022368] ETA: 0:37:12.894592\n",
      "[Epoch 36/51] [Batch 200/476] [D loss: 0.242474] [G loss: 0.721788, adv: 0.391892, cycle: 0.021974, identity: 0.022031] ETA: 0:36:03.369160\n",
      "[Epoch 36/51] [Batch 300/476] [D loss: 0.137151] [G loss: 0.726682, adv: 0.291381, cycle: 0.028528, identity: 0.030004] ETA: 0:36:06.684494\n",
      "[Epoch 36/51] [Batch 400/476] [D loss: 0.168277] [G loss: 0.862734, adv: 0.596640, cycle: 0.017698, identity: 0.017824] ETA: 0:35:48.893399\n",
      "[Epoch 37/51] [Batch 0/476] [D loss: 0.101954] [G loss: 1.048344, adv: 0.720900, cycle: 0.022601, identity: 0.020286] ETA: 0:57:16.561926\n",
      "[Epoch 37/51] [Batch 100/476] [D loss: 0.100919] [G loss: 0.881658, adv: 0.566849, cycle: 0.020925, identity: 0.021111] ETA: 0:34:58.127398\n",
      "[Epoch 37/51] [Batch 200/476] [D loss: 0.233582] [G loss: 0.566756, adv: 0.196358, cycle: 0.024192, identity: 0.025695] ETA: 0:33:32.742783\n",
      "[Epoch 37/51] [Batch 300/476] [D loss: 0.162261] [G loss: 0.775365, adv: 0.443617, cycle: 0.022635, identity: 0.021079] ETA: 0:33:55.705754\n",
      "[Epoch 37/51] [Batch 400/476] [D loss: 0.070497] [G loss: 1.085121, adv: 0.648153, cycle: 0.028501, identity: 0.030391] ETA: 0:33:20.596601\n",
      "[Epoch 38/51] [Batch 0/476] [D loss: 0.287299] [G loss: 1.258942, adv: 0.954144, cycle: 0.020618, identity: 0.019724] ETA: 0:55:37.862411\n",
      "[Epoch 38/51] [Batch 100/476] [D loss: 0.146683] [G loss: 0.691265, adv: 0.257036, cycle: 0.028107, identity: 0.030632] ETA: 0:32:13.852234\n",
      "[Epoch 38/51] [Batch 200/476] [D loss: 0.165816] [G loss: 0.934043, adv: 0.600543, cycle: 0.022381, identity: 0.021937] ETA: 0:31:48.760088\n",
      "[Epoch 38/51] [Batch 300/476] [D loss: 0.152046] [G loss: 1.050320, adv: 0.741000, cycle: 0.021115, identity: 0.019634] ETA: 0:31:04.853027\n",
      "[Epoch 38/51] [Batch 400/476] [D loss: 0.161568] [G loss: 1.000973, adv: 0.699787, cycle: 0.019731, identity: 0.020776] ETA: 0:30:46.672866\n",
      "[Epoch 39/51] [Batch 0/476] [D loss: 0.124540] [G loss: 0.840570, adv: 0.487424, cycle: 0.024002, identity: 0.022626] ETA: 0:48:52.360119\n",
      "[Epoch 39/51] [Batch 100/476] [D loss: 0.150706] [G loss: 0.763938, adv: 0.416533, cycle: 0.023265, identity: 0.022952] ETA: 0:29:32.202428\n",
      "[Epoch 39/51] [Batch 200/476] [D loss: 0.182602] [G loss: 0.716143, adv: 0.415397, cycle: 0.020805, identity: 0.018540] ETA: 0:29:07.081423\n",
      "[Epoch 39/51] [Batch 300/476] [D loss: 0.131549] [G loss: 0.672094, adv: 0.359179, cycle: 0.020971, identity: 0.020641] ETA: 0:28:41.884809\n",
      "[Epoch 39/51] [Batch 400/476] [D loss: 0.094907] [G loss: 0.689752, adv: 0.381371, cycle: 0.020579, identity: 0.020519] ETA: 0:28:08.651566\n",
      "[Epoch 40/51] [Batch 0/476] [D loss: 0.255228] [G loss: 1.047148, adv: 0.693864, cycle: 0.023280, identity: 0.024098] ETA: 0:44:56.845149\n",
      "[Epoch 40/51] [Batch 100/476] [D loss: 0.118630] [G loss: 1.097406, adv: 0.703186, cycle: 0.026173, identity: 0.026498] ETA: 0:27:10.100166\n",
      "[Epoch 40/51] [Batch 200/476] [D loss: 0.128898] [G loss: 1.045294, adv: 0.696245, cycle: 0.023460, identity: 0.022889] ETA: 0:26:38.561969\n",
      "[Epoch 40/51] [Batch 300/476] [D loss: 0.103893] [G loss: 1.208391, adv: 0.705260, cycle: 0.033552, identity: 0.033522] ETA: 0:25:49.318577\n",
      "[Epoch 40/51] [Batch 400/476] [D loss: 0.156771] [G loss: 1.111087, adv: 0.667611, cycle: 0.029006, identity: 0.030684] ETA: 0:25:43.111788\n",
      "[Epoch 41/51] [Batch 0/476] [D loss: 0.138950] [G loss: 0.733484, adv: 0.414225, cycle: 0.021278, identity: 0.021296] ETA: 0:40:45.484409\n",
      "[Epoch 41/51] [Batch 100/476] [D loss: 0.136119] [G loss: 1.085343, adv: 0.636930, cycle: 0.029183, identity: 0.031317] ETA: 0:24:44.101315\n",
      "[Epoch 41/51] [Batch 200/476] [D loss: 0.126868] [G loss: 0.839133, adv: 0.483277, cycle: 0.023565, identity: 0.024041] ETA: 0:24:15.148830\n",
      "[Epoch 41/51] [Batch 300/476] [D loss: 0.156122] [G loss: 1.301446, adv: 0.804537, cycle: 0.031577, identity: 0.036228] ETA: 0:23:40.659056\n",
      "[Epoch 41/51] [Batch 400/476] [D loss: 0.146720] [G loss: 0.929868, adv: 0.604559, cycle: 0.022110, identity: 0.020842] ETA: 0:22:59.475117\n",
      "[Epoch 42/51] [Batch 0/476] [D loss: 0.183199] [G loss: 0.753152, adv: 0.438051, cycle: 0.021073, identity: 0.020875] ETA: 0:37:03.420742\n",
      "[Epoch 42/51] [Batch 100/476] [D loss: 0.111895] [G loss: 0.837279, adv: 0.531347, cycle: 0.020714, identity: 0.019758] ETA: 0:22:06.724655\n",
      "[Epoch 42/51] [Batch 200/476] [D loss: 0.115437] [G loss: 0.792860, adv: 0.463293, cycle: 0.022810, identity: 0.020293] ETA: 0:21:42.579862\n",
      "[Epoch 42/51] [Batch 300/476] [D loss: 0.264572] [G loss: 0.659721, adv: 0.349883, cycle: 0.020676, identity: 0.020615] ETA: 0:21:08.621109\n",
      "[Epoch 42/51] [Batch 400/476] [D loss: 0.113226] [G loss: 0.936162, adv: 0.512753, cycle: 0.027683, identity: 0.029315] ETA: 0:20:36.051286\n",
      "[Epoch 43/51] [Batch 0/476] [D loss: 0.154625] [G loss: 0.534900, adv: 0.247450, cycle: 0.019454, identity: 0.018582] ETA: 0:32:38.324982\n",
      "[Epoch 43/51] [Batch 100/476] [D loss: 0.199211] [G loss: 0.785120, adv: 0.373731, cycle: 0.028924, identity: 0.024431] ETA: 0:19:30.733380\n",
      "[Epoch 43/51] [Batch 200/476] [D loss: 0.132122] [G loss: 0.969137, adv: 0.612781, cycle: 0.023178, identity: 0.024915] ETA: 0:19:08.357615\n",
      "[Epoch 43/51] [Batch 300/476] [D loss: 0.118908] [G loss: 0.865507, adv: 0.526236, cycle: 0.022984, identity: 0.021887] ETA: 0:18:21.849513\n",
      "[Epoch 43/51] [Batch 400/476] [D loss: 0.078208] [G loss: 0.921012, adv: 0.580184, cycle: 0.022042, identity: 0.024082] ETA: 0:17:39.051464\n",
      "[Epoch 44/51] [Batch 0/476] [D loss: 0.218731] [G loss: 0.943855, adv: 0.462578, cycle: 0.032844, identity: 0.030567] ETA: 0:28:42.174370\n",
      "[Epoch 44/51] [Batch 100/476] [D loss: 0.095320] [G loss: 1.071858, adv: 0.786536, cycle: 0.019703, identity: 0.017658] ETA: 0:17:10.878563\n",
      "[Epoch 44/51] [Batch 200/476] [D loss: 0.170476] [G loss: 1.353725, adv: 1.016048, cycle: 0.023247, identity: 0.021041] ETA: 0:16:38.307527\n",
      "[Epoch 44/51] [Batch 300/476] [D loss: 0.146525] [G loss: 1.113417, adv: 0.802216, cycle: 0.020331, identity: 0.021578] ETA: 0:15:55.721373\n",
      "[Epoch 44/51] [Batch 400/476] [D loss: 0.119246] [G loss: 1.251512, adv: 0.922351, cycle: 0.022536, identity: 0.020760] ETA: 0:15:33.781306\n",
      "[Epoch 45/51] [Batch 0/476] [D loss: 0.100824] [G loss: 0.777944, adv: 0.425441, cycle: 0.023805, identity: 0.022890] ETA: 0:24:16.939247\n",
      "[Epoch 45/51] [Batch 100/476] [D loss: 0.117063] [G loss: 0.788743, adv: 0.471511, cycle: 0.020673, identity: 0.022101] ETA: 0:14:38.285498\n",
      "[Epoch 45/51] [Batch 200/476] [D loss: 0.179631] [G loss: 1.100158, adv: 0.719078, cycle: 0.025776, identity: 0.024664] ETA: 0:14:06.994255\n",
      "[Epoch 45/51] [Batch 300/476] [D loss: 0.140996] [G loss: 0.870490, adv: 0.504034, cycle: 0.022764, identity: 0.027763] ETA: 0:13:27.432092\n",
      "[Epoch 45/51] [Batch 400/476] [D loss: 0.212801] [G loss: 0.678059, adv: 0.378663, cycle: 0.020120, identity: 0.019640] ETA: 0:13:02.189259\n",
      "[Epoch 46/51] [Batch 0/476] [D loss: 0.153013] [G loss: 0.633754, adv: 0.140964, cycle: 0.031597, identity: 0.035365] ETA: 0:20:48.701844\n",
      "[Epoch 46/51] [Batch 100/476] [D loss: 0.174666] [G loss: 1.065915, adv: 0.733121, cycle: 0.020913, identity: 0.024733] ETA: 0:12:01.824274\n",
      "[Epoch 46/51] [Batch 200/476] [D loss: 0.120305] [G loss: 0.947868, adv: 0.589648, cycle: 0.024277, identity: 0.023089] ETA: 0:11:35.032797\n",
      "[Epoch 46/51] [Batch 300/476] [D loss: 0.130965] [G loss: 0.891553, adv: 0.600789, cycle: 0.018976, identity: 0.020201] ETA: 0:11:02.382889\n",
      "[Epoch 46/51] [Batch 400/476] [D loss: 0.131071] [G loss: 0.954173, adv: 0.617528, cycle: 0.022938, identity: 0.021453] ETA: 0:10:31.213088\n",
      "[Epoch 47/51] [Batch 0/476] [D loss: 0.153233] [G loss: 0.799972, adv: 0.416042, cycle: 0.025019, identity: 0.026747] ETA: 0:16:18.332218\n",
      "[Epoch 47/51] [Batch 100/476] [D loss: 0.239456] [G loss: 1.263358, adv: 0.740575, cycle: 0.034386, identity: 0.035784] ETA: 0:09:35.677731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/51] [Batch 200/476] [D loss: 0.087663] [G loss: 0.892701, adv: 0.573743, cycle: 0.020127, identity: 0.023538] ETA: 0:09:02.600567\n",
      "[Epoch 47/51] [Batch 300/476] [D loss: 0.123875] [G loss: 0.949089, adv: 0.453589, cycle: 0.033750, identity: 0.031601] ETA: 0:08:31.620558\n",
      "[Epoch 47/51] [Batch 400/476] [D loss: 0.109592] [G loss: 0.984673, adv: 0.645093, cycle: 0.022704, identity: 0.022508] ETA: 0:08:00.331810\n",
      "[Epoch 48/51] [Batch 0/476] [D loss: 0.124838] [G loss: 1.004523, adv: 0.620685, cycle: 0.025828, identity: 0.025112] ETA: 0:12:16.184827\n",
      "[Epoch 48/51] [Batch 100/476] [D loss: 0.144347] [G loss: 0.960570, adv: 0.618707, cycle: 0.023095, identity: 0.022182] ETA: 0:07:02.935760\n",
      "[Epoch 48/51] [Batch 200/476] [D loss: 0.141635] [G loss: 1.349067, adv: 0.990649, cycle: 0.024785, identity: 0.022114] ETA: 0:06:33.971173\n",
      "[Epoch 48/51] [Batch 300/476] [D loss: 0.087648] [G loss: 0.671265, adv: 0.379288, cycle: 0.018562, identity: 0.021272] ETA: 0:06:01.441051\n",
      "[Epoch 48/51] [Batch 400/476] [D loss: 0.111506] [G loss: 1.310781, adv: 0.944661, cycle: 0.024367, identity: 0.024490] ETA: 0:05:26.905061\n",
      "[Epoch 49/51] [Batch 0/476] [D loss: 0.252303] [G loss: 0.688628, adv: 0.276031, cycle: 0.027436, identity: 0.027647] ETA: 0:08:10.615795\n",
      "[Epoch 49/51] [Batch 100/476] [D loss: 0.071258] [G loss: 0.887753, adv: 0.517020, cycle: 0.023962, identity: 0.026223] ETA: 0:04:33.113445\n",
      "[Epoch 49/51] [Batch 200/476] [D loss: 0.129482] [G loss: 0.619483, adv: 0.321183, cycle: 0.019823, identity: 0.020013] ETA: 0:03:59.614765\n",
      "[Epoch 49/51] [Batch 300/476] [D loss: 0.166295] [G loss: 1.048913, adv: 0.751947, cycle: 0.020110, identity: 0.019174] ETA: 0:03:28.405695\n",
      "[Epoch 49/51] [Batch 400/476] [D loss: 0.079578] [G loss: 0.744751, adv: 0.409634, cycle: 0.021106, identity: 0.024811] ETA: 0:02:55.531174\n",
      "[Epoch 50/51] [Batch 0/476] [D loss: 0.246903] [G loss: 1.237566, adv: 0.700712, cycle: 0.035502, identity: 0.036366] ETA: 0:04:06.407475\n",
      "[Epoch 50/51] [Batch 100/476] [D loss: 0.114812] [G loss: 0.941768, adv: 0.607604, cycle: 0.023147, identity: 0.020539] ETA: 0:02:01.064480\n",
      "[Epoch 50/51] [Batch 200/476] [D loss: 0.177841] [G loss: 0.616517, adv: 0.320333, cycle: 0.019300, identity: 0.020636] ETA: 0:01:26.568752\n",
      "[Epoch 50/51] [Batch 300/476] [D loss: 0.125265] [G loss: 0.757379, adv: 0.430757, cycle: 0.022487, identity: 0.020350] ETA: 0:00:56.016857\n",
      "[Epoch 50/51] [Batch 400/476] [D loss: 0.092627] [G loss: 1.178645, adv: 0.795699, cycle: 0.024455, identity: 0.027680] ETA: 0:00:24.231208\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"ganmodels/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"ganmodels/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"ganmodels/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"ganmodels/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "    \n",
    "epoch = 0\n",
    "dataset_name = 'CycleGANSS2'\n",
    "os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"ganmodels/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader2, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/51] [Batch 0/476] [D loss: 1.577025] [G loss: 6.722693, adv: 1.489475, cycle: 0.386656, identity: 0.273331] ETA: 2:57:38.017962\n",
      "[Epoch 0/51] [Batch 100/476] [D loss: 0.269672] [G loss: 0.803531, adv: 0.288308, cycle: 0.034580, identity: 0.033885] ETA: 2:08:41.585583\n",
      "[Epoch 0/51] [Batch 200/476] [D loss: 0.275029] [G loss: 0.768324, adv: 0.283902, cycle: 0.032766, identity: 0.031353] ETA: 2:07:52.936907\n",
      "[Epoch 0/51] [Batch 300/476] [D loss: 0.247419] [G loss: 0.784800, adv: 0.296999, cycle: 0.031510, identity: 0.034540] ETA: 2:07:34.077610\n",
      "[Epoch 0/51] [Batch 400/476] [D loss: 0.248487] [G loss: 0.792406, adv: 0.365100, cycle: 0.027204, identity: 0.031053] ETA: 2:07:03.371887\n",
      "[Epoch 1/51] [Batch 0/476] [D loss: 0.246048] [G loss: 0.600102, adv: 0.257811, cycle: 0.023645, identity: 0.021168] ETA: 3:54:05.323133\n",
      "[Epoch 1/51] [Batch 100/476] [D loss: 0.246003] [G loss: 0.551997, adv: 0.231879, cycle: 0.021566, identity: 0.020891] ETA: 2:05:15.864444\n",
      "[Epoch 1/51] [Batch 200/476] [D loss: 0.255082] [G loss: 0.731108, adv: 0.300204, cycle: 0.029171, identity: 0.027839] ETA: 2:05:09.961510\n",
      "[Epoch 1/51] [Batch 300/476] [D loss: 0.261042] [G loss: 0.646857, adv: 0.260609, cycle: 0.025819, identity: 0.025610] ETA: 2:03:23.666735\n",
      "[Epoch 1/51] [Batch 400/476] [D loss: 0.273276] [G loss: 0.728676, adv: 0.257218, cycle: 0.032172, identity: 0.029948] ETA: 2:04:35.685596\n",
      "[Epoch 2/51] [Batch 0/476] [D loss: 0.331915] [G loss: 0.626120, adv: 0.334296, cycle: 0.020314, identity: 0.017737] ETA: 3:19:14.568752\n",
      "[Epoch 2/51] [Batch 100/476] [D loss: 0.253613] [G loss: 0.710984, adv: 0.234117, cycle: 0.031866, identity: 0.031642] ETA: 2:03:07.786388\n",
      "[Epoch 2/51] [Batch 200/476] [D loss: 0.313603] [G loss: 0.595119, adv: 0.304217, cycle: 0.019815, identity: 0.018550] ETA: 2:03:13.856526\n",
      "[Epoch 2/51] [Batch 300/476] [D loss: 0.257469] [G loss: 0.808429, adv: 0.299365, cycle: 0.034067, identity: 0.033680] ETA: 2:01:56.440876\n",
      "[Epoch 2/51] [Batch 400/476] [D loss: 0.282623] [G loss: 0.509768, adv: 0.294928, cycle: 0.014291, identity: 0.014386] ETA: 2:01:29.085018\n",
      "[Epoch 3/51] [Batch 0/476] [D loss: 0.237019] [G loss: 0.967612, adv: 0.292474, cycle: 0.045470, identity: 0.044088] ETA: 3:22:18.021790\n",
      "[Epoch 3/51] [Batch 100/476] [D loss: 0.319864] [G loss: 0.611844, adv: 0.290182, cycle: 0.020442, identity: 0.023448] ETA: 2:00:31.110618\n",
      "[Epoch 3/51] [Batch 200/476] [D loss: 0.248654] [G loss: 0.495518, adv: 0.237385, cycle: 0.017617, identity: 0.016392] ETA: 1:58:53.505730\n",
      "[Epoch 3/51] [Batch 300/476] [D loss: 0.231968] [G loss: 0.552373, adv: 0.261668, cycle: 0.019200, identity: 0.019740] ETA: 1:59:59.193277\n",
      "[Epoch 3/51] [Batch 400/476] [D loss: 0.235445] [G loss: 0.852736, adv: 0.257310, cycle: 0.040310, identity: 0.038465] ETA: 1:58:45.433586\n",
      "[Epoch 4/51] [Batch 0/476] [D loss: 0.251226] [G loss: 0.524231, adv: 0.265225, cycle: 0.016720, identity: 0.018361] ETA: 3:12:48.984720\n",
      "[Epoch 4/51] [Batch 100/476] [D loss: 0.239303] [G loss: 0.516164, adv: 0.252915, cycle: 0.017777, identity: 0.017096] ETA: 1:58:25.341614\n",
      "[Epoch 4/51] [Batch 200/476] [D loss: 0.247561] [G loss: 0.470310, adv: 0.236313, cycle: 0.016613, identity: 0.013574] ETA: 1:55:06.421021\n",
      "[Epoch 4/51] [Batch 300/476] [D loss: 0.244675] [G loss: 0.445375, adv: 0.244461, cycle: 0.013402, identity: 0.013379] ETA: 1:57:04.917883\n",
      "[Epoch 4/51] [Batch 400/476] [D loss: 0.226109] [G loss: 0.769155, adv: 0.233476, cycle: 0.036492, identity: 0.034151] ETA: 1:56:52.274108\n",
      "[Epoch 5/51] [Batch 0/476] [D loss: 0.216743] [G loss: 0.501720, adv: 0.299197, cycle: 0.013340, identity: 0.013824] ETA: 3:10:33.827314\n",
      "[Epoch 5/51] [Batch 100/476] [D loss: 0.248460] [G loss: 0.607819, adv: 0.319339, cycle: 0.018695, identity: 0.020306] ETA: 1:55:46.527145\n",
      "[Epoch 5/51] [Batch 200/476] [D loss: 0.186018] [G loss: 0.936134, adv: 0.464332, cycle: 0.033324, identity: 0.027712] ETA: 1:54:51.725784\n",
      "[Epoch 5/51] [Batch 300/476] [D loss: 0.159258] [G loss: 1.004999, adv: 0.584970, cycle: 0.029214, identity: 0.025577] ETA: 1:54:25.871747\n",
      "[Epoch 5/51] [Batch 400/476] [D loss: 0.283868] [G loss: 0.793426, adv: 0.386257, cycle: 0.027943, identity: 0.025548] ETA: 1:54:16.419491\n",
      "[Epoch 6/51] [Batch 0/476] [D loss: 0.263902] [G loss: 0.819506, adv: 0.429359, cycle: 0.026141, identity: 0.025747] ETA: 3:06:06.319156\n",
      "[Epoch 6/51] [Batch 100/476] [D loss: 0.265656] [G loss: 0.564411, adv: 0.192759, cycle: 0.025767, identity: 0.022796] ETA: 1:53:51.344805\n",
      "[Epoch 6/51] [Batch 200/476] [D loss: 0.257819] [G loss: 0.732925, adv: 0.248139, cycle: 0.033180, identity: 0.030597] ETA: 1:53:01.023808\n",
      "[Epoch 6/51] [Batch 300/476] [D loss: 0.209327] [G loss: 0.607972, adv: 0.348423, cycle: 0.017836, identity: 0.016238] ETA: 1:52:36.213226\n",
      "[Epoch 6/51] [Batch 400/476] [D loss: 0.094356] [G loss: 1.286389, adv: 0.623413, cycle: 0.044401, identity: 0.043793] ETA: 1:51:51.960297\n",
      "[Epoch 7/51] [Batch 0/476] [D loss: 0.173394] [G loss: 0.603609, adv: 0.335497, cycle: 0.017842, identity: 0.017939] ETA: 3:02:38.690498\n",
      "[Epoch 7/51] [Batch 100/476] [D loss: 0.229413] [G loss: 0.575131, adv: 0.192157, cycle: 0.025699, identity: 0.025197] ETA: 1:48:20.441420\n",
      "[Epoch 7/51] [Batch 200/476] [D loss: 0.371276] [G loss: 0.735778, adv: 0.102434, cycle: 0.044433, identity: 0.037804] ETA: 1:50:21.940617\n",
      "[Epoch 7/51] [Batch 300/476] [D loss: 0.265460] [G loss: 1.402870, adv: 0.610759, cycle: 0.055679, identity: 0.047065] ETA: 1:49:47.144022\n",
      "[Epoch 7/51] [Batch 400/476] [D loss: 0.131265] [G loss: 0.652364, adv: 0.309805, cycle: 0.023396, identity: 0.021720] ETA: 1:48:28.219162\n",
      "[Epoch 8/51] [Batch 0/476] [D loss: 0.067021] [G loss: 1.058808, adv: 0.619671, cycle: 0.028987, identity: 0.029854] ETA: 2:57:06.123621\n",
      "[Epoch 8/51] [Batch 100/476] [D loss: 0.139784] [G loss: 1.127534, adv: 0.551666, cycle: 0.038914, identity: 0.037345] ETA: 1:48:29.668362\n",
      "[Epoch 8/51] [Batch 200/476] [D loss: 0.138674] [G loss: 0.584608, adv: 0.164698, cycle: 0.027902, identity: 0.028177] ETA: 1:48:35.916830\n",
      "[Epoch 8/51] [Batch 300/476] [D loss: 0.200813] [G loss: 1.233360, adv: 0.690116, cycle: 0.037097, identity: 0.034454] ETA: 1:47:48.116613\n",
      "[Epoch 8/51] [Batch 400/476] [D loss: 0.184228] [G loss: 0.947436, adv: 0.598920, cycle: 0.023559, identity: 0.022586] ETA: 1:44:58.880973\n",
      "[Epoch 9/51] [Batch 0/476] [D loss: 0.238770] [G loss: 0.541834, adv: 0.213894, cycle: 0.021644, identity: 0.022300] ETA: 2:58:09.291756\n",
      "[Epoch 9/51] [Batch 100/476] [D loss: 0.446302] [G loss: 0.711320, adv: 0.134136, cycle: 0.039987, identity: 0.035463] ETA: 1:46:16.204425\n",
      "[Epoch 9/51] [Batch 200/476] [D loss: 0.179803] [G loss: 0.842316, adv: 0.498011, cycle: 0.023166, identity: 0.022529] ETA: 1:45:07.235291\n",
      "[Epoch 9/51] [Batch 300/476] [D loss: 0.258397] [G loss: 0.793664, adv: 0.217451, cycle: 0.037658, identity: 0.039927] ETA: 1:44:57.382258\n",
      "[Epoch 9/51] [Batch 400/476] [D loss: 0.193534] [G loss: 0.582648, adv: 0.169108, cycle: 0.029084, identity: 0.024540] ETA: 1:43:49.309299\n",
      "[Epoch 10/51] [Batch 0/476] [D loss: 0.239656] [G loss: 0.549599, adv: 0.237129, cycle: 0.021300, identity: 0.019894] ETA: 2:48:32.026407\n",
      "[Epoch 10/51] [Batch 100/476] [D loss: 0.230377] [G loss: 0.823903, adv: 0.451323, cycle: 0.024842, identity: 0.024832] ETA: 1:42:09.988695\n",
      "[Epoch 10/51] [Batch 200/476] [D loss: 0.268028] [G loss: 0.566852, adv: 0.300260, cycle: 0.018373, identity: 0.016572] ETA: 1:41:28.110208\n",
      "[Epoch 10/51] [Batch 300/476] [D loss: 0.249818] [G loss: 0.835437, adv: 0.329820, cycle: 0.035783, identity: 0.029558] ETA: 1:41:59.985268\n",
      "[Epoch 10/51] [Batch 400/476] [D loss: 0.208281] [G loss: 0.467297, adv: 0.262569, cycle: 0.014226, identity: 0.012493] ETA: 1:41:12.659243\n",
      "[Epoch 11/51] [Batch 0/476] [D loss: 0.261157] [G loss: 0.478402, adv: 0.239943, cycle: 0.016284, identity: 0.015124] ETA: 2:45:15.543900\n",
      "[Epoch 11/51] [Batch 100/476] [D loss: 0.261245] [G loss: 0.836269, adv: 0.261292, cycle: 0.039432, identity: 0.036132] ETA: 1:40:07.771482\n",
      "[Epoch 11/51] [Batch 200/476] [D loss: 0.250070] [G loss: 0.716400, adv: 0.243786, cycle: 0.031922, identity: 0.030679] ETA: 1:40:20.560770\n",
      "[Epoch 11/51] [Batch 300/476] [D loss: 0.224874] [G loss: 0.596224, adv: 0.231013, cycle: 0.024156, identity: 0.024730] ETA: 1:39:42.268929\n",
      "[Epoch 11/51] [Batch 400/476] [D loss: 0.236164] [G loss: 0.520103, adv: 0.253521, cycle: 0.018311, identity: 0.016694] ETA: 1:39:00.489407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/51] [Batch 0/476] [D loss: 0.239292] [G loss: 0.494492, adv: 0.298944, cycle: 0.013325, identity: 0.012459] ETA: 2:40:25.227642\n",
      "[Epoch 12/51] [Batch 100/476] [D loss: 0.281717] [G loss: 0.462045, adv: 0.254326, cycle: 0.014374, identity: 0.012796] ETA: 1:38:20.440422\n",
      "[Epoch 12/51] [Batch 200/476] [D loss: 0.211068] [G loss: 0.558043, adv: 0.273710, cycle: 0.019174, identity: 0.018518] ETA: 1:37:23.628248\n",
      "[Epoch 12/51] [Batch 300/476] [D loss: 0.246957] [G loss: 0.649393, adv: 0.350370, cycle: 0.020526, identity: 0.018752] ETA: 1:37:02.144669\n",
      "[Epoch 12/51] [Batch 400/476] [D loss: 0.279780] [G loss: 0.482561, adv: 0.213771, cycle: 0.017975, identity: 0.017807] ETA: 1:34:52.940268\n",
      "[Epoch 13/51] [Batch 0/476] [D loss: 0.230332] [G loss: 0.636415, adv: 0.168827, cycle: 0.031292, identity: 0.030934] ETA: 2:36:34.650616\n",
      "[Epoch 13/51] [Batch 100/476] [D loss: 0.257444] [G loss: 0.557313, adv: 0.317055, cycle: 0.016612, identity: 0.014829] ETA: 1:35:17.157603\n",
      "[Epoch 13/51] [Batch 200/476] [D loss: 0.272278] [G loss: 0.588719, adv: 0.346743, cycle: 0.016187, identity: 0.016021] ETA: 1:33:25.924881\n",
      "[Epoch 13/51] [Batch 300/476] [D loss: 0.241697] [G loss: 0.526343, adv: 0.199372, cycle: 0.021736, identity: 0.021922] ETA: 1:34:26.288775\n",
      "[Epoch 13/51] [Batch 400/476] [D loss: 0.260829] [G loss: 0.495428, adv: 0.253325, cycle: 0.016486, identity: 0.015449] ETA: 1:34:02.518490\n",
      "[Epoch 14/51] [Batch 0/476] [D loss: 0.260646] [G loss: 0.563242, adv: 0.310972, cycle: 0.016626, identity: 0.017202] ETA: 2:33:41.120109\n",
      "[Epoch 14/51] [Batch 100/476] [D loss: 0.302707] [G loss: 0.528654, adv: 0.268601, cycle: 0.018248, identity: 0.015514] ETA: 1:33:12.620106\n",
      "[Epoch 14/51] [Batch 200/476] [D loss: 0.244865] [G loss: 0.453598, adv: 0.214474, cycle: 0.016325, identity: 0.015175] ETA: 1:31:43.574132\n",
      "[Epoch 14/51] [Batch 300/476] [D loss: 0.257204] [G loss: 0.511132, adv: 0.317381, cycle: 0.013253, identity: 0.012244] ETA: 1:31:22.441795\n",
      "[Epoch 14/51] [Batch 400/476] [D loss: 0.236513] [G loss: 0.782443, adv: 0.293429, cycle: 0.033051, identity: 0.031701] ETA: 1:31:10.671997\n",
      "[Epoch 15/51] [Batch 0/476] [D loss: 0.212749] [G loss: 0.546737, adv: 0.244538, cycle: 0.021167, identity: 0.018105] ETA: 2:27:17.437328\n",
      "[Epoch 15/51] [Batch 100/476] [D loss: 0.239302] [G loss: 0.482846, adv: 0.243948, cycle: 0.016101, identity: 0.015578] ETA: 1:30:32.644148\n",
      "[Epoch 15/51] [Batch 200/476] [D loss: 0.250367] [G loss: 0.487805, adv: 0.177244, cycle: 0.022085, identity: 0.017942] ETA: 1:29:58.215136\n",
      "[Epoch 15/51] [Batch 300/476] [D loss: 0.272868] [G loss: 0.630534, adv: 0.267818, cycle: 0.024598, identity: 0.023346] ETA: 1:29:27.581263\n",
      "[Epoch 15/51] [Batch 400/476] [D loss: 0.303409] [G loss: 0.759240, adv: 0.427659, cycle: 0.022484, identity: 0.021348] ETA: 1:27:31.235703\n",
      "[Epoch 16/51] [Batch 0/476] [D loss: 0.271243] [G loss: 0.838264, adv: 0.265344, cycle: 0.039543, identity: 0.035498] ETA: 2:24:00.352430\n",
      "[Epoch 16/51] [Batch 100/476] [D loss: 0.178691] [G loss: 0.767087, adv: 0.495093, cycle: 0.018439, identity: 0.017521] ETA: 1:28:08.440018\n",
      "[Epoch 16/51] [Batch 200/476] [D loss: 0.238403] [G loss: 0.633150, adv: 0.385938, cycle: 0.016807, identity: 0.015828] ETA: 1:27:38.074741\n",
      "[Epoch 16/51] [Batch 300/476] [D loss: 0.177001] [G loss: 0.589912, adv: 0.292389, cycle: 0.020330, identity: 0.018844] ETA: 1:25:52.008448\n",
      "[Epoch 16/51] [Batch 400/476] [D loss: 0.224044] [G loss: 0.504618, adv: 0.201604, cycle: 0.020878, identity: 0.018847] ETA: 1:26:07.432609\n",
      "[Epoch 17/51] [Batch 0/476] [D loss: 0.220073] [G loss: 0.811533, adv: 0.249747, cycle: 0.037835, identity: 0.036687] ETA: 2:19:33.096554\n",
      "[Epoch 17/51] [Batch 100/476] [D loss: 0.172067] [G loss: 0.622683, adv: 0.342206, cycle: 0.018306, identity: 0.019484] ETA: 1:25:11.008914\n",
      "[Epoch 17/51] [Batch 200/476] [D loss: 0.190717] [G loss: 0.706977, adv: 0.440961, cycle: 0.018624, identity: 0.015955] ETA: 1:24:53.450340\n",
      "[Epoch 17/51] [Batch 300/476] [D loss: 0.218251] [G loss: 0.769876, adv: 0.357967, cycle: 0.028031, identity: 0.026320] ETA: 1:24:21.118604\n",
      "[Epoch 17/51] [Batch 400/476] [D loss: 0.293375] [G loss: 0.629268, adv: 0.342861, cycle: 0.019717, identity: 0.017847] ETA: 1:23:58.200729\n",
      "[Epoch 18/51] [Batch 0/476] [D loss: 0.200120] [G loss: 0.638978, adv: 0.338785, cycle: 0.020193, identity: 0.019653] ETA: 2:19:38.824142\n",
      "[Epoch 18/51] [Batch 100/476] [D loss: 0.122010] [G loss: 1.054143, adv: 0.499665, cycle: 0.037932, identity: 0.035031] ETA: 1:23:06.119192\n",
      "[Epoch 18/51] [Batch 200/476] [D loss: 0.196242] [G loss: 0.860103, adv: 0.380564, cycle: 0.033017, identity: 0.029874] ETA: 1:22:29.817740\n",
      "[Epoch 18/51] [Batch 300/476] [D loss: 0.228027] [G loss: 0.576293, adv: 0.182938, cycle: 0.027590, identity: 0.023492] ETA: 1:21:52.227940\n",
      "[Epoch 18/51] [Batch 400/476] [D loss: 0.115811] [G loss: 1.026245, adv: 0.585641, cycle: 0.030083, identity: 0.027954] ETA: 1:21:14.456283\n",
      "[Epoch 19/51] [Batch 0/476] [D loss: 0.151396] [G loss: 0.883127, adv: 0.466788, cycle: 0.028239, identity: 0.026789] ETA: 2:12:36.919312\n",
      "[Epoch 19/51] [Batch 100/476] [D loss: 0.152021] [G loss: 0.842925, adv: 0.545940, cycle: 0.020486, identity: 0.018425] ETA: 1:20:32.443949\n",
      "[Epoch 19/51] [Batch 200/476] [D loss: 0.157290] [G loss: 0.937300, adv: 0.633061, cycle: 0.020758, identity: 0.019332] ETA: 1:19:39.682596\n",
      "[Epoch 19/51] [Batch 300/476] [D loss: 0.130134] [G loss: 0.833371, adv: 0.602801, cycle: 0.015864, identity: 0.014386] ETA: 1:19:00.374139\n",
      "[Epoch 19/51] [Batch 400/476] [D loss: 0.106105] [G loss: 0.921906, adv: 0.537729, cycle: 0.026593, identity: 0.023650] ETA: 1:18:44.155289\n",
      "[Epoch 20/51] [Batch 0/476] [D loss: 0.206936] [G loss: 0.439271, adv: 0.178911, cycle: 0.017995, identity: 0.016083] ETA: 2:08:26.193419\n",
      "[Epoch 20/51] [Batch 100/476] [D loss: 0.284785] [G loss: 1.094472, adv: 0.727736, cycle: 0.025197, identity: 0.022953] ETA: 1:17:53.726608\n",
      "[Epoch 20/51] [Batch 200/476] [D loss: 0.099208] [G loss: 0.699882, adv: 0.318107, cycle: 0.026195, identity: 0.023966] ETA: 1:17:45.297143\n",
      "[Epoch 20/51] [Batch 300/476] [D loss: 0.160789] [G loss: 0.861836, adv: 0.547312, cycle: 0.021947, identity: 0.019011] ETA: 1:16:30.333107\n",
      "[Epoch 20/51] [Batch 400/476] [D loss: 0.120492] [G loss: 0.776174, adv: 0.504290, cycle: 0.018234, identity: 0.017908] ETA: 1:16:21.443165\n",
      "[Epoch 21/51] [Batch 0/476] [D loss: 0.158640] [G loss: 0.771021, adv: 0.414508, cycle: 0.023849, identity: 0.023604] ETA: 2:03:56.198301\n",
      "[Epoch 21/51] [Batch 100/476] [D loss: 0.110429] [G loss: 0.904484, adv: 0.495196, cycle: 0.026810, identity: 0.028237] ETA: 1:14:22.948151\n",
      "[Epoch 21/51] [Batch 200/476] [D loss: 0.108922] [G loss: 1.247053, adv: 0.920126, cycle: 0.021912, identity: 0.021562] ETA: 1:14:51.936340\n",
      "[Epoch 21/51] [Batch 300/476] [D loss: 0.092798] [G loss: 0.893776, adv: 0.549152, cycle: 0.023769, identity: 0.021387] ETA: 1:14:27.552838\n",
      "[Epoch 21/51] [Batch 400/476] [D loss: 0.137393] [G loss: 0.766055, adv: 0.419756, cycle: 0.023373, identity: 0.022514] ETA: 1:14:08.005781\n",
      "[Epoch 22/51] [Batch 0/476] [D loss: 0.114611] [G loss: 0.856002, adv: 0.571409, cycle: 0.018968, identity: 0.018983] ETA: 1:59:34.956454\n",
      "[Epoch 22/51] [Batch 100/476] [D loss: 0.130683] [G loss: 1.147075, adv: 0.817249, cycle: 0.022298, identity: 0.021370] ETA: 1:12:39.225729\n",
      "[Epoch 22/51] [Batch 200/476] [D loss: 0.126646] [G loss: 0.942991, adv: 0.577208, cycle: 0.024894, identity: 0.023368] ETA: 1:12:20.178812\n",
      "[Epoch 22/51] [Batch 300/476] [D loss: 0.215182] [G loss: 1.415762, adv: 1.057165, cycle: 0.023249, identity: 0.025221] ETA: 1:12:07.270782\n",
      "[Epoch 22/51] [Batch 400/476] [D loss: 0.072269] [G loss: 0.970740, adv: 0.611739, cycle: 0.022720, identity: 0.026360] ETA: 1:11:27.582155\n",
      "[Epoch 23/51] [Batch 0/476] [D loss: 0.063727] [G loss: 0.656746, adv: 0.335871, cycle: 0.021502, identity: 0.021171] ETA: 1:59:30.983379\n",
      "[Epoch 23/51] [Batch 100/476] [D loss: 0.114357] [G loss: 1.132414, adv: 0.826087, cycle: 0.021237, identity: 0.018792] ETA: 1:10:12.374258\n",
      "[Epoch 23/51] [Batch 200/476] [D loss: 0.097113] [G loss: 0.830825, adv: 0.509832, cycle: 0.021196, identity: 0.021807] ETA: 1:09:38.520447\n",
      "[Epoch 23/51] [Batch 300/476] [D loss: 0.095679] [G loss: 1.096487, adv: 0.848052, cycle: 0.016718, identity: 0.016252] ETA: 1:09:12.027685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/51] [Batch 400/476] [D loss: 0.083178] [G loss: 0.819151, adv: 0.493404, cycle: 0.021664, identity: 0.021821] ETA: 1:08:47.385590\n",
      "[Epoch 24/51] [Batch 0/476] [D loss: 0.115622] [G loss: 0.750169, adv: 0.377270, cycle: 0.025036, identity: 0.024508] ETA: 1:50:02.985626\n",
      "[Epoch 24/51] [Batch 100/476] [D loss: 0.161753] [G loss: 1.590630, adv: 1.237472, cycle: 0.023716, identity: 0.023199] ETA: 1:07:56.109081\n",
      "[Epoch 24/51] [Batch 200/476] [D loss: 0.087847] [G loss: 0.773581, adv: 0.456354, cycle: 0.021242, identity: 0.020961] ETA: 1:07:05.376124\n",
      "[Epoch 24/51] [Batch 300/476] [D loss: 0.067903] [G loss: 1.260040, adv: 0.626967, cycle: 0.043335, identity: 0.039943] ETA: 1:05:59.177673\n",
      "[Epoch 24/51] [Batch 400/476] [D loss: 0.184238] [G loss: 0.903406, adv: 0.561870, cycle: 0.023766, identity: 0.020775] ETA: 1:06:27.477330\n",
      "[Epoch 25/51] [Batch 0/476] [D loss: 0.154407] [G loss: 1.092422, adv: 0.786420, cycle: 0.021279, identity: 0.018643] ETA: 1:46:38.792795\n",
      "[Epoch 25/51] [Batch 100/476] [D loss: 0.194387] [G loss: 1.059817, adv: 0.610349, cycle: 0.030291, identity: 0.029313] ETA: 1:05:11.659684\n",
      "[Epoch 25/51] [Batch 200/476] [D loss: 0.156279] [G loss: 1.108292, adv: 0.808540, cycle: 0.020015, identity: 0.019921] ETA: 1:04:06.152710\n",
      "[Epoch 25/51] [Batch 300/476] [D loss: 0.194549] [G loss: 0.636924, adv: 0.287339, cycle: 0.023741, identity: 0.022435] ETA: 1:04:11.103929\n",
      "[Epoch 25/51] [Batch 400/476] [D loss: 0.103023] [G loss: 0.959327, adv: 0.414242, cycle: 0.036953, identity: 0.035112] ETA: 1:02:54.376579\n",
      "[Epoch 26/51] [Batch 0/476] [D loss: 0.096117] [G loss: 0.656550, adv: 0.326995, cycle: 0.022731, identity: 0.020449] ETA: 1:56:29.599895\n",
      "[Epoch 26/51] [Batch 100/476] [D loss: 0.159690] [G loss: 0.709652, adv: 0.405043, cycle: 0.020436, identity: 0.020050] ETA: 1:02:06.419735\n",
      "[Epoch 26/51] [Batch 200/476] [D loss: 0.240237] [G loss: 0.559586, adv: 0.242628, cycle: 0.021195, identity: 0.021001] ETA: 1:02:15.502410\n",
      "[Epoch 26/51] [Batch 300/476] [D loss: 0.212281] [G loss: 1.109551, adv: 0.682885, cycle: 0.028548, identity: 0.028237] ETA: 1:01:18.833485\n",
      "[Epoch 26/51] [Batch 400/476] [D loss: 0.146111] [G loss: 0.948548, adv: 0.642977, cycle: 0.020175, identity: 0.020764] ETA: 1:00:16.890907\n",
      "[Epoch 27/51] [Batch 0/476] [D loss: 0.172187] [G loss: 1.046870, adv: 0.650078, cycle: 0.026822, identity: 0.025714] ETA: 1:38:49.320808\n",
      "[Epoch 27/51] [Batch 100/476] [D loss: 0.185091] [G loss: 0.961119, adv: 0.375007, cycle: 0.039381, identity: 0.038459] ETA: 1:00:18.460430\n",
      "[Epoch 27/51] [Batch 200/476] [D loss: 0.153743] [G loss: 1.117467, adv: 0.702982, cycle: 0.027623, identity: 0.027652] ETA: 0:58:09.522564\n",
      "[Epoch 27/51] [Batch 300/476] [D loss: 0.205589] [G loss: 0.844129, adv: 0.519799, cycle: 0.022379, identity: 0.020109] ETA: 0:59:03.535509\n",
      "[Epoch 27/51] [Batch 400/476] [D loss: 0.141343] [G loss: 0.954561, adv: 0.649371, cycle: 0.020969, identity: 0.019100] ETA: 0:58:26.077049\n",
      "[Epoch 28/51] [Batch 0/476] [D loss: 0.097270] [G loss: 0.730538, adv: 0.412839, cycle: 0.021166, identity: 0.021209] ETA: 1:34:30.934868\n",
      "[Epoch 28/51] [Batch 100/476] [D loss: 0.089336] [G loss: 0.899451, adv: 0.526644, cycle: 0.025146, identity: 0.024269] ETA: 0:57:27.021584\n",
      "[Epoch 28/51] [Batch 200/476] [D loss: 0.138644] [G loss: 0.718702, adv: 0.409448, cycle: 0.020867, identity: 0.020118] ETA: 0:57:02.631133\n",
      "[Epoch 28/51] [Batch 300/476] [D loss: 0.219258] [G loss: 0.792302, adv: 0.464785, cycle: 0.022547, identity: 0.020410] ETA: 0:56:37.994095\n",
      "[Epoch 28/51] [Batch 400/476] [D loss: 0.133246] [G loss: 0.759800, adv: 0.352031, cycle: 0.027707, identity: 0.026139] ETA: 0:55:57.335443\n",
      "[Epoch 29/51] [Batch 0/476] [D loss: 0.191644] [G loss: 0.654289, adv: 0.242763, cycle: 0.029140, identity: 0.024026] ETA: 1:31:15.710026\n",
      "[Epoch 29/51] [Batch 100/476] [D loss: 0.196049] [G loss: 1.041557, adv: 0.650108, cycle: 0.025201, identity: 0.027889] ETA: 0:55:05.356876\n",
      "[Epoch 29/51] [Batch 200/476] [D loss: 0.235010] [G loss: 0.933679, adv: 0.605777, cycle: 0.021839, identity: 0.021901] ETA: 0:54:23.837151\n",
      "[Epoch 29/51] [Batch 300/476] [D loss: 0.169091] [G loss: 1.265347, adv: 0.788505, cycle: 0.032204, identity: 0.030960] ETA: 0:53:49.009716\n",
      "[Epoch 29/51] [Batch 400/476] [D loss: 0.106462] [G loss: 0.786836, adv: 0.500059, cycle: 0.019379, identity: 0.018598] ETA: 0:53:03.993345\n",
      "[Epoch 30/51] [Batch 0/476] [D loss: 0.119004] [G loss: 1.049437, adv: 0.614633, cycle: 0.029002, identity: 0.028957] ETA: 1:28:31.285395\n",
      "[Epoch 30/51] [Batch 100/476] [D loss: 0.154764] [G loss: 0.781859, adv: 0.317352, cycle: 0.030941, identity: 0.031020] ETA: 0:52:28.304298\n",
      "[Epoch 30/51] [Batch 200/476] [D loss: 0.118461] [G loss: 0.812986, adv: 0.430233, cycle: 0.026145, identity: 0.024260] ETA: 0:51:40.031781\n",
      "[Epoch 30/51] [Batch 300/476] [D loss: 0.105969] [G loss: 1.110122, adv: 0.791056, cycle: 0.021403, identity: 0.021007] ETA: 0:51:59.888397\n",
      "[Epoch 30/51] [Batch 400/476] [D loss: 0.156461] [G loss: 1.259968, adv: 0.892972, cycle: 0.024767, identity: 0.023865] ETA: 0:50:41.633736\n",
      "[Epoch 31/51] [Batch 0/476] [D loss: 0.144641] [G loss: 1.067397, adv: 0.548641, cycle: 0.034709, identity: 0.034334] ETA: 1:22:21.541004\n",
      "[Epoch 31/51] [Batch 100/476] [D loss: 0.225070] [G loss: 1.219471, adv: 0.870118, cycle: 0.023506, identity: 0.022858] ETA: 0:49:52.791538\n",
      "[Epoch 31/51] [Batch 200/476] [D loss: 0.162165] [G loss: 1.158211, adv: 0.818667, cycle: 0.022887, identity: 0.022135] ETA: 0:49:16.283493\n",
      "[Epoch 31/51] [Batch 300/476] [D loss: 0.144834] [G loss: 0.758126, adv: 0.442156, cycle: 0.021517, identity: 0.020160] ETA: 0:48:56.069193\n",
      "[Epoch 31/51] [Batch 400/476] [D loss: 0.109819] [G loss: 1.154537, adv: 0.868656, cycle: 0.019802, identity: 0.017573] ETA: 0:48:22.546005\n",
      "[Epoch 32/51] [Batch 0/476] [D loss: 0.141923] [G loss: 1.152500, adv: 0.755045, cycle: 0.026808, identity: 0.025875] ETA: 1:18:10.684034\n",
      "[Epoch 32/51] [Batch 100/476] [D loss: 0.125877] [G loss: 0.782418, adv: 0.352133, cycle: 0.028739, identity: 0.028580] ETA: 0:47:25.604359\n",
      "[Epoch 32/51] [Batch 200/476] [D loss: 0.136778] [G loss: 1.095824, adv: 0.789716, cycle: 0.020890, identity: 0.019441] ETA: 0:46:49.622025\n",
      "[Epoch 32/51] [Batch 300/476] [D loss: 0.126479] [G loss: 1.044323, adv: 0.698424, cycle: 0.023966, identity: 0.021248] ETA: 0:46:20.311245\n",
      "[Epoch 32/51] [Batch 400/476] [D loss: 0.182955] [G loss: 1.012065, adv: 0.674350, cycle: 0.023147, identity: 0.021249] ETA: 0:45:48.704058\n",
      "[Epoch 33/51] [Batch 0/476] [D loss: 0.129048] [G loss: 0.928447, adv: 0.526162, cycle: 0.026790, identity: 0.026877] ETA: 1:16:20.681757\n",
      "[Epoch 33/51] [Batch 100/476] [D loss: 0.125999] [G loss: 1.103187, adv: 0.620451, cycle: 0.031044, identity: 0.034459] ETA: 0:45:01.124477\n",
      "[Epoch 33/51] [Batch 200/476] [D loss: 0.223325] [G loss: 1.032388, adv: 0.698878, cycle: 0.022301, identity: 0.022101] ETA: 0:44:24.835270\n",
      "[Epoch 33/51] [Batch 300/476] [D loss: 0.128499] [G loss: 1.475287, adv: 1.104030, cycle: 0.025263, identity: 0.023726] ETA: 0:44:10.807806\n",
      "[Epoch 33/51] [Batch 400/476] [D loss: 0.131394] [G loss: 0.789395, adv: 0.463420, cycle: 0.022010, identity: 0.021176] ETA: 0:42:21.020060\n",
      "[Epoch 34/51] [Batch 0/476] [D loss: 0.160724] [G loss: 1.210066, adv: 0.819269, cycle: 0.027026, identity: 0.024108] ETA: 1:12:19.459400\n",
      "[Epoch 34/51] [Batch 100/476] [D loss: 0.142199] [G loss: 0.937027, adv: 0.596002, cycle: 0.023212, identity: 0.021781] ETA: 0:42:22.020636\n",
      "[Epoch 34/51] [Batch 200/476] [D loss: 0.095956] [G loss: 1.002615, adv: 0.710510, cycle: 0.019906, identity: 0.018610] ETA: 0:41:55.468878\n",
      "[Epoch 34/51] [Batch 300/476] [D loss: 0.130145] [G loss: 0.934164, adv: 0.611327, cycle: 0.022008, identity: 0.020551] ETA: 0:40:40.580704\n",
      "[Epoch 34/51] [Batch 400/476] [D loss: 0.105519] [G loss: 0.735706, adv: 0.360082, cycle: 0.024940, identity: 0.025245] ETA: 0:40:46.836016\n",
      "[Epoch 35/51] [Batch 0/476] [D loss: 0.075709] [G loss: 0.522697, adv: 0.226110, cycle: 0.020194, identity: 0.018929] ETA: 1:06:42.522583\n",
      "[Epoch 35/51] [Batch 100/476] [D loss: 0.182460] [G loss: 0.695447, adv: 0.292144, cycle: 0.027550, identity: 0.025562] ETA: 0:39:47.755464\n",
      "[Epoch 35/51] [Batch 200/476] [D loss: 0.100728] [G loss: 1.061879, adv: 0.738208, cycle: 0.021802, identity: 0.021130] ETA: 0:39:25.412304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/51] [Batch 300/476] [D loss: 0.217498] [G loss: 0.587641, adv: 0.271805, cycle: 0.021690, identity: 0.019788] ETA: 0:38:36.193889\n",
      "[Epoch 35/51] [Batch 400/476] [D loss: 0.096692] [G loss: 1.215003, adv: 0.662103, cycle: 0.037333, identity: 0.035914] ETA: 0:38:20.052860\n",
      "[Epoch 36/51] [Batch 0/476] [D loss: 0.128613] [G loss: 1.083854, adv: 0.651681, cycle: 0.029386, identity: 0.027662] ETA: 1:02:45.436950\n",
      "[Epoch 36/51] [Batch 100/476] [D loss: 0.144258] [G loss: 0.925100, adv: 0.596576, cycle: 0.022229, identity: 0.021246] ETA: 0:37:15.318298\n",
      "[Epoch 36/51] [Batch 200/476] [D loss: 0.143986] [G loss: 0.815032, adv: 0.465880, cycle: 0.023546, identity: 0.022739] ETA: 0:36:48.993788\n",
      "[Epoch 36/51] [Batch 300/476] [D loss: 0.081801] [G loss: 1.384680, adv: 0.902984, cycle: 0.030704, identity: 0.034931] ETA: 0:35:47.928858\n",
      "[Epoch 36/51] [Batch 400/476] [D loss: 0.107789] [G loss: 0.816736, adv: 0.482466, cycle: 0.022523, identity: 0.021807] ETA: 0:35:51.901593\n",
      "[Epoch 37/51] [Batch 0/476] [D loss: 0.046676] [G loss: 1.107179, adv: 0.527084, cycle: 0.039422, identity: 0.037175] ETA: 0:56:57.778879\n",
      "[Epoch 37/51] [Batch 100/476] [D loss: 0.172749] [G loss: 0.761653, adv: 0.382995, cycle: 0.025909, identity: 0.023914] ETA: 0:34:48.039539\n",
      "[Epoch 37/51] [Batch 200/476] [D loss: 0.183865] [G loss: 0.720413, adv: 0.383256, cycle: 0.023190, identity: 0.021051] ETA: 0:34:18.061478\n",
      "[Epoch 37/51] [Batch 300/476] [D loss: 0.064232] [G loss: 1.021183, adv: 0.695309, cycle: 0.022331, identity: 0.020513] ETA: 0:33:56.168530\n",
      "[Epoch 37/51] [Batch 400/476] [D loss: 0.132764] [G loss: 1.041027, adv: 0.559347, cycle: 0.032509, identity: 0.031318] ETA: 0:33:15.814562\n",
      "[Epoch 38/51] [Batch 0/476] [D loss: 0.169663] [G loss: 0.576418, adv: 0.270357, cycle: 0.020916, identity: 0.019379] ETA: 0:54:34.313867\n",
      "[Epoch 38/51] [Batch 100/476] [D loss: 0.117199] [G loss: 1.163259, adv: 0.782013, cycle: 0.026564, identity: 0.023121] ETA: 0:32:21.469666\n",
      "[Epoch 38/51] [Batch 200/476] [D loss: 0.114604] [G loss: 1.069704, adv: 0.739087, cycle: 0.022002, identity: 0.022120] ETA: 0:31:34.913306\n",
      "[Epoch 38/51] [Batch 300/476] [D loss: 0.103302] [G loss: 0.994330, adv: 0.707872, cycle: 0.019018, identity: 0.019255] ETA: 0:30:50.858459\n",
      "[Epoch 38/51] [Batch 400/476] [D loss: 0.154856] [G loss: 1.392516, adv: 0.903644, cycle: 0.031693, identity: 0.034389] ETA: 0:30:45.919404\n",
      "[Epoch 39/51] [Batch 0/476] [D loss: 0.085114] [G loss: 1.021130, adv: 0.702883, cycle: 0.020593, identity: 0.022463] ETA: 0:49:20.047829\n",
      "[Epoch 39/51] [Batch 100/476] [D loss: 0.141246] [G loss: 0.762668, adv: 0.335885, cycle: 0.029845, identity: 0.025666] ETA: 0:29:42.687036\n",
      "[Epoch 39/51] [Batch 200/476] [D loss: 0.084186] [G loss: 1.013667, adv: 0.675953, cycle: 0.022663, identity: 0.022218] ETA: 0:29:20.152090\n",
      "[Epoch 39/51] [Batch 300/476] [D loss: 0.074114] [G loss: 1.201401, adv: 0.826156, cycle: 0.026065, identity: 0.022918] ETA: 0:28:42.750615\n",
      "[Epoch 39/51] [Batch 400/476] [D loss: 0.179118] [G loss: 0.905335, adv: 0.572973, cycle: 0.022437, identity: 0.021597] ETA: 0:27:56.493362\n",
      "[Epoch 40/51] [Batch 0/476] [D loss: 0.066738] [G loss: 0.648127, adv: 0.348221, cycle: 0.019844, identity: 0.020292] ETA: 0:45:05.848319\n",
      "[Epoch 40/51] [Batch 100/476] [D loss: 0.094890] [G loss: 0.664266, adv: 0.335428, cycle: 0.022528, identity: 0.020711] ETA: 0:27:15.890911\n",
      "[Epoch 40/51] [Batch 200/476] [D loss: 0.124055] [G loss: 1.146516, adv: 0.838738, cycle: 0.020850, identity: 0.019856] ETA: 0:26:50.216930\n",
      "[Epoch 40/51] [Batch 300/476] [D loss: 0.108748] [G loss: 0.702755, adv: 0.343986, cycle: 0.023461, identity: 0.024831] ETA: 0:26:18.598209\n",
      "[Epoch 40/51] [Batch 400/476] [D loss: 0.104707] [G loss: 1.302200, adv: 0.797217, cycle: 0.032336, identity: 0.036326] ETA: 0:25:47.673025\n",
      "[Epoch 41/51] [Batch 0/476] [D loss: 0.168190] [G loss: 0.948956, adv: 0.611988, cycle: 0.022546, identity: 0.022301] ETA: 0:41:15.516539\n",
      "[Epoch 41/51] [Batch 100/476] [D loss: 0.167289] [G loss: 1.384554, adv: 0.930560, cycle: 0.028949, identity: 0.032900] ETA: 0:24:45.594540\n",
      "[Epoch 41/51] [Batch 200/476] [D loss: 0.165175] [G loss: 1.749485, adv: 1.100246, cycle: 0.043560, identity: 0.042727] ETA: 0:24:06.339340\n",
      "[Epoch 41/51] [Batch 300/476] [D loss: 0.164827] [G loss: 0.953071, adv: 0.512066, cycle: 0.029607, identity: 0.028987] ETA: 0:23:21.631527\n",
      "[Epoch 41/51] [Batch 400/476] [D loss: 0.068750] [G loss: 1.193114, adv: 0.846311, cycle: 0.023638, identity: 0.022084] ETA: 0:23:04.157047\n",
      "[Epoch 42/51] [Batch 0/476] [D loss: 0.067392] [G loss: 1.024899, adv: 0.743128, cycle: 0.019565, identity: 0.017223] ETA: 0:37:28.638742\n",
      "[Epoch 42/51] [Batch 100/476] [D loss: 0.064695] [G loss: 0.864075, adv: 0.584437, cycle: 0.019451, identity: 0.017025] ETA: 0:22:11.950785\n",
      "[Epoch 42/51] [Batch 200/476] [D loss: 0.048345] [G loss: 1.111120, adv: 0.793823, cycle: 0.021463, identity: 0.020533] ETA: 0:21:48.476598\n",
      "[Epoch 42/51] [Batch 300/476] [D loss: 0.151890] [G loss: 0.874348, adv: 0.588301, cycle: 0.019619, identity: 0.017972] ETA: 0:21:09.905319\n",
      "[Epoch 42/51] [Batch 400/476] [D loss: 0.076488] [G loss: 1.107149, adv: 0.775281, cycle: 0.021812, identity: 0.022749] ETA: 0:20:34.664111\n",
      "[Epoch 43/51] [Batch 0/476] [D loss: 0.182970] [G loss: 0.540677, adv: 0.256419, cycle: 0.019468, identity: 0.017917] ETA: 0:34:02.807610\n",
      "[Epoch 43/51] [Batch 100/476] [D loss: 0.244282] [G loss: 0.925075, adv: 0.579560, cycle: 0.023032, identity: 0.023038] ETA: 0:19:44.858829\n",
      "[Epoch 43/51] [Batch 200/476] [D loss: 0.129678] [G loss: 0.803054, adv: 0.495476, cycle: 0.020736, identity: 0.020043] ETA: 0:19:05.864714\n",
      "[Epoch 43/51] [Batch 300/476] [D loss: 0.099895] [G loss: 1.002849, adv: 0.673616, cycle: 0.022373, identity: 0.021101] ETA: 0:18:35.111870\n",
      "[Epoch 43/51] [Batch 400/476] [D loss: 0.165853] [G loss: 0.650301, adv: 0.328989, cycle: 0.021964, identity: 0.020334] ETA: 0:18:12.013390\n",
      "[Epoch 44/51] [Batch 0/476] [D loss: 0.073105] [G loss: 0.865886, adv: 0.569414, cycle: 0.019750, identity: 0.019793] ETA: 0:28:56.863818\n",
      "[Epoch 44/51] [Batch 100/476] [D loss: 0.115319] [G loss: 0.986169, adv: 0.609022, cycle: 0.025783, identity: 0.023864] ETA: 0:17:05.919182\n",
      "[Epoch 44/51] [Batch 200/476] [D loss: 0.171997] [G loss: 1.111089, adv: 0.669963, cycle: 0.030106, identity: 0.028014] ETA: 0:16:40.582804\n",
      "[Epoch 44/51] [Batch 300/476] [D loss: 0.235152] [G loss: 1.065667, adv: 0.734037, cycle: 0.022322, identity: 0.021682] ETA: 0:15:41.823183\n",
      "[Epoch 44/51] [Batch 400/476] [D loss: 0.068323] [G loss: 1.124223, adv: 0.779214, cycle: 0.022834, identity: 0.023334] ETA: 0:15:31.248672\n",
      "[Epoch 45/51] [Batch 0/476] [D loss: 0.052701] [G loss: 1.131979, adv: 0.789798, cycle: 0.023801, identity: 0.020833] ETA: 0:25:10.638233\n",
      "[Epoch 45/51] [Batch 100/476] [D loss: 0.178031] [G loss: 0.722762, adv: 0.340653, cycle: 0.026089, identity: 0.024243] ETA: 0:14:42.643263\n",
      "[Epoch 45/51] [Batch 200/476] [D loss: 0.103441] [G loss: 1.270681, adv: 0.971717, cycle: 0.020008, identity: 0.019777] ETA: 0:14:05.626457\n",
      "[Epoch 45/51] [Batch 300/476] [D loss: 0.073205] [G loss: 1.106022, adv: 0.635819, cycle: 0.031336, identity: 0.031368] ETA: 0:13:27.625880\n",
      "[Epoch 45/51] [Batch 400/476] [D loss: 0.150974] [G loss: 0.906221, adv: 0.584486, cycle: 0.022021, identity: 0.020306] ETA: 0:12:46.637476\n",
      "[Epoch 46/51] [Batch 0/476] [D loss: 0.118353] [G loss: 1.194936, adv: 0.753710, cycle: 0.030457, identity: 0.027331] ETA: 0:21:00.177674\n",
      "[Epoch 46/51] [Batch 100/476] [D loss: 0.075189] [G loss: 0.711906, adv: 0.407336, cycle: 0.020572, identity: 0.019770] ETA: 0:12:03.559427\n",
      "[Epoch 46/51] [Batch 200/476] [D loss: 0.146480] [G loss: 0.899852, adv: 0.498389, cycle: 0.025458, identity: 0.029376] ETA: 0:11:34.093084\n",
      "[Epoch 46/51] [Batch 300/476] [D loss: 0.149098] [G loss: 0.649495, adv: 0.361735, cycle: 0.018846, identity: 0.019860] ETA: 0:11:04.536133\n",
      "[Epoch 46/51] [Batch 400/476] [D loss: 0.067315] [G loss: 1.246531, adv: 0.902214, cycle: 0.024927, identity: 0.019010] ETA: 0:10:34.318357\n",
      "[Epoch 47/51] [Batch 0/476] [D loss: 0.107520] [G loss: 1.110762, adv: 0.545135, cycle: 0.037186, identity: 0.038753] ETA: 0:16:38.729961\n",
      "[Epoch 47/51] [Batch 100/476] [D loss: 0.142419] [G loss: 1.044852, adv: 0.736187, cycle: 0.021115, identity: 0.019502] ETA: 0:09:24.378817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/51] [Batch 200/476] [D loss: 0.125004] [G loss: 1.129272, adv: 0.710858, cycle: 0.028587, identity: 0.026510] ETA: 0:09:06.691252\n",
      "[Epoch 47/51] [Batch 300/476] [D loss: 0.094156] [G loss: 0.934757, adv: 0.587440, cycle: 0.024043, identity: 0.021378] ETA: 0:08:30.225095\n",
      "[Epoch 47/51] [Batch 400/476] [D loss: 0.207027] [G loss: 0.917046, adv: 0.642664, cycle: 0.018951, identity: 0.016974] ETA: 0:07:55.959984\n",
      "[Epoch 48/51] [Batch 0/476] [D loss: 0.184001] [G loss: 0.692541, adv: 0.299993, cycle: 0.027013, identity: 0.024483] ETA: 0:12:34.202062\n",
      "[Epoch 48/51] [Batch 100/476] [D loss: 0.151685] [G loss: 0.668088, adv: 0.414656, cycle: 0.017454, identity: 0.015779] ETA: 0:06:57.651691\n",
      "[Epoch 48/51] [Batch 200/476] [D loss: 0.153612] [G loss: 0.819380, adv: 0.487566, cycle: 0.023056, identity: 0.020250] ETA: 0:06:30.241474\n",
      "[Epoch 48/51] [Batch 300/476] [D loss: 0.244825] [G loss: 1.160728, adv: 0.851578, cycle: 0.021121, identity: 0.019588] ETA: 0:05:59.518696\n",
      "[Epoch 48/51] [Batch 400/476] [D loss: 0.222339] [G loss: 0.757906, adv: 0.438877, cycle: 0.021076, identity: 0.021653] ETA: 0:05:22.647528\n",
      "[Epoch 49/51] [Batch 0/476] [D loss: 0.161501] [G loss: 0.699597, adv: 0.433621, cycle: 0.017825, identity: 0.017546] ETA: 0:08:16.362335\n",
      "[Epoch 49/51] [Batch 100/476] [D loss: 0.128135] [G loss: 0.727853, adv: 0.412751, cycle: 0.022697, identity: 0.017627] ETA: 0:04:33.000707\n",
      "[Epoch 49/51] [Batch 200/476] [D loss: 0.126248] [G loss: 0.951358, adv: 0.609876, cycle: 0.023320, identity: 0.021656] ETA: 0:03:57.773090\n",
      "[Epoch 49/51] [Batch 300/476] [D loss: 0.190450] [G loss: 0.881759, adv: 0.565696, cycle: 0.021223, identity: 0.020766] ETA: 0:03:28.216669\n",
      "[Epoch 49/51] [Batch 400/476] [D loss: 0.127079] [G loss: 0.830084, adv: 0.437977, cycle: 0.026465, identity: 0.025492] ETA: 0:02:56.380434\n",
      "[Epoch 50/51] [Batch 0/476] [D loss: 0.577038] [G loss: 0.719625, adv: 0.197143, cycle: 0.035061, identity: 0.034375] ETA: 0:04:13.064864\n",
      "[Epoch 50/51] [Batch 100/476] [D loss: 0.163394] [G loss: 1.144039, adv: 0.814620, cycle: 0.021956, identity: 0.021971] ETA: 0:01:59.639029\n",
      "[Epoch 50/51] [Batch 200/476] [D loss: 0.107368] [G loss: 1.000291, adv: 0.529824, cycle: 0.031874, identity: 0.030345] ETA: 0:01:28.421582\n",
      "[Epoch 50/51] [Batch 300/476] [D loss: 0.107241] [G loss: 1.150205, adv: 0.674951, cycle: 0.030855, identity: 0.033341] ETA: 0:00:56.366272\n",
      "[Epoch 50/51] [Batch 400/476] [D loss: 0.101993] [G loss: 1.041836, adv: 0.707159, cycle: 0.022660, identity: 0.021615] ETA: 0:00:23.844350\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"ganmodels/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"ganmodels/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"ganmodels/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"ganmodels/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "    \n",
    "epoch = 0\n",
    "dataset_name = 'CycleGANSS3'\n",
    "os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"ganmodels/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader3, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
