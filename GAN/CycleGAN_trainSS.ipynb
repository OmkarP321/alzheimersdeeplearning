{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from CycleGAN_utils import *\n",
    "from CycleGAN_models import *\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 51\n",
    "batch_size = 1\n",
    "lr=0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 50\n",
    "n_cpu = 8\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "channels = 1\n",
    "sample_interval = 100\n",
    "checkpoint_interval = 25\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"../datasets/SSdataset.pt\")\n",
    "\n",
    "dataset = [sample for sample in dataset if sample != None]\n",
    "\n",
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "        \n",
    "def process_gan(dataset, s):\n",
    "    output = []\n",
    "    dataset = [sample[0] for sample in dataset]\n",
    "    for sample in dataset:\n",
    "        sample = sample[s][0]\n",
    "        sample /= torch.max(sample)\n",
    "        output.append(torch.unsqueeze(sample, 0))\n",
    "    return output\n",
    "\n",
    "        \n",
    "NCgan1 = process_gan(NC, 0)\n",
    "NCgan2 = process_gan(NC, 1)\n",
    "NCgan3 = process_gan(NC, 2)\n",
    "\n",
    "ADgan1 = process_gan(AD, 0)\n",
    "ADgan2 = process_gan(AD, 1)\n",
    "ADgan3 = process_gan(AD, 2)\n",
    "\n",
    "gan1 = []\n",
    "for i in range(len(ADgan1)):\n",
    "    gan1.append({\"A\": NCgan1[i], \"B\": ADgan1[i]})\n",
    "\n",
    "gan2 = []\n",
    "for i in range(len(ADgan2)):\n",
    "    gan2.append({\"A\": NCgan2[i], \"B\": ADgan2[i]})\n",
    "    \n",
    "gan3 = []\n",
    "for i in range(len(ADgan3)):\n",
    "    gan3.append({\"A\": NCgan3[i], \"B\": ADgan3[i]})\n",
    "\n",
    "batch_size = 1\n",
    "dataloader1 = DataLoader(gan1, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader2 = DataLoader(gan2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader3 = DataLoader(gan3, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "        \n",
    "def sample_images(batches_done, dataloader):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=4, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=4, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=4, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=4, normalize=True)\n",
    "\n",
    "    image_grid = torch.stack((real_A, fake_B, real_B, fake_A), 0)\n",
    "    save_image(image_grid, \"ganimages/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n",
    "    \n",
    "def train_gan(dataloader, epoch):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "                \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "           \n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "            \n",
    "                print(\n",
    "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        n_epochs,\n",
    "                        i,\n",
    "                        len(dataloader),\n",
    "                        loss_D.item(),\n",
    "                        loss_G.item(),\n",
    "                        loss_GAN.item(),\n",
    "                        loss_cycle.item(),\n",
    "                        loss_identity.item(),\n",
    "                        time_left,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, dataloader)\n",
    "                \n",
    "            \n",
    "            G_losses.append(loss_G.item())\n",
    "            D_losses.append(loss_D.item())\n",
    "            \n",
    "            \n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/51] [Batch 0/476] [D loss: 2.168109] [G loss: 6.816429, adv: 1.470208, cycle: 0.383003, identity: 0.303239] ETA: 5:32:35.700311\n",
      "[Epoch 0/51] [Batch 100/476] [D loss: 0.264667] [G loss: 1.121777, adv: 0.322583, cycle: 0.054034, identity: 0.051771] ETA: 2:04:17.726608\n",
      "[Epoch 0/51] [Batch 200/476] [D loss: 0.272156] [G loss: 1.007270, adv: 0.233533, cycle: 0.052532, identity: 0.049683] ETA: 2:04:43.689384\n",
      "[Epoch 0/51] [Batch 300/476] [D loss: 0.239394] [G loss: 0.828983, adv: 0.240991, cycle: 0.040091, identity: 0.037417] ETA: 2:04:24.164183\n",
      "[Epoch 0/51] [Batch 400/476] [D loss: 0.257638] [G loss: 0.626128, adv: 0.215599, cycle: 0.028597, identity: 0.024912] ETA: 2:04:00.187817\n",
      "[Epoch 1/51] [Batch 0/476] [D loss: 0.300588] [G loss: 0.736618, adv: 0.301798, cycle: 0.030062, identity: 0.026840] ETA: 3:55:22.494459\n",
      "[Epoch 1/51] [Batch 100/476] [D loss: 0.283368] [G loss: 0.689448, adv: 0.271335, cycle: 0.027733, identity: 0.028156] ETA: 2:04:34.982929\n",
      "[Epoch 1/51] [Batch 200/476] [D loss: 0.260351] [G loss: 0.795816, adv: 0.269806, cycle: 0.037724, identity: 0.029755] ETA: 2:03:16.212578\n",
      "[Epoch 1/51] [Batch 300/476] [D loss: 0.252402] [G loss: 0.956488, adv: 0.326007, cycle: 0.044094, identity: 0.037909] ETA: 2:02:56.184821\n",
      "[Epoch 1/51] [Batch 400/476] [D loss: 0.244874] [G loss: 0.605339, adv: 0.299449, cycle: 0.021104, identity: 0.018970] ETA: 2:02:42.822533\n",
      "[Epoch 2/51] [Batch 0/476] [D loss: 0.296233] [G loss: 0.631782, adv: 0.230276, cycle: 0.026789, identity: 0.026724] ETA: 3:29:52.417790\n",
      "[Epoch 2/51] [Batch 100/476] [D loss: 0.233072] [G loss: 0.611285, adv: 0.281037, cycle: 0.022386, identity: 0.021277] ETA: 2:02:13.479168\n",
      "[Epoch 2/51] [Batch 200/476] [D loss: 0.297578] [G loss: 0.965485, adv: 0.272104, cycle: 0.048800, identity: 0.041076] ETA: 2:02:22.027016\n",
      "[Epoch 2/51] [Batch 300/476] [D loss: 0.288821] [G loss: 0.834489, adv: 0.291757, cycle: 0.036179, identity: 0.036188] ETA: 2:01:04.648865\n",
      "[Epoch 2/51] [Batch 400/476] [D loss: 0.245899] [G loss: 0.712623, adv: 0.267789, cycle: 0.029681, identity: 0.029604] ETA: 1:59:50.580176\n",
      "[Epoch 3/51] [Batch 0/476] [D loss: 0.246724] [G loss: 0.556560, adv: 0.327385, cycle: 0.015440, identity: 0.014954] ETA: 3:21:07.325592\n",
      "[Epoch 3/51] [Batch 100/476] [D loss: 0.275745] [G loss: 0.712523, adv: 0.331255, cycle: 0.026295, identity: 0.023663] ETA: 2:00:43.373255\n",
      "[Epoch 3/51] [Batch 200/476] [D loss: 0.237085] [G loss: 0.823754, adv: 0.261810, cycle: 0.037730, identity: 0.036929] ETA: 1:58:53.273542\n",
      "[Epoch 3/51] [Batch 300/476] [D loss: 0.298800] [G loss: 0.757394, adv: 0.350286, cycle: 0.026781, identity: 0.027859] ETA: 1:58:44.140866\n",
      "[Epoch 3/51] [Batch 400/476] [D loss: 0.282769] [G loss: 0.823495, adv: 0.347749, cycle: 0.031454, identity: 0.032241] ETA: 1:59:29.100719\n",
      "[Epoch 4/51] [Batch 0/476] [D loss: 0.271157] [G loss: 0.728381, adv: 0.304549, cycle: 0.029094, identity: 0.026578] ETA: 3:16:48.215489\n",
      "[Epoch 4/51] [Batch 100/476] [D loss: 0.232335] [G loss: 0.705842, adv: 0.299707, cycle: 0.028439, identity: 0.024348] ETA: 1:57:46.355164\n",
      "[Epoch 4/51] [Batch 200/476] [D loss: 0.267401] [G loss: 0.593478, adv: 0.305167, cycle: 0.019402, identity: 0.018859] ETA: 1:55:35.215043\n",
      "[Epoch 4/51] [Batch 300/476] [D loss: 0.271030] [G loss: 0.907675, adv: 0.369771, cycle: 0.035266, identity: 0.037048] ETA: 1:59:48.151489\n",
      "[Epoch 4/51] [Batch 400/476] [D loss: 0.243220] [G loss: 0.625066, adv: 0.298288, cycle: 0.022574, identity: 0.020208] ETA: 1:56:14.551432\n",
      "[Epoch 5/51] [Batch 0/476] [D loss: 0.253429] [G loss: 0.522170, adv: 0.264087, cycle: 0.017873, identity: 0.015870] ETA: 3:20:35.088406\n",
      "[Epoch 5/51] [Batch 100/476] [D loss: 0.240762] [G loss: 1.116863, adv: 0.261764, cycle: 0.055822, identity: 0.059377] ETA: 1:55:39.548150\n",
      "[Epoch 5/51] [Batch 200/476] [D loss: 0.259195] [G loss: 0.883126, adv: 0.266713, cycle: 0.045129, identity: 0.033025] ETA: 2:05:25.700684\n",
      "[Epoch 5/51] [Batch 300/476] [D loss: 0.268219] [G loss: 0.615384, adv: 0.280488, cycle: 0.023084, identity: 0.020811] ETA: 1:57:37.364027\n",
      "[Epoch 5/51] [Batch 400/476] [D loss: 0.262090] [G loss: 0.514026, adv: 0.325471, cycle: 0.013276, identity: 0.011158] ETA: 1:54:26.167328\n",
      "[Epoch 6/51] [Batch 0/476] [D loss: 0.252099] [G loss: 0.406072, adv: 0.199107, cycle: 0.013663, identity: 0.014066] ETA: 3:16:14.201660\n",
      "[Epoch 6/51] [Batch 100/476] [D loss: 0.262658] [G loss: 0.621955, adv: 0.317777, cycle: 0.022035, identity: 0.016765] ETA: 1:52:47.364025\n",
      "[Epoch 6/51] [Batch 200/476] [D loss: 0.272613] [G loss: 0.600140, adv: 0.296837, cycle: 0.020199, identity: 0.020263] ETA: 1:52:56.237764\n",
      "[Epoch 6/51] [Batch 300/476] [D loss: 0.327177] [G loss: 0.469605, adv: 0.204914, cycle: 0.018471, identity: 0.015996] ETA: 1:51:35.274811\n",
      "[Epoch 6/51] [Batch 400/476] [D loss: 0.225612] [G loss: 0.675942, adv: 0.248904, cycle: 0.029446, identity: 0.026515] ETA: 1:51:17.360497\n",
      "[Epoch 7/51] [Batch 0/476] [D loss: 0.242179] [G loss: 0.533334, adv: 0.298812, cycle: 0.016799, identity: 0.013306] ETA: 3:05:36.217232\n",
      "[Epoch 7/51] [Batch 100/476] [D loss: 0.223117] [G loss: 0.677406, adv: 0.236619, cycle: 0.029460, identity: 0.029238] ETA: 1:50:46.542598\n",
      "[Epoch 7/51] [Batch 200/476] [D loss: 0.278701] [G loss: 0.848232, adv: 0.240405, cycle: 0.041290, identity: 0.038986] ETA: 1:48:21.204845\n",
      "[Epoch 7/51] [Batch 300/476] [D loss: 0.251613] [G loss: 0.715843, adv: 0.246677, cycle: 0.030969, identity: 0.031894] ETA: 1:50:09.779901\n",
      "[Epoch 7/51] [Batch 400/476] [D loss: 0.258041] [G loss: 0.460011, adv: 0.215926, cycle: 0.016042, identity: 0.016733] ETA: 1:47:27.576141\n",
      "[Epoch 8/51] [Batch 0/476] [D loss: 0.237150] [G loss: 0.554836, adv: 0.278990, cycle: 0.018385, identity: 0.018398] ETA: 3:18:05.443899\n",
      "[Epoch 8/51] [Batch 100/476] [D loss: 0.253725] [G loss: 0.550729, adv: 0.259107, cycle: 0.020745, identity: 0.016835] ETA: 1:51:18.369610\n",
      "[Epoch 8/51] [Batch 200/476] [D loss: 0.204234] [G loss: 0.917296, adv: 0.316225, cycle: 0.040265, identity: 0.039685] ETA: 1:47:40.746829\n",
      "[Epoch 8/51] [Batch 300/476] [D loss: 0.256220] [G loss: 0.563512, adv: 0.263684, cycle: 0.020702, identity: 0.018561] ETA: 1:46:51.410847\n",
      "[Epoch 8/51] [Batch 400/476] [D loss: 0.253998] [G loss: 0.455558, adv: 0.266937, cycle: 0.012571, identity: 0.012582] ETA: 1:46:35.687462\n",
      "[Epoch 9/51] [Batch 0/476] [D loss: 0.245029] [G loss: 0.436294, adv: 0.268085, cycle: 0.011308, identity: 0.011025] ETA: 2:56:07.437098\n",
      "[Epoch 9/51] [Batch 100/476] [D loss: 0.199825] [G loss: 1.004988, adv: 0.271207, cycle: 0.050623, identity: 0.045510] ETA: 1:46:13.975392\n",
      "[Epoch 9/51] [Batch 200/476] [D loss: 0.246019] [G loss: 0.656938, adv: 0.242412, cycle: 0.029290, identity: 0.024325] ETA: 1:45:43.376431\n",
      "[Epoch 9/51] [Batch 300/476] [D loss: 0.262334] [G loss: 0.640348, adv: 0.243294, cycle: 0.028469, identity: 0.022472] ETA: 1:46:49.990362\n",
      "[Epoch 9/51] [Batch 400/476] [D loss: 0.242526] [G loss: 0.583104, adv: 0.281328, cycle: 0.021267, identity: 0.017822] ETA: 1:44:00.122889\n",
      "[Epoch 10/51] [Batch 0/476] [D loss: 0.202146] [G loss: 0.981653, adv: 0.354001, cycle: 0.042711, identity: 0.040109] ETA: 2:54:32.059808\n",
      "[Epoch 10/51] [Batch 100/476] [D loss: 0.239413] [G loss: 0.828905, adv: 0.233685, cycle: 0.039195, identity: 0.040655] ETA: 1:43:12.167238\n",
      "[Epoch 10/51] [Batch 200/476] [D loss: 0.284725] [G loss: 0.673380, adv: 0.373072, cycle: 0.019184, identity: 0.021694] ETA: 1:42:24.957948\n",
      "[Epoch 10/51] [Batch 300/476] [D loss: 0.273835] [G loss: 0.453848, adv: 0.216660, cycle: 0.015820, identity: 0.015797] ETA: 1:41:39.487854\n",
      "[Epoch 10/51] [Batch 400/476] [D loss: 0.257853] [G loss: 0.599585, adv: 0.269591, cycle: 0.021530, identity: 0.022939] ETA: 1:42:03.841198\n",
      "[Epoch 11/51] [Batch 0/476] [D loss: 0.247980] [G loss: 0.576104, adv: 0.283017, cycle: 0.020781, identity: 0.017055] ETA: 2:51:40.751381\n",
      "[Epoch 11/51] [Batch 100/476] [D loss: 0.232864] [G loss: 0.840527, adv: 0.276568, cycle: 0.037873, identity: 0.037045] ETA: 1:41:04.551239\n",
      "[Epoch 11/51] [Batch 200/476] [D loss: 0.238076] [G loss: 0.518742, adv: 0.240359, cycle: 0.019705, identity: 0.016267] ETA: 1:40:36.825600\n",
      "[Epoch 11/51] [Batch 300/476] [D loss: 0.259724] [G loss: 0.487970, adv: 0.228447, cycle: 0.017846, identity: 0.016212] ETA: 1:39:30.089259\n",
      "[Epoch 11/51] [Batch 400/476] [D loss: 0.243539] [G loss: 0.549549, adv: 0.250287, cycle: 0.020594, identity: 0.018664] ETA: 1:39:31.989346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/51] [Batch 0/476] [D loss: 0.259171] [G loss: 0.421254, adv: 0.247987, cycle: 0.011151, identity: 0.012351] ETA: 2:47:47.796910\n",
      "[Epoch 12/51] [Batch 100/476] [D loss: 0.269078] [G loss: 0.513436, adv: 0.306977, cycle: 0.013379, identity: 0.014533] ETA: 1:38:50.700874\n",
      "[Epoch 12/51] [Batch 200/476] [D loss: 0.264578] [G loss: 0.532459, adv: 0.324038, cycle: 0.013962, identity: 0.013760] ETA: 1:38:04.517367\n",
      "[Epoch 12/51] [Batch 300/476] [D loss: 0.250814] [G loss: 0.804241, adv: 0.399250, cycle: 0.027677, identity: 0.025644] ETA: 1:37:06.938948\n",
      "[Epoch 12/51] [Batch 400/476] [D loss: 0.220536] [G loss: 0.806691, adv: 0.256897, cycle: 0.036010, identity: 0.037940] ETA: 1:36:10.545248\n",
      "[Epoch 13/51] [Batch 0/476] [D loss: 0.247118] [G loss: 0.656712, adv: 0.244928, cycle: 0.027622, identity: 0.027114] ETA: 3:02:35.207575\n",
      "[Epoch 13/51] [Batch 100/476] [D loss: 0.277555] [G loss: 0.779757, adv: 0.285987, cycle: 0.034668, identity: 0.029417] ETA: 1:55:14.992661\n",
      "[Epoch 13/51] [Batch 200/476] [D loss: 0.264270] [G loss: 0.488508, adv: 0.208064, cycle: 0.018675, identity: 0.018739] ETA: 1:35:08.191277\n",
      "[Epoch 13/51] [Batch 300/476] [D loss: 0.334438] [G loss: 0.867663, adv: 0.579324, cycle: 0.020154, identity: 0.017361] ETA: 2:02:31.844680\n",
      "[Epoch 13/51] [Batch 400/476] [D loss: 0.332251] [G loss: 0.706851, adv: 0.281143, cycle: 0.030987, identity: 0.023169] ETA: 1:34:13.668629\n",
      "[Epoch 14/51] [Batch 0/476] [D loss: 0.270619] [G loss: 0.486802, adv: 0.263574, cycle: 0.015477, identity: 0.013692] ETA: 2:42:35.710962\n",
      "[Epoch 14/51] [Batch 100/476] [D loss: 0.250633] [G loss: 0.467720, adv: 0.239308, cycle: 0.015223, identity: 0.015236] ETA: 1:33:59.144205\n",
      "[Epoch 14/51] [Batch 200/476] [D loss: 0.216262] [G loss: 0.524463, adv: 0.301179, cycle: 0.015182, identity: 0.014293] ETA: 1:36:04.984282\n",
      "[Epoch 14/51] [Batch 300/476] [D loss: 0.343856] [G loss: 0.510924, adv: 0.252576, cycle: 0.017663, identity: 0.016343] ETA: 1:38:22.105606\n",
      "[Epoch 14/51] [Batch 400/476] [D loss: 0.198984] [G loss: 0.704923, adv: 0.270800, cycle: 0.029653, identity: 0.027519] ETA: 1:37:46.654725\n",
      "[Epoch 15/51] [Batch 0/476] [D loss: 0.240144] [G loss: 0.476359, adv: 0.286428, cycle: 0.012851, identity: 0.012284] ETA: 2:41:45.010082\n",
      "[Epoch 15/51] [Batch 100/476] [D loss: 0.285045] [G loss: 0.620436, adv: 0.316091, cycle: 0.020148, identity: 0.020573] ETA: 1:42:08.461912\n",
      "[Epoch 15/51] [Batch 200/476] [D loss: 0.245916] [G loss: 0.552123, adv: 0.261692, cycle: 0.019616, identity: 0.018853] ETA: 1:40:14.279062\n",
      "[Epoch 15/51] [Batch 300/476] [D loss: 0.255585] [G loss: 0.574568, adv: 0.313064, cycle: 0.017846, identity: 0.016609] ETA: 1:41:33.941397\n",
      "[Epoch 15/51] [Batch 400/476] [D loss: 0.239750] [G loss: 0.766698, adv: 0.308434, cycle: 0.030716, identity: 0.030220] ETA: 1:40:42.933975\n",
      "[Epoch 16/51] [Batch 0/476] [D loss: 0.281325] [G loss: 0.502287, adv: 0.169086, cycle: 0.023270, identity: 0.020100] ETA: 2:32:04.565616\n",
      "[Epoch 16/51] [Batch 100/476] [D loss: 0.197239] [G loss: 0.693730, adv: 0.452154, cycle: 0.015795, identity: 0.016725] ETA: 1:31:15.245705\n",
      "[Epoch 16/51] [Batch 200/476] [D loss: 0.250883] [G loss: 0.493003, adv: 0.195335, cycle: 0.020497, identity: 0.018539] ETA: 1:29:33.631735\n",
      "[Epoch 16/51] [Batch 300/476] [D loss: 0.232312] [G loss: 0.530542, adv: 0.295568, cycle: 0.016517, identity: 0.013961] ETA: 1:29:45.170307\n",
      "[Epoch 16/51] [Batch 400/476] [D loss: 0.233006] [G loss: 1.303223, adv: 0.642048, cycle: 0.045947, identity: 0.040340] ETA: 1:26:23.420062\n",
      "[Epoch 17/51] [Batch 0/476] [D loss: 0.250221] [G loss: 0.519127, adv: 0.256534, cycle: 0.018781, identity: 0.014956] ETA: 2:31:46.019644\n",
      "[Epoch 17/51] [Batch 100/476] [D loss: 0.224091] [G loss: 0.553817, adv: 0.369833, cycle: 0.012481, identity: 0.011835] ETA: 1:25:28.464580\n",
      "[Epoch 17/51] [Batch 200/476] [D loss: 0.251508] [G loss: 0.422358, adv: 0.210924, cycle: 0.014110, identity: 0.014066] ETA: 1:31:38.893948\n",
      "[Epoch 17/51] [Batch 300/476] [D loss: 0.279634] [G loss: 0.660217, adv: 0.391349, cycle: 0.018987, identity: 0.015800] ETA: 1:37:57.172858\n",
      "[Epoch 17/51] [Batch 400/476] [D loss: 0.239518] [G loss: 0.416464, adv: 0.252712, cycle: 0.010471, identity: 0.011808] ETA: 1:32:21.276442\n",
      "[Epoch 18/51] [Batch 0/476] [D loss: 0.258017] [G loss: 0.441102, adv: 0.245803, cycle: 0.012190, identity: 0.014680] ETA: 2:30:15.214188\n",
      "[Epoch 18/51] [Batch 100/476] [D loss: 0.252037] [G loss: 0.829811, adv: 0.482483, cycle: 0.024243, identity: 0.020979] ETA: 1:32:45.794914\n",
      "[Epoch 18/51] [Batch 200/476] [D loss: 0.345232] [G loss: 0.470731, adv: 0.244331, cycle: 0.014711, identity: 0.015859] ETA: 1:32:39.987799\n",
      "[Epoch 18/51] [Batch 300/476] [D loss: 0.264089] [G loss: 0.607888, adv: 0.303413, cycle: 0.022376, identity: 0.016143] ETA: 1:22:06.830315\n",
      "[Epoch 18/51] [Batch 400/476] [D loss: 0.201754] [G loss: 0.460470, adv: 0.237110, cycle: 0.014697, identity: 0.015277] ETA: 1:21:11.828490\n",
      "[Epoch 19/51] [Batch 0/476] [D loss: 0.140437] [G loss: 1.100635, adv: 0.491778, cycle: 0.044228, identity: 0.033316] ETA: 2:16:38.002533\n",
      "[Epoch 19/51] [Batch 100/476] [D loss: 0.293221] [G loss: 0.519481, adv: 0.198169, cycle: 0.022716, identity: 0.018831] ETA: 1:20:30.676151\n",
      "[Epoch 19/51] [Batch 200/476] [D loss: 0.253360] [G loss: 0.574650, adv: 0.292931, cycle: 0.018922, identity: 0.018499] ETA: 1:19:59.996187\n",
      "[Epoch 19/51] [Batch 300/476] [D loss: 0.201657] [G loss: 0.712378, adv: 0.301153, cycle: 0.026700, identity: 0.028845] ETA: 1:20:02.821260\n",
      "[Epoch 19/51] [Batch 400/476] [D loss: 0.262225] [G loss: 0.506984, adv: 0.262907, cycle: 0.017520, identity: 0.013775] ETA: 1:19:00.849804\n",
      "[Epoch 20/51] [Batch 0/476] [D loss: 0.210852] [G loss: 1.280509, adv: 0.648539, cycle: 0.040340, identity: 0.045715] ETA: 2:11:05.612809\n",
      "[Epoch 20/51] [Batch 100/476] [D loss: 0.279158] [G loss: 0.672698, adv: 0.394461, cycle: 0.018403, identity: 0.018842] ETA: 1:18:13.762711\n",
      "[Epoch 20/51] [Batch 200/476] [D loss: 0.204945] [G loss: 0.556404, adv: 0.199419, cycle: 0.024437, identity: 0.022523] ETA: 1:17:52.789782\n",
      "[Epoch 20/51] [Batch 300/476] [D loss: 0.198680] [G loss: 0.806733, adv: 0.456527, cycle: 0.023396, identity: 0.023250] ETA: 1:16:59.766891\n",
      "[Epoch 20/51] [Batch 400/476] [D loss: 0.328290] [G loss: 0.909587, adv: 0.533540, cycle: 0.024188, identity: 0.026833] ETA: 1:16:12.968468\n",
      "[Epoch 21/51] [Batch 0/476] [D loss: 0.206281] [G loss: 0.577619, adv: 0.338435, cycle: 0.016433, identity: 0.014971] ETA: 2:07:22.684937\n",
      "[Epoch 21/51] [Batch 100/476] [D loss: 0.127502] [G loss: 0.941622, adv: 0.375257, cycle: 0.037386, identity: 0.038501] ETA: 1:15:15.062804\n",
      "[Epoch 21/51] [Batch 200/476] [D loss: 0.270373] [G loss: 0.436680, adv: 0.134100, cycle: 0.020288, identity: 0.019940] ETA: 1:17:53.794861\n",
      "[Epoch 21/51] [Batch 300/476] [D loss: 0.220755] [G loss: 0.731691, adv: 0.405828, cycle: 0.022574, identity: 0.020026] ETA: 1:14:32.232499\n",
      "[Epoch 21/51] [Batch 400/476] [D loss: 0.181829] [G loss: 0.590600, adv: 0.344245, cycle: 0.016516, identity: 0.016239] ETA: 1:13:34.996014\n",
      "[Epoch 22/51] [Batch 0/476] [D loss: 0.266319] [G loss: 0.504466, adv: 0.209463, cycle: 0.020393, identity: 0.018215] ETA: 2:05:31.283815\n",
      "[Epoch 22/51] [Batch 100/476] [D loss: 0.247268] [G loss: 0.463565, adv: 0.207471, cycle: 0.017541, identity: 0.016137] ETA: 1:12:51.801521\n",
      "[Epoch 22/51] [Batch 200/476] [D loss: 0.287920] [G loss: 0.663753, adv: 0.396954, cycle: 0.018570, identity: 0.016219] ETA: 1:12:49.113597\n",
      "[Epoch 22/51] [Batch 300/476] [D loss: 0.202837] [G loss: 0.496273, adv: 0.249719, cycle: 0.017050, identity: 0.015211] ETA: 1:11:14.778351\n",
      "[Epoch 22/51] [Batch 400/476] [D loss: 0.211493] [G loss: 0.572536, adv: 0.348783, cycle: 0.014813, identity: 0.015125] ETA: 1:11:13.770069\n",
      "[Epoch 23/51] [Batch 0/476] [D loss: 0.191902] [G loss: 1.019356, adv: 0.453387, cycle: 0.043075, identity: 0.027044] ETA: 1:58:12.187370\n",
      "[Epoch 23/51] [Batch 100/476] [D loss: 0.202757] [G loss: 1.218149, adv: 0.755008, cycle: 0.029788, identity: 0.033052] ETA: 1:09:55.450962\n",
      "[Epoch 23/51] [Batch 200/476] [D loss: 0.277332] [G loss: 0.874265, adv: 0.499091, cycle: 0.025468, identity: 0.024099] ETA: 1:09:44.057344\n",
      "[Epoch 23/51] [Batch 300/476] [D loss: 0.127468] [G loss: 0.881176, adv: 0.342814, cycle: 0.036203, identity: 0.035267] ETA: 1:08:48.852944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/51] [Batch 400/476] [D loss: 0.278130] [G loss: 0.701702, adv: 0.433667, cycle: 0.018476, identity: 0.016656] ETA: 1:08:42.839233\n",
      "[Epoch 24/51] [Batch 0/476] [D loss: 0.178935] [G loss: 1.154909, adv: 0.399584, cycle: 0.050506, identity: 0.050053] ETA: 1:54:06.745331\n",
      "[Epoch 24/51] [Batch 100/476] [D loss: 0.192469] [G loss: 1.003663, adv: 0.341423, cycle: 0.044880, identity: 0.042689] ETA: 1:08:04.138550\n",
      "[Epoch 24/51] [Batch 200/476] [D loss: 0.214090] [G loss: 0.595402, adv: 0.289697, cycle: 0.022233, identity: 0.016674] ETA: 1:06:58.694639\n",
      "[Epoch 24/51] [Batch 300/476] [D loss: 0.128401] [G loss: 1.025875, adv: 0.667261, cycle: 0.025365, identity: 0.020994] ETA: 1:06:37.106266\n",
      "[Epoch 24/51] [Batch 400/476] [D loss: 0.135564] [G loss: 0.908972, adv: 0.474736, cycle: 0.029295, identity: 0.028258] ETA: 1:05:53.897367\n",
      "[Epoch 25/51] [Batch 0/476] [D loss: 0.109155] [G loss: 1.201258, adv: 0.774051, cycle: 0.029441, identity: 0.026560] ETA: 1:50:05.616941\n",
      "[Epoch 25/51] [Batch 100/476] [D loss: 0.137090] [G loss: 1.255016, adv: 0.412370, cycle: 0.052422, identity: 0.063686] ETA: 1:04:24.865582\n",
      "[Epoch 25/51] [Batch 200/476] [D loss: 0.102717] [G loss: 1.353865, adv: 1.003814, cycle: 0.023056, identity: 0.023899] ETA: 1:04:32.027012\n",
      "[Epoch 25/51] [Batch 300/476] [D loss: 0.161529] [G loss: 0.715333, adv: 0.376072, cycle: 0.022749, identity: 0.022354] ETA: 1:04:14.319931\n",
      "[Epoch 25/51] [Batch 400/476] [D loss: 0.285586] [G loss: 1.052476, adv: 0.410078, cycle: 0.046270, identity: 0.035940] ETA: 1:03:31.889523\n",
      "[Epoch 26/51] [Batch 0/476] [D loss: 0.113806] [G loss: 1.959726, adv: 0.858065, cycle: 0.081028, identity: 0.058277] ETA: 2:00:25.091600\n",
      "[Epoch 26/51] [Batch 100/476] [D loss: 0.248233] [G loss: 1.284090, adv: 0.822333, cycle: 0.031978, identity: 0.028395] ETA: 1:02:43.434839\n",
      "[Epoch 26/51] [Batch 200/476] [D loss: 0.066082] [G loss: 1.677758, adv: 1.036650, cycle: 0.044706, identity: 0.038810] ETA: 1:05:28.359890\n",
      "[Epoch 26/51] [Batch 300/476] [D loss: 0.034973] [G loss: 1.209027, adv: 0.624180, cycle: 0.039006, identity: 0.038956] ETA: 1:01:55.193558\n",
      "[Epoch 26/51] [Batch 400/476] [D loss: 0.177683] [G loss: 1.437090, adv: 0.936317, cycle: 0.034997, identity: 0.030162] ETA: 1:00:46.639585\n",
      "[Epoch 27/51] [Batch 0/476] [D loss: 0.220070] [G loss: 0.994995, adv: 0.457740, cycle: 0.036443, identity: 0.034565] ETA: 1:49:50.339516\n",
      "[Epoch 27/51] [Batch 100/476] [D loss: 0.090565] [G loss: 0.695559, adv: 0.369397, cycle: 0.021717, identity: 0.021798] ETA: 1:00:45.091770\n",
      "[Epoch 27/51] [Batch 200/476] [D loss: 0.099501] [G loss: 1.197368, adv: 0.787925, cycle: 0.028406, identity: 0.025077] ETA: 1:01:55.385847\n",
      "[Epoch 27/51] [Batch 300/476] [D loss: 0.205013] [G loss: 0.865261, adv: 0.535193, cycle: 0.022126, identity: 0.021762] ETA: 1:00:51.972061\n",
      "[Epoch 27/51] [Batch 400/476] [D loss: 0.126507] [G loss: 0.985839, adv: 0.589409, cycle: 0.027755, identity: 0.023776] ETA: 0:58:40.540730\n",
      "[Epoch 28/51] [Batch 0/476] [D loss: 0.236969] [G loss: 1.222277, adv: 0.844090, cycle: 0.025711, identity: 0.024215] ETA: 1:48:38.328001\n",
      "[Epoch 28/51] [Batch 100/476] [D loss: 0.075076] [G loss: 0.776170, adv: 0.498818, cycle: 0.018298, identity: 0.018875] ETA: 0:57:47.342651\n",
      "[Epoch 28/51] [Batch 200/476] [D loss: 0.218514] [G loss: 0.565194, adv: 0.316650, cycle: 0.017269, identity: 0.015170] ETA: 0:57:04.017458\n",
      "[Epoch 28/51] [Batch 300/476] [D loss: 0.304108] [G loss: 0.772445, adv: 0.355128, cycle: 0.028767, identity: 0.025929] ETA: 0:56:29.657066\n",
      "[Epoch 28/51] [Batch 400/476] [D loss: 0.269102] [G loss: 0.936107, adv: 0.516756, cycle: 0.029578, identity: 0.024715] ETA: 0:56:17.881680\n",
      "[Epoch 29/51] [Batch 0/476] [D loss: 0.150413] [G loss: 0.622328, adv: 0.250345, cycle: 0.025264, identity: 0.023868] ETA: 1:33:05.171196\n",
      "[Epoch 29/51] [Batch 100/476] [D loss: 0.130020] [G loss: 1.270745, adv: 0.742407, cycle: 0.036057, identity: 0.033553] ETA: 1:00:43.641573\n",
      "[Epoch 29/51] [Batch 200/476] [D loss: 0.105481] [G loss: 0.905961, adv: 0.294380, cycle: 0.041215, identity: 0.039886] ETA: 0:54:52.635361\n",
      "[Epoch 29/51] [Batch 300/476] [D loss: 0.051431] [G loss: 1.556096, adv: 1.083498, cycle: 0.033986, identity: 0.026548] ETA: 0:54:06.216466\n",
      "[Epoch 29/51] [Batch 400/476] [D loss: 0.209609] [G loss: 0.807945, adv: 0.427682, cycle: 0.026806, identity: 0.022440] ETA: 0:53:28.880957\n",
      "[Epoch 30/51] [Batch 0/476] [D loss: 0.108075] [G loss: 0.933577, adv: 0.562049, cycle: 0.026642, identity: 0.021022] ETA: 1:31:14.810866\n",
      "[Epoch 30/51] [Batch 100/476] [D loss: 0.177807] [G loss: 0.907322, adv: 0.453336, cycle: 0.031555, identity: 0.027688] ETA: 0:52:40.424486\n",
      "[Epoch 30/51] [Batch 200/476] [D loss: 0.132856] [G loss: 1.492585, adv: 0.791027, cycle: 0.048748, identity: 0.042816] ETA: 0:52:01.152145\n",
      "[Epoch 30/51] [Batch 300/476] [D loss: 0.174811] [G loss: 3.305931, adv: 0.553020, cycle: 0.206481, identity: 0.137621] ETA: 0:58:03.570694\n",
      "[Epoch 30/51] [Batch 400/476] [D loss: 0.145403] [G loss: 1.136753, adv: 0.435219, cycle: 0.048478, identity: 0.043351] ETA: 0:52:16.246092\n",
      "[Epoch 31/51] [Batch 0/476] [D loss: 0.040567] [G loss: 1.296861, adv: 0.846765, cycle: 0.032062, identity: 0.025895] ETA: 1:36:28.489494\n",
      "[Epoch 31/51] [Batch 100/476] [D loss: 0.074973] [G loss: 1.071191, adv: 0.701096, cycle: 0.024619, identity: 0.024781] ETA: 0:50:07.798662\n",
      "[Epoch 31/51] [Batch 200/476] [D loss: 0.038007] [G loss: 1.313424, adv: 0.792178, cycle: 0.034371, identity: 0.035508] ETA: 0:49:42.143841\n",
      "[Epoch 31/51] [Batch 300/476] [D loss: 0.010306] [G loss: 1.272327, adv: 0.870749, cycle: 0.027188, identity: 0.025941] ETA: 0:49:25.342879\n",
      "[Epoch 31/51] [Batch 400/476] [D loss: 0.111805] [G loss: 1.084017, adv: 0.658574, cycle: 0.028125, identity: 0.028838] ETA: 0:48:52.465439\n",
      "[Epoch 32/51] [Batch 0/476] [D loss: 0.020303] [G loss: 1.308093, adv: 0.857448, cycle: 0.032622, identity: 0.024884] ETA: 1:19:26.075426\n",
      "[Epoch 32/51] [Batch 100/476] [D loss: 0.011306] [G loss: 1.188374, adv: 0.781716, cycle: 0.028373, identity: 0.024585] ETA: 0:47:57.362427\n",
      "[Epoch 32/51] [Batch 200/476] [D loss: 0.035856] [G loss: 1.522531, adv: 1.013751, cycle: 0.036135, identity: 0.029487] ETA: 0:47:06.155354\n",
      "[Epoch 32/51] [Batch 300/476] [D loss: 0.013848] [G loss: 1.735094, adv: 0.786037, cycle: 0.064335, identity: 0.061142] ETA: 0:46:24.099203\n",
      "[Epoch 32/51] [Batch 400/476] [D loss: 0.157879] [G loss: 1.143532, adv: 0.610942, cycle: 0.038059, identity: 0.030400] ETA: 0:46:30.239239\n",
      "[Epoch 33/51] [Batch 0/476] [D loss: 0.086237] [G loss: 1.005522, adv: 0.701449, cycle: 0.021095, identity: 0.018624] ETA: 1:17:21.848431\n",
      "[Epoch 33/51] [Batch 100/476] [D loss: 0.092860] [G loss: 0.965264, adv: 0.541489, cycle: 0.029054, identity: 0.026648] ETA: 0:44:51.629457\n",
      "[Epoch 33/51] [Batch 200/476] [D loss: 0.036716] [G loss: 1.204918, adv: 0.847786, cycle: 0.023632, identity: 0.024162] ETA: 0:44:29.032932\n",
      "[Epoch 33/51] [Batch 300/476] [D loss: 0.167797] [G loss: 1.283583, adv: 0.476027, cycle: 0.053918, identity: 0.053675] ETA: 0:44:14.967133\n",
      "[Epoch 33/51] [Batch 400/476] [D loss: 0.015010] [G loss: 1.655360, adv: 0.978281, cycle: 0.044689, identity: 0.046037] ETA: 0:43:41.780807\n",
      "[Epoch 34/51] [Batch 0/476] [D loss: 0.118724] [G loss: 1.107515, adv: 0.824976, cycle: 0.019470, identity: 0.017568] ETA: 1:13:32.945795\n",
      "[Epoch 34/51] [Batch 100/476] [D loss: 0.902643] [G loss: 2.053652, adv: 1.600907, cycle: 0.030748, identity: 0.029053] ETA: 0:42:40.585350\n",
      "[Epoch 34/51] [Batch 200/476] [D loss: 0.135977] [G loss: 0.917968, adv: 0.240902, cycle: 0.045258, identity: 0.044897] ETA: 0:41:46.192593\n",
      "[Epoch 34/51] [Batch 300/476] [D loss: 0.199984] [G loss: 0.590916, adv: 0.304933, cycle: 0.018944, identity: 0.019309] ETA: 0:43:45.880875\n",
      "[Epoch 34/51] [Batch 400/476] [D loss: 0.164614] [G loss: 1.132663, adv: 0.503189, cycle: 0.041336, identity: 0.043222] ETA: 0:47:44.268242\n",
      "[Epoch 35/51] [Batch 0/476] [D loss: 0.110839] [G loss: 0.878342, adv: 0.527939, cycle: 0.022729, identity: 0.024622] ETA: 1:09:57.370193\n",
      "[Epoch 35/51] [Batch 100/476] [D loss: 0.062144] [G loss: 1.259570, adv: 0.578868, cycle: 0.044315, identity: 0.047511] ETA: 0:42:30.791026\n",
      "[Epoch 35/51] [Batch 200/476] [D loss: 0.196466] [G loss: 0.954874, adv: 0.542323, cycle: 0.027754, identity: 0.027001] ETA: 0:41:37.479675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/51] [Batch 300/476] [D loss: 0.236965] [G loss: 1.026210, adv: 0.770922, cycle: 0.017207, identity: 0.016644] ETA: 0:42:26.460247\n",
      "[Epoch 35/51] [Batch 400/476] [D loss: 0.213565] [G loss: 0.819545, adv: 0.385913, cycle: 0.029199, identity: 0.028328] ETA: 0:40:30.657467\n",
      "[Epoch 36/51] [Batch 0/476] [D loss: 0.158442] [G loss: 0.832348, adv: 0.460518, cycle: 0.025964, identity: 0.022438] ETA: 1:07:18.516197\n",
      "[Epoch 36/51] [Batch 100/476] [D loss: 0.185027] [G loss: 0.758593, adv: 0.303116, cycle: 0.030052, identity: 0.030992] ETA: 0:40:29.203033\n",
      "[Epoch 36/51] [Batch 200/476] [D loss: 0.229853] [G loss: 0.618881, adv: 0.303711, cycle: 0.021259, identity: 0.020515] ETA: 0:39:11.928563\n",
      "[Epoch 36/51] [Batch 300/476] [D loss: 0.249178] [G loss: 0.559196, adv: 0.309163, cycle: 0.017322, identity: 0.015362] ETA: 0:39:16.383705\n",
      "[Epoch 36/51] [Batch 400/476] [D loss: 0.234963] [G loss: 0.571385, adv: 0.249305, cycle: 0.022257, identity: 0.019903] ETA: 0:38:48.562284\n",
      "[Epoch 37/51] [Batch 0/476] [D loss: 0.347109] [G loss: 0.625545, adv: 0.273668, cycle: 0.023618, identity: 0.023140] ETA: 1:02:25.943586\n",
      "[Epoch 37/51] [Batch 100/476] [D loss: 0.144646] [G loss: 1.096987, adv: 0.528477, cycle: 0.038820, identity: 0.036062] ETA: 0:37:34.396866\n",
      "[Epoch 37/51] [Batch 200/476] [D loss: 0.247497] [G loss: 0.544138, adv: 0.248985, cycle: 0.019873, identity: 0.019284] ETA: 0:43:16.555023\n",
      "[Epoch 37/51] [Batch 300/476] [D loss: 0.314121] [G loss: 1.346806, adv: 0.635617, cycle: 0.045472, identity: 0.051293] ETA: 0:38:31.091914\n",
      "[Epoch 37/51] [Batch 400/476] [D loss: 0.292426] [G loss: 0.644216, adv: 0.394102, cycle: 0.016792, identity: 0.016438] ETA: 0:38:26.282736\n",
      "[Epoch 38/51] [Batch 0/476] [D loss: 0.137374] [G loss: 0.824645, adv: 0.479860, cycle: 0.022469, identity: 0.024018] ETA: 1:06:56.389249\n",
      "[Epoch 38/51] [Batch 100/476] [D loss: 0.231687] [G loss: 0.514195, adv: 0.253000, cycle: 0.016907, identity: 0.018425] ETA: 0:35:14.436747\n",
      "[Epoch 38/51] [Batch 200/476] [D loss: 0.123697] [G loss: 1.443751, adv: 0.574881, cycle: 0.055079, identity: 0.063615] ETA: 0:34:48.506990\n",
      "[Epoch 38/51] [Batch 300/476] [D loss: 0.218014] [G loss: 0.441361, adv: 0.210450, cycle: 0.016377, identity: 0.013429] ETA: 0:34:02.823669\n",
      "[Epoch 38/51] [Batch 400/476] [D loss: 0.266431] [G loss: 0.611477, adv: 0.248984, cycle: 0.025267, identity: 0.021964] ETA: 0:34:31.087196\n",
      "[Epoch 39/51] [Batch 0/476] [D loss: 0.274525] [G loss: 0.547821, adv: 0.279033, cycle: 0.018828, identity: 0.016101] ETA: 1:04:43.804939\n",
      "[Epoch 39/51] [Batch 100/476] [D loss: 0.285093] [G loss: 0.594741, adv: 0.293015, cycle: 0.020988, identity: 0.018369] ETA: 0:32:30.116961\n",
      "[Epoch 39/51] [Batch 200/476] [D loss: 0.242446] [G loss: 0.824195, adv: 0.279524, cycle: 0.040588, identity: 0.027758] ETA: 0:31:59.950394\n",
      "[Epoch 39/51] [Batch 300/476] [D loss: 0.215461] [G loss: 0.720604, adv: 0.282327, cycle: 0.028962, identity: 0.029731] ETA: 0:30:55.391779\n",
      "[Epoch 39/51] [Batch 400/476] [D loss: 0.357587] [G loss: 0.566987, adv: 0.242451, cycle: 0.022953, identity: 0.019001] ETA: 0:30:56.092819\n",
      "[Epoch 40/51] [Batch 0/476] [D loss: 0.156660] [G loss: 0.576754, adv: 0.350827, cycle: 0.015997, identity: 0.013192] ETA: 0:50:10.480538\n",
      "[Epoch 40/51] [Batch 100/476] [D loss: 0.217042] [G loss: 0.558830, adv: 0.286874, cycle: 0.019511, identity: 0.015368] ETA: 0:33:26.347961\n",
      "[Epoch 40/51] [Batch 200/476] [D loss: 0.217674] [G loss: 0.975663, adv: 0.320880, cycle: 0.044262, identity: 0.042432] ETA: 0:32:14.533916\n",
      "[Epoch 40/51] [Batch 300/476] [D loss: 0.225346] [G loss: 0.589809, adv: 0.278985, cycle: 0.021730, identity: 0.018705] ETA: 0:28:25.393847\n",
      "[Epoch 40/51] [Batch 400/476] [D loss: 0.191115] [G loss: 1.048674, adv: 0.379620, cycle: 0.043808, identity: 0.046194] ETA: 0:28:36.803152\n",
      "[Epoch 41/51] [Batch 0/476] [D loss: 0.182533] [G loss: 0.549315, adv: 0.348386, cycle: 0.013436, identity: 0.013314] ETA: 0:47:06.254539\n",
      "[Epoch 41/51] [Batch 100/476] [D loss: 0.260909] [G loss: 0.547586, adv: 0.288740, cycle: 0.018362, identity: 0.015045] ETA: 0:26:11.149449\n",
      "[Epoch 41/51] [Batch 200/476] [D loss: 0.235987] [G loss: 0.398381, adv: 0.206067, cycle: 0.012859, identity: 0.012745] ETA: 0:25:42.728405\n",
      "[Epoch 41/51] [Batch 300/476] [D loss: 0.226793] [G loss: 0.509536, adv: 0.246563, cycle: 0.017919, identity: 0.016757] ETA: 0:29:22.039123\n",
      "[Epoch 41/51] [Batch 400/476] [D loss: 0.208753] [G loss: 0.733709, adv: 0.452072, cycle: 0.019109, identity: 0.018110] ETA: 0:24:45.184460\n",
      "[Epoch 42/51] [Batch 0/476] [D loss: 0.256825] [G loss: 0.837949, adv: 0.231580, cycle: 0.041073, identity: 0.039128] ETA: 0:40:11.306591\n",
      "[Epoch 42/51] [Batch 100/476] [D loss: 0.215620] [G loss: 0.990480, adv: 0.504647, cycle: 0.030832, identity: 0.035502] ETA: 0:26:33.325094\n",
      "[Epoch 42/51] [Batch 200/476] [D loss: 0.228541] [G loss: 0.652007, adv: 0.307214, cycle: 0.023823, identity: 0.021313] ETA: 0:23:41.982926\n",
      "[Epoch 42/51] [Batch 300/476] [D loss: 0.186377] [G loss: 0.938186, adv: 0.304753, cycle: 0.043884, identity: 0.038919] ETA: 0:22:35.098228\n",
      "[Epoch 42/51] [Batch 400/476] [D loss: 0.222017] [G loss: 0.645914, adv: 0.423798, cycle: 0.015050, identity: 0.014322] ETA: 0:21:53.500633\n",
      "[Epoch 43/51] [Batch 0/476] [D loss: 0.248615] [G loss: 1.046762, adv: 0.350590, cycle: 0.046742, identity: 0.045750] ETA: 0:38:43.489708\n",
      "[Epoch 43/51] [Batch 100/476] [D loss: 0.188547] [G loss: 0.620328, adv: 0.319011, cycle: 0.020762, identity: 0.018739] ETA: 0:21:03.894327\n",
      "[Epoch 43/51] [Batch 200/476] [D loss: 0.203559] [G loss: 1.006970, adv: 0.348001, cycle: 0.046081, identity: 0.039633] ETA: 0:20:19.062923\n",
      "[Epoch 43/51] [Batch 300/476] [D loss: 0.207037] [G loss: 0.769018, adv: 0.283536, cycle: 0.032455, identity: 0.032187] ETA: 0:19:43.267855\n",
      "[Epoch 43/51] [Batch 400/476] [D loss: 0.229536] [G loss: 0.553397, adv: 0.292909, cycle: 0.018262, identity: 0.015574] ETA: 0:22:02.862247\n",
      "[Epoch 44/51] [Batch 0/476] [D loss: 0.172198] [G loss: 0.642909, adv: 0.376554, cycle: 0.018701, identity: 0.015869] ETA: 0:27:43.610413\n",
      "[Epoch 44/51] [Batch 100/476] [D loss: 0.276920] [G loss: 0.733351, adv: 0.288251, cycle: 0.031107, identity: 0.026806] ETA: 0:22:21.898643\n",
      "[Epoch 44/51] [Batch 200/476] [D loss: 0.241299] [G loss: 0.811305, adv: 0.565310, cycle: 0.016230, identity: 0.016739] ETA: 0:19:34.541070\n",
      "[Epoch 44/51] [Batch 300/476] [D loss: 0.249196] [G loss: 0.913127, adv: 0.597647, cycle: 0.020078, identity: 0.022939] ETA: 0:18:56.612534\n",
      "[Epoch 44/51] [Batch 400/476] [D loss: 0.224101] [G loss: 0.861894, adv: 0.290971, cycle: 0.039631, identity: 0.034923] ETA: 0:18:40.019716\n",
      "[Epoch 45/51] [Batch 0/476] [D loss: 0.197615] [G loss: 0.896529, adv: 0.421708, cycle: 0.034573, identity: 0.025819] ETA: 0:26:01.213142\n",
      "[Epoch 45/51] [Batch 100/476] [D loss: 0.258315] [G loss: 0.511322, adv: 0.212460, cycle: 0.019792, identity: 0.020187] ETA: 0:17:10.386747\n",
      "[Epoch 45/51] [Batch 200/476] [D loss: 0.237049] [G loss: 0.503161, adv: 0.268849, cycle: 0.015517, identity: 0.015829] ETA: 0:16:42.172821\n",
      "[Epoch 45/51] [Batch 300/476] [D loss: 0.195037] [G loss: 0.600382, adv: 0.392850, cycle: 0.013487, identity: 0.014533] ETA: 0:16:02.530558\n",
      "[Epoch 45/51] [Batch 400/476] [D loss: 0.192005] [G loss: 0.602551, adv: 0.367420, cycle: 0.015856, identity: 0.015314] ETA: 0:15:19.400846\n",
      "[Epoch 46/51] [Batch 0/476] [D loss: 0.215188] [G loss: 0.666691, adv: 0.409892, cycle: 0.016371, identity: 0.018619] ETA: 0:21:57.445040\n",
      "[Epoch 46/51] [Batch 100/476] [D loss: 0.278788] [G loss: 0.612153, adv: 0.274935, cycle: 0.022972, identity: 0.021500] ETA: 0:14:05.707254\n",
      "[Epoch 46/51] [Batch 200/476] [D loss: 0.310630] [G loss: 0.531153, adv: 0.234565, cycle: 0.018979, identity: 0.021360] ETA: 0:13:29.887185\n",
      "[Epoch 46/51] [Batch 300/476] [D loss: 0.203480] [G loss: 1.146969, adv: 0.540839, cycle: 0.041280, identity: 0.038666] ETA: 0:12:58.534088\n",
      "[Epoch 46/51] [Batch 400/476] [D loss: 0.214007] [G loss: 0.607806, adv: 0.323586, cycle: 0.019452, identity: 0.017941] ETA: 0:13:17.391214\n",
      "[Epoch 47/51] [Batch 0/476] [D loss: 0.188805] [G loss: 0.558203, adv: 0.277623, cycle: 0.018758, identity: 0.018599] ETA: 0:16:41.739189\n",
      "[Epoch 47/51] [Batch 100/476] [D loss: 0.192965] [G loss: 0.909212, adv: 0.596205, cycle: 0.021147, identity: 0.020308] ETA: 0:10:08.183076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/51] [Batch 200/476] [D loss: 0.191741] [G loss: 0.975357, adv: 0.284430, cycle: 0.045072, identity: 0.048041] ETA: 0:09:51.537249\n",
      "[Epoch 47/51] [Batch 300/476] [D loss: 0.222476] [G loss: 0.607172, adv: 0.327046, cycle: 0.018713, identity: 0.018600] ETA: 0:09:11.544796\n",
      "[Epoch 47/51] [Batch 400/476] [D loss: 0.182929] [G loss: 0.760731, adv: 0.403233, cycle: 0.024323, identity: 0.022854] ETA: 0:09:03.422081\n",
      "[Epoch 48/51] [Batch 0/476] [D loss: 0.199048] [G loss: 0.751035, adv: 0.442426, cycle: 0.021007, identity: 0.019708] ETA: 0:11:26.801193\n",
      "[Epoch 48/51] [Batch 100/476] [D loss: 0.154052] [G loss: 0.812317, adv: 0.432929, cycle: 0.025693, identity: 0.024492] ETA: 0:07:41.002968\n",
      "[Epoch 48/51] [Batch 200/476] [D loss: 0.199017] [G loss: 0.790458, adv: 0.438326, cycle: 0.022916, identity: 0.024595] ETA: 0:07:00.350180\n",
      "[Epoch 48/51] [Batch 300/476] [D loss: 0.199589] [G loss: 0.691289, adv: 0.388708, cycle: 0.020012, identity: 0.020492] ETA: 0:06:44.217768\n",
      "[Epoch 48/51] [Batch 400/476] [D loss: 0.224641] [G loss: 0.658459, adv: 0.396634, cycle: 0.017204, identity: 0.017957] ETA: 0:05:56.601912\n",
      "[Epoch 49/51] [Batch 0/476] [D loss: 0.105725] [G loss: 0.640212, adv: 0.318923, cycle: 0.021861, identity: 0.020536] ETA: 0:08:02.246792\n",
      "[Epoch 49/51] [Batch 100/476] [D loss: 0.247960] [G loss: 0.856520, adv: 0.501563, cycle: 0.022348, identity: 0.026295] ETA: 0:05:11.029166\n",
      "[Epoch 49/51] [Batch 200/476] [D loss: 0.299916] [G loss: 0.899520, adv: 0.345951, cycle: 0.038041, identity: 0.034632] ETA: 0:04:34.063335\n",
      "[Epoch 49/51] [Batch 300/476] [D loss: 0.199510] [G loss: 1.180459, adv: 0.497267, cycle: 0.046377, identity: 0.043885] ETA: 0:03:41.146443\n",
      "[Epoch 49/51] [Batch 400/476] [D loss: 0.192117] [G loss: 0.847126, adv: 0.467744, cycle: 0.024850, identity: 0.026177] ETA: 0:03:08.118467\n",
      "[Epoch 50/51] [Batch 0/476] [D loss: 0.169582] [G loss: 0.676056, adv: 0.418155, cycle: 0.016973, identity: 0.017635] ETA: 0:03:58.200305\n",
      "[Epoch 50/51] [Batch 100/476] [D loss: 0.271590] [G loss: 0.804814, adv: 0.450697, cycle: 0.022774, identity: 0.025275] ETA: 0:02:40.789667\n",
      "[Epoch 50/51] [Batch 200/476] [D loss: 0.203836] [G loss: 0.802025, adv: 0.339343, cycle: 0.031913, identity: 0.028711] ETA: 0:01:34.795312\n",
      "[Epoch 50/51] [Batch 300/476] [D loss: 0.169269] [G loss: 0.761274, adv: 0.450518, cycle: 0.021137, identity: 0.019876] ETA: 0:01:07.179039\n",
      "[Epoch 50/51] [Batch 400/476] [D loss: 0.195816] [G loss: 0.727705, adv: 0.355494, cycle: 0.025797, identity: 0.022848] ETA: 0:00:26.486273\n"
     ]
    }
   ],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "    \n",
    "epoch = 0\n",
    "dataset_name = 'CycleGAN1SS'\n",
    "os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader1, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "    \n",
    "epoch = 0\n",
    "dataset_name = 'CycleGAN2SS'\n",
    "os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader2, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "    \n",
    "epoch = 0\n",
    "dataset_name = 'CycleGAN3SS'\n",
    "os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader3, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
