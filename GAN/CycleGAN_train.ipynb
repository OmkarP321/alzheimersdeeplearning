{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from CycleGAN_utils import *\n",
    "from CycleGAN_models import *\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "n_epochs = 51\n",
    "batch_size = 1\n",
    "lr=0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 50\n",
    "n_cpu = 8\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "channels = 1\n",
    "sample_interval = 100\n",
    "checkpoint_interval = 25\n",
    "n_residual_blocks = 9\n",
    "lambda_cyc = 10\n",
    "lambda_id = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"../datasets/224dataset.pt\")\n",
    "\n",
    "dataset = [sample for sample in dataset if sample != None]\n",
    "\n",
    "NC = []\n",
    "AD = []\n",
    "for data in dataset:\n",
    "    if data[1] == 0:\n",
    "        NC.append(data)\n",
    "    else:\n",
    "        AD.append(data)\n",
    "        \n",
    "        \n",
    "def process_gan(dataset, s):\n",
    "    output = []\n",
    "    dataset = [sample[0] for sample in dataset]\n",
    "    for sample in dataset:\n",
    "        sample = sample[s][0]\n",
    "        sample /= torch.max(sample)\n",
    "        output.append(torch.unsqueeze(sample, 0))\n",
    "    return output\n",
    "\n",
    "        \n",
    "NCgan1 = process_gan(NC, 0)\n",
    "NCgan2 = process_gan(NC, 1)\n",
    "NCgan3 = process_gan(NC, 2)\n",
    "\n",
    "ADgan1 = process_gan(AD, 0)\n",
    "ADgan2 = process_gan(AD, 1)\n",
    "ADgan3 = process_gan(AD, 2)\n",
    "\n",
    "gan1 = []\n",
    "for i in range(len(ADgan1)):\n",
    "    gan1.append({\"A\": NCgan1[i], \"B\": ADgan1[i]})\n",
    "\n",
    "gan2 = []\n",
    "for i in range(len(ADgan2)):\n",
    "    gan2.append({\"A\": NCgan2[i], \"B\": ADgan2[i]})\n",
    "    \n",
    "gan3 = []\n",
    "for i in range(len(ADgan3)):\n",
    "    gan3.append({\"A\": NCgan3[i], \"B\": ADgan3[i]})\n",
    "\n",
    "batch_size = 1\n",
    "dataloader1 = DataLoader(gan1, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader2 = DataLoader(gan2, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader3 = DataLoader(gan3, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "        \n",
    "def sample_images(batches_done, dataloader):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "\n",
    "    real_A = make_grid(real_A, nrow=4, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=4, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=4, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=4, normalize=True)\n",
    "\n",
    "    image_grid = torch.stack((real_A, fake_B, real_B, fake_A), 0)\n",
    "    save_image(image_grid, \"ganimages/%s/%s.png\" % (dataset_name, batches_done), normalize=False)\n",
    "    \n",
    "def train_gan(dataloader, epoch):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            G_AB.train()\n",
    "            G_BA.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "                \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "           \n",
    "            optimizer_D_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "            \n",
    "                print(\n",
    "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        n_epochs,\n",
    "                        i,\n",
    "                        len(dataloader),\n",
    "                        loss_D.item(),\n",
    "                        loss_G.item(),\n",
    "                        loss_GAN.item(),\n",
    "                        loss_cycle.item(),\n",
    "                        loss_identity.item(),\n",
    "                        time_left,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, dataloader)\n",
    "                \n",
    "            \n",
    "            G_losses.append(loss_G.item())\n",
    "            D_losses.append(loss_D.item())\n",
    "            \n",
    "            \n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D_A.step()\n",
    "        lr_scheduler_D_B.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "            torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks).cuda()\n",
    "D_A = Discriminator(input_shape).cuda()\n",
    "D_B = Discriminator(input_shape).cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "\n",
    "if epoch != 0:\n",
    "\n",
    "    G_AB.load_state_dict(torch.load(\"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch)))\n",
    "    G_BA.load_state_dict(torch.load(\"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_A.load_state_dict(torch.load(\"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch)))\n",
    "    D_B.load_state_dict(torch.load(\"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epoch = 0\n",
    "# dataset_name = '224CycleGAN1'\n",
    "# os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "# os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "# train_gan(dataloader1, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 0\n",
    "# dataset_name = '224CycleGAN2'\n",
    "# os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "# os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "# train_gan(dataloader2, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/51] [Batch 0/476] [D loss: 1.808080] [G loss: 11.352324, adv: 2.453085, cycle: 0.615865, identity: 0.548117] ETA: 5:21:07.200894\n",
      "[Epoch 0/51] [Batch 100/476] [D loss: 0.231213] [G loss: 1.935448, adv: 0.549290, cycle: 0.096912, identity: 0.083407] ETA: 2:04:12.809910\n",
      "[Epoch 0/51] [Batch 200/476] [D loss: 0.199476] [G loss: 1.878462, adv: 0.728786, cycle: 0.071792, identity: 0.086351] ETA: 2:05:38.083194\n",
      "[Epoch 0/51] [Batch 300/476] [D loss: 0.277035] [G loss: 1.276387, adv: 0.436343, cycle: 0.052765, identity: 0.062479] ETA: 2:04:27.199551\n",
      "[Epoch 0/51] [Batch 400/476] [D loss: 0.262154] [G loss: 1.472710, adv: 0.602391, cycle: 0.058835, identity: 0.056395] ETA: 2:34:58.953962\n",
      "[Epoch 1/51] [Batch 0/476] [D loss: 0.234816] [G loss: 1.125616, adv: 0.365015, cycle: 0.053618, identity: 0.044885] ETA: 4:34:50.683842\n",
      "[Epoch 1/51] [Batch 100/476] [D loss: 0.141460] [G loss: 1.715423, adv: 0.971555, cycle: 0.047523, identity: 0.053728] ETA: 2:04:58.065305\n",
      "[Epoch 1/51] [Batch 200/476] [D loss: 0.081015] [G loss: 1.545753, adv: 0.667832, cycle: 0.056265, identity: 0.063055] ETA: 2:45:27.728367\n",
      "[Epoch 1/51] [Batch 300/476] [D loss: 0.146271] [G loss: 1.278615, adv: 0.538472, cycle: 0.048457, identity: 0.051114] ETA: 2:06:08.698287\n",
      "[Epoch 1/51] [Batch 400/476] [D loss: 0.123926] [G loss: 1.617600, adv: 0.448364, cycle: 0.074896, identity: 0.084056] ETA: 2:05:29.165840\n",
      "[Epoch 2/51] [Batch 0/476] [D loss: 0.277290] [G loss: 1.793756, adv: 0.971970, cycle: 0.051962, identity: 0.060432] ETA: 3:41:39.449674\n",
      "[Epoch 2/51] [Batch 100/476] [D loss: 0.208116] [G loss: 1.063602, adv: 0.340343, cycle: 0.049034, identity: 0.046583] ETA: 2:15:54.676544\n",
      "[Epoch 2/51] [Batch 200/476] [D loss: 0.139785] [G loss: 1.111702, adv: 0.359842, cycle: 0.047500, identity: 0.055372] ETA: 2:03:44.267289\n",
      "[Epoch 2/51] [Batch 300/476] [D loss: 0.159255] [G loss: 1.857300, adv: 0.844215, cycle: 0.069461, identity: 0.063695] ETA: 2:03:15.130699\n",
      "[Epoch 2/51] [Batch 400/476] [D loss: 0.229553] [G loss: 1.378540, adv: 0.552604, cycle: 0.058145, identity: 0.048898] ETA: 2:13:29.427977\n",
      "[Epoch 3/51] [Batch 0/476] [D loss: 0.046056] [G loss: 1.656963, adv: 0.834600, cycle: 0.053066, identity: 0.058340] ETA: 3:37:34.501190\n",
      "[Epoch 3/51] [Batch 100/476] [D loss: 0.361777] [G loss: 1.391923, adv: 0.638333, cycle: 0.049027, identity: 0.052664] ETA: 2:05:43.810576\n",
      "[Epoch 3/51] [Batch 200/476] [D loss: 0.059416] [G loss: 1.517761, adv: 0.568441, cycle: 0.062307, identity: 0.065250] ETA: 2:08:26.500717\n",
      "[Epoch 3/51] [Batch 300/476] [D loss: 0.055011] [G loss: 2.049692, adv: 0.773399, cycle: 0.080094, identity: 0.095071] ETA: 2:11:09.229980\n",
      "[Epoch 3/51] [Batch 400/476] [D loss: 0.160910] [G loss: 0.832681, adv: 0.375356, cycle: 0.028929, identity: 0.033607] ETA: 1:58:28.965420\n",
      "[Epoch 4/51] [Batch 0/476] [D loss: 0.262478] [G loss: 0.950379, adv: 0.367871, cycle: 0.039827, identity: 0.036848] ETA: 4:06:53.068309\n",
      "[Epoch 4/51] [Batch 100/476] [D loss: 0.218427] [G loss: 0.765752, adv: 0.166964, cycle: 0.041078, identity: 0.037603] ETA: 2:00:53.970154\n",
      "[Epoch 4/51] [Batch 200/476] [D loss: 0.206329] [G loss: 0.819799, adv: 0.295571, cycle: 0.036272, identity: 0.032301] ETA: 1:57:04.504530\n",
      "[Epoch 4/51] [Batch 300/476] [D loss: 0.214740] [G loss: 0.912973, adv: 0.244192, cycle: 0.047832, identity: 0.038092] ETA: 1:55:55.801851\n",
      "[Epoch 4/51] [Batch 400/476] [D loss: 0.308376] [G loss: 0.814874, adv: 0.319402, cycle: 0.033419, identity: 0.032256] ETA: 1:57:34.936721\n",
      "[Epoch 5/51] [Batch 0/476] [D loss: 0.279263] [G loss: 0.743360, adv: 0.201625, cycle: 0.037408, identity: 0.033532] ETA: 3:25:48.015635\n",
      "[Epoch 5/51] [Batch 100/476] [D loss: 0.269596] [G loss: 0.956055, adv: 0.281886, cycle: 0.043281, identity: 0.048273] ETA: 1:55:45.139661\n",
      "[Epoch 5/51] [Batch 200/476] [D loss: 0.236139] [G loss: 0.785442, adv: 0.372792, cycle: 0.029072, identity: 0.024387] ETA: 1:54:45.663345\n",
      "[Epoch 5/51] [Batch 300/476] [D loss: 0.291266] [G loss: 0.828682, adv: 0.268967, cycle: 0.038491, identity: 0.034961] ETA: 1:55:18.369805\n",
      "[Epoch 5/51] [Batch 400/476] [D loss: 0.239605] [G loss: 0.719954, adv: 0.224912, cycle: 0.033024, identity: 0.032961] ETA: 1:52:56.289400\n",
      "[Epoch 6/51] [Batch 0/476] [D loss: 0.225235] [G loss: 1.057363, adv: 0.483463, cycle: 0.038339, identity: 0.038102] ETA: 3:18:39.529452\n",
      "[Epoch 6/51] [Batch 100/476] [D loss: 0.119830] [G loss: 2.005698, adv: 0.474381, cycle: 0.110343, identity: 0.085578] ETA: 1:53:06.390009\n",
      "[Epoch 6/51] [Batch 200/476] [D loss: 0.317379] [G loss: 1.234627, adv: 0.329808, cycle: 0.060756, identity: 0.059451] ETA: 1:51:48.165660\n",
      "[Epoch 6/51] [Batch 300/476] [D loss: 0.111290] [G loss: 0.843593, adv: 0.262105, cycle: 0.039885, identity: 0.036528] ETA: 1:52:34.022827\n",
      "[Epoch 6/51] [Batch 400/476] [D loss: 0.196405] [G loss: 1.308465, adv: 0.791397, cycle: 0.034617, identity: 0.034179] ETA: 1:52:33.130250\n",
      "[Epoch 7/51] [Batch 0/476] [D loss: 0.137813] [G loss: 2.160389, adv: 1.144167, cycle: 0.065107, identity: 0.073031] ETA: 3:21:17.700142\n",
      "[Epoch 7/51] [Batch 100/476] [D loss: 0.141774] [G loss: 1.024451, adv: 0.477685, cycle: 0.036790, identity: 0.035774] ETA: 1:51:07.588840\n",
      "[Epoch 7/51] [Batch 200/476] [D loss: 0.187906] [G loss: 1.207955, adv: 0.708748, cycle: 0.034971, identity: 0.029899] ETA: 1:49:45.579426\n",
      "[Epoch 7/51] [Batch 300/476] [D loss: 0.184667] [G loss: 1.074688, adv: 0.415694, cycle: 0.043261, identity: 0.045277] ETA: 1:49:56.318468\n",
      "[Epoch 7/51] [Batch 400/476] [D loss: 0.269910] [G loss: 0.609229, adv: 0.192896, cycle: 0.028477, identity: 0.026312] ETA: 1:49:04.797958\n",
      "[Epoch 8/51] [Batch 0/476] [D loss: 0.289074] [G loss: 0.860611, adv: 0.168308, cycle: 0.045108, identity: 0.048245] ETA: 3:10:54.510024\n",
      "[Epoch 8/51] [Batch 100/476] [D loss: 0.246837] [G loss: 0.713057, adv: 0.214454, cycle: 0.034372, identity: 0.030976] ETA: 1:48:08.573421\n",
      "[Epoch 8/51] [Batch 200/476] [D loss: 0.181416] [G loss: 1.120656, adv: 0.665808, cycle: 0.030976, identity: 0.029017] ETA: 1:46:06.174517\n",
      "[Epoch 8/51] [Batch 300/476] [D loss: 0.154713] [G loss: 1.166615, adv: 0.581719, cycle: 0.038185, identity: 0.040609] ETA: 1:46:28.868946\n",
      "[Epoch 8/51] [Batch 400/476] [D loss: 0.100553] [G loss: 1.156668, adv: 0.638746, cycle: 0.034062, identity: 0.035461] ETA: 1:46:15.611347\n",
      "[Epoch 9/51] [Batch 0/476] [D loss: 0.134555] [G loss: 1.123231, adv: 0.582062, cycle: 0.038063, identity: 0.032108] ETA: 3:07:26.634418\n",
      "[Epoch 9/51] [Batch 100/476] [D loss: 0.212437] [G loss: 1.530610, adv: 0.938793, cycle: 0.041294, identity: 0.035775] ETA: 1:44:54.351505\n",
      "[Epoch 9/51] [Batch 200/476] [D loss: 0.137736] [G loss: 1.162314, adv: 0.633301, cycle: 0.035477, identity: 0.034849] ETA: 1:43:56.047768\n",
      "[Epoch 9/51] [Batch 300/476] [D loss: 0.152923] [G loss: 1.045929, adv: 0.475980, cycle: 0.038838, identity: 0.036314] ETA: 1:44:34.827773\n",
      "[Epoch 9/51] [Batch 400/476] [D loss: 0.232488] [G loss: 1.441559, adv: 0.674298, cycle: 0.051181, identity: 0.051090] ETA: 1:44:37.435610\n",
      "[Epoch 10/51] [Batch 0/476] [D loss: 0.179120] [G loss: 1.350702, adv: 0.680257, cycle: 0.045499, identity: 0.043091] ETA: 3:03:36.258038\n",
      "[Epoch 10/51] [Batch 100/476] [D loss: 0.269145] [G loss: 1.027257, adv: 0.563943, cycle: 0.031782, identity: 0.029098] ETA: 1:43:31.123547\n",
      "[Epoch 10/51] [Batch 200/476] [D loss: 0.097332] [G loss: 1.070161, adv: 0.574550, cycle: 0.033060, identity: 0.033003] ETA: 1:41:50.740619\n",
      "[Epoch 10/51] [Batch 300/476] [D loss: 0.214326] [G loss: 1.208979, adv: 0.483966, cycle: 0.048427, identity: 0.048148] ETA: 1:41:22.683090\n",
      "[Epoch 10/51] [Batch 400/476] [D loss: 0.141264] [G loss: 1.248252, adv: 0.763197, cycle: 0.032038, identity: 0.032936] ETA: 1:41:48.732722\n",
      "[Epoch 11/51] [Batch 0/476] [D loss: 0.089926] [G loss: 1.233776, adv: 0.633942, cycle: 0.042700, identity: 0.034566] ETA: 3:01:50.209656\n",
      "[Epoch 11/51] [Batch 100/476] [D loss: 0.147264] [G loss: 1.542849, adv: 0.895347, cycle: 0.043754, identity: 0.041992] ETA: 1:40:19.566355\n",
      "[Epoch 11/51] [Batch 200/476] [D loss: 0.089852] [G loss: 1.006029, adv: 0.520624, cycle: 0.032700, identity: 0.031682] ETA: 1:40:32.455072\n",
      "[Epoch 11/51] [Batch 300/476] [D loss: 0.121482] [G loss: 1.230303, adv: 0.354988, cycle: 0.060591, identity: 0.053882] ETA: 1:39:24.035168\n",
      "[Epoch 11/51] [Batch 400/476] [D loss: 0.278210] [G loss: 0.928869, adv: 0.334197, cycle: 0.041147, identity: 0.036640] ETA: 1:39:36.033497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/51] [Batch 0/476] [D loss: 0.199032] [G loss: 0.763189, adv: 0.350826, cycle: 0.026401, identity: 0.029670] ETA: 2:58:18.940441\n",
      "[Epoch 12/51] [Batch 100/476] [D loss: 0.235158] [G loss: 1.399800, adv: 0.635200, cycle: 0.053339, identity: 0.046243] ETA: 1:37:42.577438\n",
      "[Epoch 12/51] [Batch 200/476] [D loss: 0.317604] [G loss: 0.823813, adv: 0.231317, cycle: 0.038689, identity: 0.041122] ETA: 1:37:17.178985\n",
      "[Epoch 12/51] [Batch 300/476] [D loss: 0.086125] [G loss: 1.085543, adv: 0.609343, cycle: 0.032841, identity: 0.029558] ETA: 1:34:19.139179\n",
      "[Epoch 12/51] [Batch 400/476] [D loss: 0.226094] [G loss: 0.763605, adv: 0.337320, cycle: 0.028467, identity: 0.028324] ETA: 1:36:51.474080\n",
      "[Epoch 13/51] [Batch 0/476] [D loss: 0.147136] [G loss: 0.791497, adv: 0.363829, cycle: 0.028654, identity: 0.028226] ETA: 2:54:24.335526\n",
      "[Epoch 13/51] [Batch 100/476] [D loss: 0.252798] [G loss: 1.433631, adv: 0.641600, cycle: 0.053460, identity: 0.051487] ETA: 1:35:35.431641\n",
      "[Epoch 13/51] [Batch 200/476] [D loss: 0.239096] [G loss: 0.869853, adv: 0.292199, cycle: 0.039630, identity: 0.036270] ETA: 1:35:15.450020\n",
      "[Epoch 13/51] [Batch 300/476] [D loss: 0.257692] [G loss: 0.943429, adv: 0.233452, cycle: 0.047971, identity: 0.046052] ETA: 1:34:17.497204\n",
      "[Epoch 13/51] [Batch 400/476] [D loss: 0.268414] [G loss: 0.952568, adv: 0.290038, cycle: 0.045397, identity: 0.041713] ETA: 1:33:32.897243\n",
      "[Epoch 14/51] [Batch 0/476] [D loss: 0.190888] [G loss: 0.762463, adv: 0.351403, cycle: 0.027861, identity: 0.026490] ETA: 2:47:22.265231\n",
      "[Epoch 14/51] [Batch 100/476] [D loss: 0.290853] [G loss: 0.973799, adv: 0.240527, cycle: 0.050111, identity: 0.046433] ETA: 1:32:25.139889\n",
      "[Epoch 14/51] [Batch 200/476] [D loss: 0.186687] [G loss: 0.887442, adv: 0.288216, cycle: 0.040169, identity: 0.039507] ETA: 1:33:12.624619\n",
      "[Epoch 14/51] [Batch 300/476] [D loss: 0.213987] [G loss: 0.773006, adv: 0.240347, cycle: 0.036487, identity: 0.033559] ETA: 1:30:24.673271\n",
      "[Epoch 14/51] [Batch 400/476] [D loss: 0.328389] [G loss: 0.755740, adv: 0.224984, cycle: 0.035954, identity: 0.034243] ETA: 1:31:26.093554\n",
      "[Epoch 15/51] [Batch 0/476] [D loss: 0.285200] [G loss: 1.381808, adv: 0.516872, cycle: 0.059425, identity: 0.054138] ETA: 2:42:39.899323\n",
      "[Epoch 15/51] [Batch 100/476] [D loss: 0.279025] [G loss: 0.641408, adv: 0.268738, cycle: 0.025632, identity: 0.023269] ETA: 1:30:25.341213\n",
      "[Epoch 15/51] [Batch 200/476] [D loss: 0.217442] [G loss: 1.035866, adv: 0.406177, cycle: 0.042439, identity: 0.041060] ETA: 1:29:38.465977\n",
      "[Epoch 15/51] [Batch 300/476] [D loss: 0.220283] [G loss: 1.129085, adv: 0.364466, cycle: 0.050995, identity: 0.050935] ETA: 1:28:44.217856\n",
      "[Epoch 15/51] [Batch 400/476] [D loss: 0.183586] [G loss: 0.972406, adv: 0.278936, cycle: 0.047948, identity: 0.042797] ETA: 1:27:09.469307\n",
      "[Epoch 16/51] [Batch 0/476] [D loss: 0.347297] [G loss: 0.714898, adv: 0.282763, cycle: 0.030142, identity: 0.026143] ETA: 2:39:43.866081\n",
      "[Epoch 16/51] [Batch 100/476] [D loss: 0.208964] [G loss: 0.636777, adv: 0.169222, cycle: 0.031459, identity: 0.030592] ETA: 1:28:09.332314\n",
      "[Epoch 16/51] [Batch 200/476] [D loss: 0.239633] [G loss: 0.785334, adv: 0.353138, cycle: 0.028029, identity: 0.030381] ETA: 1:27:06.989808\n",
      "[Epoch 16/51] [Batch 300/476] [D loss: 0.258746] [G loss: 1.444479, adv: 0.434730, cycle: 0.069098, identity: 0.063754] ETA: 1:25:10.717459\n",
      "[Epoch 16/51] [Batch 400/476] [D loss: 0.203097] [G loss: 0.676628, adv: 0.276228, cycle: 0.026795, identity: 0.026490] ETA: 1:26:19.078174\n",
      "[Epoch 17/51] [Batch 0/476] [D loss: 0.168931] [G loss: 0.999503, adv: 0.375666, cycle: 0.042057, identity: 0.040654] ETA: 2:32:31.728220\n",
      "[Epoch 17/51] [Batch 100/476] [D loss: 0.239664] [G loss: 1.002682, adv: 0.568525, cycle: 0.029586, identity: 0.027660] ETA: 1:24:58.971714\n",
      "[Epoch 17/51] [Batch 200/476] [D loss: 0.260770] [G loss: 0.750470, adv: 0.303613, cycle: 0.030989, identity: 0.027393] ETA: 1:23:22.999043\n",
      "[Epoch 17/51] [Batch 300/476] [D loss: 0.167983] [G loss: 1.217125, adv: 0.668283, cycle: 0.036514, identity: 0.036740] ETA: 1:24:31.506456\n",
      "[Epoch 17/51] [Batch 400/476] [D loss: 0.080436] [G loss: 1.117006, adv: 0.576084, cycle: 0.037414, identity: 0.033357] ETA: 1:22:00.514212\n",
      "[Epoch 18/51] [Batch 0/476] [D loss: 0.149356] [G loss: 0.984523, adv: 0.550401, cycle: 0.028218, identity: 0.030389] ETA: 2:28:05.739315\n",
      "[Epoch 18/51] [Batch 100/476] [D loss: 0.081613] [G loss: 1.336370, adv: 0.584269, cycle: 0.051521, identity: 0.047378] ETA: 1:23:03.737600\n",
      "[Epoch 18/51] [Batch 200/476] [D loss: 0.133916] [G loss: 2.005864, adv: 0.950354, cycle: 0.072400, identity: 0.066301] ETA: 1:22:26.423532\n",
      "[Epoch 18/51] [Batch 300/476] [D loss: 0.075718] [G loss: 1.566367, adv: 0.824330, cycle: 0.049961, identity: 0.048486] ETA: 1:21:47.088638\n",
      "[Epoch 18/51] [Batch 400/476] [D loss: 0.107119] [G loss: 1.515703, adv: 0.901147, cycle: 0.042668, identity: 0.037575] ETA: 1:20:50.415632\n",
      "[Epoch 19/51] [Batch 0/476] [D loss: 0.266448] [G loss: 1.521500, adv: 0.579125, cycle: 0.063139, identity: 0.062196] ETA: 2:26:42.542725\n",
      "[Epoch 19/51] [Batch 100/476] [D loss: 0.195185] [G loss: 1.304136, adv: 0.747997, cycle: 0.036834, identity: 0.037559] ETA: 1:21:01.533237\n",
      "[Epoch 19/51] [Batch 200/476] [D loss: 0.105128] [G loss: 1.289685, adv: 0.636349, cycle: 0.044376, identity: 0.041914] ETA: 1:20:05.866629\n",
      "[Epoch 19/51] [Batch 300/476] [D loss: 0.225782] [G loss: 1.200033, adv: 0.644388, cycle: 0.039615, identity: 0.031899] ETA: 1:19:40.784451\n",
      "[Epoch 19/51] [Batch 400/476] [D loss: 0.431655] [G loss: 1.636200, adv: 1.019230, cycle: 0.042264, identity: 0.038865] ETA: 1:18:45.258591\n",
      "[Epoch 20/51] [Batch 0/476] [D loss: 0.118481] [G loss: 1.141282, adv: 0.450796, cycle: 0.047759, identity: 0.042579] ETA: 2:18:59.234117\n",
      "[Epoch 20/51] [Batch 100/476] [D loss: 0.099772] [G loss: 1.093861, adv: 0.607067, cycle: 0.032665, identity: 0.032028] ETA: 1:39:12.675674\n",
      "[Epoch 20/51] [Batch 200/476] [D loss: 0.114381] [G loss: 1.380435, adv: 0.810025, cycle: 0.038407, identity: 0.037268] ETA: 1:23:41.063865\n",
      "[Epoch 20/51] [Batch 300/476] [D loss: 0.051143] [G loss: 1.384249, adv: 0.888312, cycle: 0.033769, identity: 0.031650] ETA: 1:22:22.046152\n",
      "[Epoch 20/51] [Batch 400/476] [D loss: 0.069240] [G loss: 1.272698, adv: 0.512990, cycle: 0.051114, identity: 0.049714] ETA: 1:16:40.346942\n",
      "[Epoch 21/51] [Batch 0/476] [D loss: 0.113395] [G loss: 1.391440, adv: 0.629740, cycle: 0.051051, identity: 0.050238] ETA: 2:16:01.334124\n",
      "[Epoch 21/51] [Batch 100/476] [D loss: 0.082716] [G loss: 1.210663, adv: 0.783788, cycle: 0.029355, identity: 0.026665] ETA: 1:15:39.279299\n",
      "[Epoch 21/51] [Batch 200/476] [D loss: 0.056607] [G loss: 1.588934, adv: 0.849516, cycle: 0.052036, identity: 0.043811] ETA: 1:14:51.392517\n",
      "[Epoch 21/51] [Batch 300/476] [D loss: 0.065484] [G loss: 1.326615, adv: 0.714684, cycle: 0.041976, identity: 0.038434] ETA: 1:16:17.344880\n",
      "[Epoch 21/51] [Batch 400/476] [D loss: 0.230655] [G loss: 0.942403, adv: 0.440165, cycle: 0.033999, identity: 0.032449] ETA: 1:20:32.881470\n",
      "[Epoch 22/51] [Batch 0/476] [D loss: 0.178406] [G loss: 0.960994, adv: 0.334580, cycle: 0.042839, identity: 0.039605] ETA: 2:09:23.703421\n",
      "[Epoch 22/51] [Batch 100/476] [D loss: 0.204396] [G loss: 0.929790, adv: 0.432011, cycle: 0.034269, identity: 0.031018] ETA: 1:13:04.419788\n",
      "[Epoch 22/51] [Batch 200/476] [D loss: 0.241245] [G loss: 0.910426, adv: 0.532322, cycle: 0.025836, identity: 0.023950] ETA: 1:12:23.409285\n",
      "[Epoch 22/51] [Batch 300/476] [D loss: 0.229302] [G loss: 1.217015, adv: 0.528440, cycle: 0.048804, identity: 0.040106] ETA: 1:11:44.859116\n",
      "[Epoch 22/51] [Batch 400/476] [D loss: 0.210932] [G loss: 0.857824, adv: 0.235704, cycle: 0.042435, identity: 0.039554] ETA: 1:11:18.477427\n",
      "[Epoch 23/51] [Batch 0/476] [D loss: 0.154310] [G loss: 0.995193, adv: 0.391314, cycle: 0.039996, identity: 0.040784] ETA: 2:05:23.558739\n",
      "[Epoch 23/51] [Batch 100/476] [D loss: 0.183816] [G loss: 1.008054, adv: 0.544078, cycle: 0.031660, identity: 0.029475] ETA: 1:10:22.255116\n",
      "[Epoch 23/51] [Batch 200/476] [D loss: 0.237433] [G loss: 1.159050, adv: 0.515776, cycle: 0.044277, identity: 0.040101] ETA: 1:09:55.991879\n",
      "[Epoch 23/51] [Batch 300/476] [D loss: 0.143724] [G loss: 1.011422, adv: 0.390369, cycle: 0.042161, identity: 0.039888] ETA: 1:09:28.990191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/51] [Batch 400/476] [D loss: 0.217785] [G loss: 1.255351, adv: 0.369913, cycle: 0.060538, identity: 0.056011] ETA: 1:08:42.743683\n",
      "[Epoch 24/51] [Batch 0/476] [D loss: 0.129457] [G loss: 1.550289, adv: 0.658451, cycle: 0.060425, identity: 0.057518] ETA: 1:59:55.271579\n",
      "[Epoch 24/51] [Batch 100/476] [D loss: 0.174636] [G loss: 0.998666, adv: 0.382239, cycle: 0.040648, identity: 0.041989] ETA: 1:13:55.717388\n",
      "[Epoch 24/51] [Batch 200/476] [D loss: 0.251075] [G loss: 1.096492, adv: 0.644293, cycle: 0.029995, identity: 0.030450] ETA: 1:15:25.148199\n",
      "[Epoch 24/51] [Batch 300/476] [D loss: 0.116612] [G loss: 1.734406, adv: 0.956461, cycle: 0.053433, identity: 0.048724] ETA: 1:12:52.118717\n",
      "[Epoch 24/51] [Batch 400/476] [D loss: 0.103835] [G loss: 1.243263, adv: 0.790691, cycle: 0.030399, identity: 0.029717] ETA: 1:15:37.754088\n",
      "[Epoch 25/51] [Batch 0/476] [D loss: 0.089416] [G loss: 1.200074, adv: 0.622990, cycle: 0.039166, identity: 0.037084] ETA: 2:04:01.175398\n",
      "[Epoch 25/51] [Batch 100/476] [D loss: 0.139347] [G loss: 1.062127, adv: 0.600995, cycle: 0.031911, identity: 0.028405] ETA: 1:11:23.126647\n",
      "[Epoch 25/51] [Batch 200/476] [D loss: 0.039850] [G loss: 1.393709, adv: 0.794576, cycle: 0.042191, identity: 0.035445] ETA: 1:09:38.106098\n",
      "[Epoch 25/51] [Batch 300/476] [D loss: 0.075377] [G loss: 1.430380, adv: 0.767383, cycle: 0.045343, identity: 0.041912] ETA: 1:09:18.653958\n",
      "[Epoch 25/51] [Batch 400/476] [D loss: 0.108186] [G loss: 1.256631, adv: 0.636792, cycle: 0.042051, identity: 0.039866] ETA: 1:07:58.763077\n",
      "[Epoch 26/51] [Batch 0/476] [D loss: 0.129180] [G loss: 1.585883, adv: 0.805253, cycle: 0.052046, identity: 0.052035] ETA: 2:29:45.389400\n",
      "[Epoch 26/51] [Batch 100/476] [D loss: 0.184347] [G loss: 1.006199, adv: 0.354253, cycle: 0.042876, identity: 0.044637] ETA: 1:08:13.048477\n",
      "[Epoch 26/51] [Batch 200/476] [D loss: 0.220712] [G loss: 1.181164, adv: 0.677818, cycle: 0.034527, identity: 0.031614] ETA: 1:06:46.412816\n",
      "[Epoch 26/51] [Batch 300/476] [D loss: 0.086943] [G loss: 1.667384, adv: 0.798007, cycle: 0.060492, identity: 0.052891] ETA: 1:16:36.552658\n",
      "[Epoch 26/51] [Batch 400/476] [D loss: 0.108711] [G loss: 1.400091, adv: 0.753772, cycle: 0.046510, identity: 0.036243] ETA: 1:05:50.013041\n",
      "[Epoch 27/51] [Batch 0/476] [D loss: 0.117913] [G loss: 1.025292, adv: 0.435968, cycle: 0.040466, identity: 0.036933] ETA: 1:57:21.424072\n",
      "[Epoch 27/51] [Batch 100/476] [D loss: 0.074838] [G loss: 1.228587, adv: 0.582295, cycle: 0.042689, identity: 0.043880] ETA: 1:06:08.004868\n",
      "[Epoch 27/51] [Batch 200/476] [D loss: 0.222120] [G loss: 1.485216, adv: 0.930326, cycle: 0.036977, identity: 0.037025] ETA: 1:07:08.262276\n",
      "[Epoch 27/51] [Batch 300/476] [D loss: 0.147389] [G loss: 1.073270, adv: 0.583274, cycle: 0.033715, identity: 0.030569] ETA: 1:03:48.725816\n",
      "[Epoch 27/51] [Batch 400/476] [D loss: 0.207548] [G loss: 0.761759, adv: 0.314651, cycle: 0.029697, identity: 0.030028] ETA: 1:03:39.491833\n",
      "[Epoch 28/51] [Batch 0/476] [D loss: 0.164212] [G loss: 1.165085, adv: 0.557033, cycle: 0.040757, identity: 0.040097] ETA: 1:51:12.807858\n",
      "[Epoch 28/51] [Batch 100/476] [D loss: 0.209714] [G loss: 0.781909, adv: 0.342429, cycle: 0.029331, identity: 0.029234] ETA: 1:06:40.674339\n",
      "[Epoch 28/51] [Batch 200/476] [D loss: 0.157621] [G loss: 1.414723, adv: 0.558282, cycle: 0.054545, identity: 0.062197] ETA: 1:00:07.184031\n",
      "[Epoch 28/51] [Batch 300/476] [D loss: 0.157685] [G loss: 0.835401, adv: 0.353082, cycle: 0.032989, identity: 0.030486] ETA: 0:56:38.298737\n",
      "[Epoch 28/51] [Batch 400/476] [D loss: 0.123789] [G loss: 1.342429, adv: 0.465363, cycle: 0.058409, identity: 0.058595] ETA: 0:58:14.806612\n",
      "[Epoch 29/51] [Batch 0/476] [D loss: 0.145077] [G loss: 0.983339, adv: 0.429774, cycle: 0.039398, identity: 0.031917] ETA: 1:39:36.050089\n",
      "[Epoch 29/51] [Batch 100/476] [D loss: 0.184574] [G loss: 0.928422, adv: 0.438547, cycle: 0.033629, identity: 0.030717] ETA: 0:55:31.289943\n",
      "[Epoch 29/51] [Batch 200/476] [D loss: 0.183503] [G loss: 1.384087, adv: 0.546126, cycle: 0.058369, identity: 0.050854] ETA: 0:54:44.578033\n",
      "[Epoch 29/51] [Batch 300/476] [D loss: 0.066911] [G loss: 1.211617, adv: 0.621570, cycle: 0.038461, identity: 0.041088] ETA: 0:53:45.107579\n",
      "[Epoch 29/51] [Batch 400/476] [D loss: 0.093962] [G loss: 1.238390, adv: 0.647221, cycle: 0.038407, identity: 0.041419] ETA: 0:53:33.839748\n",
      "[Epoch 30/51] [Batch 0/476] [D loss: 0.078748] [G loss: 1.895143, adv: 1.001392, cycle: 0.061939, identity: 0.054873] ETA: 1:33:54.771023\n",
      "[Epoch 30/51] [Batch 100/476] [D loss: 0.161828] [G loss: 1.120947, adv: 0.525043, cycle: 0.040375, identity: 0.038431] ETA: 0:52:35.894457\n",
      "[Epoch 30/51] [Batch 200/476] [D loss: 0.144610] [G loss: 1.075226, adv: 0.552652, cycle: 0.035401, identity: 0.033713] ETA: 0:52:15.861429\n",
      "[Epoch 30/51] [Batch 300/476] [D loss: 0.190230] [G loss: 1.322745, adv: 0.531595, cycle: 0.056484, identity: 0.045263] ETA: 0:51:42.698547\n",
      "[Epoch 30/51] [Batch 400/476] [D loss: 0.158528] [G loss: 1.044919, adv: 0.440612, cycle: 0.038897, identity: 0.043067] ETA: 0:50:56.209722\n",
      "[Epoch 31/51] [Batch 0/476] [D loss: 0.103949] [G loss: 1.219828, adv: 0.508762, cycle: 0.051352, identity: 0.039508] ETA: 1:28:23.356495\n",
      "[Epoch 31/51] [Batch 100/476] [D loss: 0.153850] [G loss: 0.899531, adv: 0.431055, cycle: 0.032156, identity: 0.029384] ETA: 0:50:01.575265\n",
      "[Epoch 31/51] [Batch 200/476] [D loss: 0.124722] [G loss: 1.087166, adv: 0.512925, cycle: 0.038335, identity: 0.038178] ETA: 0:49:39.190722\n",
      "[Epoch 31/51] [Batch 300/476] [D loss: 0.149153] [G loss: 1.088715, adv: 0.419086, cycle: 0.047125, identity: 0.039675] ETA: 0:48:57.308989\n",
      "[Epoch 31/51] [Batch 400/476] [D loss: 0.094998] [G loss: 1.300726, adv: 0.698888, cycle: 0.043798, identity: 0.032772] ETA: 0:48:30.699921\n",
      "[Epoch 32/51] [Batch 0/476] [D loss: 0.126487] [G loss: 0.957010, adv: 0.450726, cycle: 0.034254, identity: 0.032748] ETA: 1:25:34.771762\n",
      "[Epoch 32/51] [Batch 100/476] [D loss: 0.157081] [G loss: 1.336187, adv: 0.793242, cycle: 0.037280, identity: 0.034029] ETA: 0:47:38.228260\n",
      "[Epoch 32/51] [Batch 200/476] [D loss: 0.139428] [G loss: 1.094494, adv: 0.618613, cycle: 0.030336, identity: 0.034504] ETA: 0:47:10.608662\n",
      "[Epoch 32/51] [Batch 300/476] [D loss: 0.056556] [G loss: 1.460277, adv: 1.024598, cycle: 0.029033, identity: 0.029069] ETA: 0:46:26.261070\n",
      "[Epoch 32/51] [Batch 400/476] [D loss: 0.050723] [G loss: 1.301615, adv: 0.796873, cycle: 0.034222, identity: 0.032504] ETA: 0:46:36.766078\n",
      "[Epoch 33/51] [Batch 0/476] [D loss: 0.073136] [G loss: 1.162940, adv: 0.689207, cycle: 0.033419, identity: 0.027909] ETA: 1:20:03.431568\n",
      "[Epoch 33/51] [Batch 100/476] [D loss: 0.080933] [G loss: 1.399334, adv: 0.899372, cycle: 0.033975, identity: 0.032042] ETA: 0:44:47.131284\n",
      "[Epoch 33/51] [Batch 200/476] [D loss: 0.077198] [G loss: 1.508112, adv: 1.034660, cycle: 0.031088, identity: 0.032515] ETA: 0:44:24.871181\n",
      "[Epoch 33/51] [Batch 300/476] [D loss: 0.120325] [G loss: 1.141912, adv: 0.658650, cycle: 0.032520, identity: 0.031613] ETA: 0:43:53.330750\n",
      "[Epoch 33/51] [Batch 400/476] [D loss: 0.208606] [G loss: 0.762211, adv: 0.302045, cycle: 0.032032, identity: 0.027969] ETA: 0:43:31.334938\n",
      "[Epoch 34/51] [Batch 0/476] [D loss: 0.222116] [G loss: 1.025182, adv: 0.364403, cycle: 0.044827, identity: 0.042502] ETA: 1:15:16.706501\n",
      "[Epoch 34/51] [Batch 100/476] [D loss: 0.128807] [G loss: 1.198759, adv: 0.586700, cycle: 0.043481, identity: 0.035451] ETA: 0:42:30.015867\n",
      "[Epoch 34/51] [Batch 200/476] [D loss: 0.191050] [G loss: 0.966437, adv: 0.439313, cycle: 0.033800, identity: 0.037824] ETA: 0:41:57.675994\n",
      "[Epoch 34/51] [Batch 300/476] [D loss: 0.224444] [G loss: 0.951707, adv: 0.484695, cycle: 0.031614, identity: 0.030174] ETA: 0:41:16.041580\n",
      "[Epoch 34/51] [Batch 400/476] [D loss: 0.112372] [G loss: 0.859435, adv: 0.455481, cycle: 0.027104, identity: 0.026582] ETA: 0:40:57.483730\n",
      "[Epoch 35/51] [Batch 0/476] [D loss: 0.132065] [G loss: 1.164043, adv: 0.721109, cycle: 0.030112, identity: 0.028363] ETA: 1:11:15.396759\n",
      "[Epoch 35/51] [Batch 100/476] [D loss: 0.043279] [G loss: 1.346738, adv: 0.749300, cycle: 0.042642, identity: 0.034204] ETA: 0:39:53.846315\n",
      "[Epoch 35/51] [Batch 200/476] [D loss: 0.077588] [G loss: 1.236733, adv: 0.744415, cycle: 0.031666, identity: 0.035131] ETA: 0:39:31.027828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/51] [Batch 300/476] [D loss: 0.062390] [G loss: 1.196762, adv: 0.708844, cycle: 0.033251, identity: 0.031081] ETA: 0:38:54.093591\n",
      "[Epoch 35/51] [Batch 400/476] [D loss: 0.142146] [G loss: 0.976547, adv: 0.247637, cycle: 0.051886, identity: 0.042009] ETA: 0:38:17.921249\n",
      "[Epoch 36/51] [Batch 0/476] [D loss: 0.067984] [G loss: 1.551296, adv: 0.707087, cycle: 0.057606, identity: 0.053630] ETA: 1:06:19.078388\n",
      "[Epoch 36/51] [Batch 100/476] [D loss: 0.162543] [G loss: 1.547662, adv: 0.989604, cycle: 0.036703, identity: 0.038206] ETA: 0:37:35.046997\n",
      "[Epoch 36/51] [Batch 200/476] [D loss: 0.202071] [G loss: 1.103562, adv: 0.593632, cycle: 0.031878, identity: 0.038229] ETA: 0:36:41.865664\n",
      "[Epoch 36/51] [Batch 300/476] [D loss: 0.038834] [G loss: 1.157582, adv: 0.709255, cycle: 0.031295, identity: 0.027075] ETA: 0:36:21.216402\n",
      "[Epoch 36/51] [Batch 400/476] [D loss: 0.098968] [G loss: 1.425502, adv: 0.689261, cycle: 0.052681, identity: 0.041885] ETA: 0:35:56.291757\n",
      "[Epoch 37/51] [Batch 0/476] [D loss: 0.096767] [G loss: 0.859008, adv: 0.312714, cycle: 0.034757, identity: 0.039745] ETA: 1:03:06.653957\n",
      "[Epoch 37/51] [Batch 100/476] [D loss: 0.193238] [G loss: 1.099261, adv: 0.545220, cycle: 0.037604, identity: 0.035601] ETA: 0:34:55.393378\n",
      "[Epoch 37/51] [Batch 200/476] [D loss: 0.197208] [G loss: 0.995758, adv: 0.587496, cycle: 0.027674, identity: 0.026304] ETA: 0:34:27.420807\n",
      "[Epoch 37/51] [Batch 300/476] [D loss: 0.110150] [G loss: 1.047553, adv: 0.607697, cycle: 0.029709, identity: 0.028553] ETA: 0:33:54.739237\n",
      "[Epoch 37/51] [Batch 400/476] [D loss: 0.127513] [G loss: 0.967363, adv: 0.520859, cycle: 0.030958, identity: 0.027385] ETA: 0:33:14.884140\n",
      "[Epoch 38/51] [Batch 0/476] [D loss: 0.169346] [G loss: 0.817789, adv: 0.394996, cycle: 0.028628, identity: 0.027302] ETA: 0:59:01.359678\n",
      "[Epoch 38/51] [Batch 100/476] [D loss: 0.143771] [G loss: 1.357321, adv: 0.701698, cycle: 0.043359, identity: 0.044407] ETA: 0:33:00.133066\n",
      "[Epoch 38/51] [Batch 200/476] [D loss: 0.115791] [G loss: 0.842213, adv: 0.437849, cycle: 0.028761, identity: 0.023351] ETA: 0:31:58.342478\n",
      "[Epoch 38/51] [Batch 300/476] [D loss: 0.189634] [G loss: 0.981842, adv: 0.460753, cycle: 0.035543, identity: 0.033131] ETA: 0:31:37.120972\n",
      "[Epoch 38/51] [Batch 400/476] [D loss: 0.069257] [G loss: 1.356347, adv: 0.854370, cycle: 0.033465, identity: 0.033465] ETA: 0:32:41.248744\n",
      "[Epoch 39/51] [Batch 0/476] [D loss: 0.121968] [G loss: 0.770707, adv: 0.290893, cycle: 0.035151, identity: 0.025661] ETA: 0:54:29.764503\n",
      "[Epoch 39/51] [Batch 100/476] [D loss: 0.171067] [G loss: 1.201431, adv: 0.607534, cycle: 0.043418, identity: 0.031943] ETA: 0:30:12.073641\n",
      "[Epoch 39/51] [Batch 200/476] [D loss: 0.146702] [G loss: 1.539534, adv: 0.735584, cycle: 0.046625, identity: 0.067539] ETA: 0:33:07.736246\n",
      "[Epoch 39/51] [Batch 300/476] [D loss: 0.085335] [G loss: 1.254879, adv: 0.585835, cycle: 0.043672, identity: 0.046465] ETA: 0:28:55.766087\n",
      "[Epoch 39/51] [Batch 400/476] [D loss: 0.107116] [G loss: 1.110627, adv: 0.665137, cycle: 0.029892, identity: 0.029314] ETA: 0:28:32.319534\n",
      "[Epoch 40/51] [Batch 0/476] [D loss: 0.095017] [G loss: 0.747581, adv: 0.297805, cycle: 0.028865, identity: 0.032225] ETA: 0:50:03.212588\n",
      "[Epoch 40/51] [Batch 100/476] [D loss: 0.185166] [G loss: 1.257857, adv: 0.789866, cycle: 0.033920, identity: 0.025758] ETA: 0:28:51.415546\n",
      "[Epoch 40/51] [Batch 200/476] [D loss: 0.068092] [G loss: 1.626098, adv: 0.975653, cycle: 0.044724, identity: 0.040640] ETA: 0:26:54.948794\n",
      "[Epoch 40/51] [Batch 300/476] [D loss: 0.108521] [G loss: 1.393177, adv: 0.778197, cycle: 0.040752, identity: 0.041492] ETA: 0:26:15.261885\n",
      "[Epoch 40/51] [Batch 400/476] [D loss: 0.060566] [G loss: 1.231174, adv: 0.802869, cycle: 0.028550, identity: 0.028560] ETA: 0:26:33.390321\n",
      "[Epoch 41/51] [Batch 0/476] [D loss: 0.197053] [G loss: 1.105533, adv: 0.584193, cycle: 0.034768, identity: 0.034732] ETA: 0:44:46.424637\n",
      "[Epoch 41/51] [Batch 100/476] [D loss: 0.062457] [G loss: 1.426713, adv: 0.873663, cycle: 0.037453, identity: 0.035704] ETA: 0:25:24.313955\n",
      "[Epoch 41/51] [Batch 200/476] [D loss: 0.058082] [G loss: 1.392874, adv: 0.889274, cycle: 0.036297, identity: 0.028126] ETA: 0:24:21.924191\n",
      "[Epoch 41/51] [Batch 300/476] [D loss: 0.055318] [G loss: 1.581841, adv: 0.886215, cycle: 0.042896, identity: 0.053333] ETA: 0:23:42.864437\n",
      "[Epoch 41/51] [Batch 400/476] [D loss: 0.094443] [G loss: 1.147175, adv: 0.648643, cycle: 0.033660, identity: 0.032386] ETA: 0:23:10.168505\n",
      "[Epoch 42/51] [Batch 0/476] [D loss: 0.077547] [G loss: 1.370790, adv: 0.909385, cycle: 0.030538, identity: 0.031204] ETA: 0:41:07.896438\n",
      "[Epoch 42/51] [Batch 100/476] [D loss: 0.071810] [G loss: 1.294736, adv: 0.703394, cycle: 0.041520, identity: 0.035229] ETA: 0:23:29.993587\n",
      "[Epoch 42/51] [Batch 200/476] [D loss: 0.046922] [G loss: 1.139872, adv: 0.701321, cycle: 0.029657, identity: 0.028397] ETA: 0:21:48.843683\n",
      "[Epoch 42/51] [Batch 300/476] [D loss: 0.073272] [G loss: 1.193787, adv: 0.796061, cycle: 0.027159, identity: 0.025227] ETA: 0:21:18.772259\n",
      "[Epoch 42/51] [Batch 400/476] [D loss: 0.101118] [G loss: 1.145796, adv: 0.685913, cycle: 0.031818, identity: 0.028341] ETA: 0:21:04.673569\n",
      "[Epoch 43/51] [Batch 0/476] [D loss: 0.048624] [G loss: 1.251176, adv: 0.861393, cycle: 0.027000, identity: 0.023957] ETA: 0:37:07.464066\n",
      "[Epoch 43/51] [Batch 100/476] [D loss: 0.137365] [G loss: 1.336951, adv: 0.824267, cycle: 0.030547, identity: 0.041442] ETA: 0:20:08.616068\n",
      "[Epoch 43/51] [Batch 200/476] [D loss: 0.142972] [G loss: 1.097044, adv: 0.646378, cycle: 0.029083, identity: 0.031966] ETA: 0:19:27.027704\n",
      "[Epoch 43/51] [Batch 300/476] [D loss: 0.129427] [G loss: 1.255675, adv: 0.668718, cycle: 0.040602, identity: 0.036188] ETA: 0:18:31.387504\n",
      "[Epoch 43/51] [Batch 400/476] [D loss: 0.044615] [G loss: 1.367724, adv: 0.869895, cycle: 0.033092, identity: 0.033381] ETA: 0:18:47.502285\n",
      "[Epoch 44/51] [Batch 0/476] [D loss: 0.071485] [G loss: 1.346333, adv: 0.871675, cycle: 0.032929, identity: 0.029073] ETA: 0:32:29.643548\n",
      "[Epoch 44/51] [Batch 100/476] [D loss: 0.039977] [G loss: 1.268391, adv: 0.845050, cycle: 0.028873, identity: 0.026922] ETA: 0:17:31.512856\n",
      "[Epoch 44/51] [Batch 200/476] [D loss: 0.072530] [G loss: 1.314564, adv: 0.491545, cycle: 0.055164, identity: 0.054275] ETA: 0:17:23.598012\n",
      "[Epoch 44/51] [Batch 300/476] [D loss: 0.083994] [G loss: 0.909216, adv: 0.492985, cycle: 0.027382, identity: 0.028482] ETA: 0:18:27.362432\n",
      "[Epoch 44/51] [Batch 400/476] [D loss: 0.113704] [G loss: 0.918966, adv: 0.445481, cycle: 0.032540, identity: 0.029617] ETA: 0:16:01.422176\n",
      "[Epoch 45/51] [Batch 0/476] [D loss: 0.128085] [G loss: 0.942805, adv: 0.550850, cycle: 0.026754, identity: 0.024884] ETA: 0:27:23.121426\n",
      "[Epoch 45/51] [Batch 100/476] [D loss: 0.158851] [G loss: 0.890796, adv: 0.428548, cycle: 0.032424, identity: 0.027603] ETA: 0:16:23.468493\n",
      "[Epoch 45/51] [Batch 200/476] [D loss: 0.130835] [G loss: 1.431205, adv: 0.909850, cycle: 0.035123, identity: 0.034025] ETA: 0:14:36.242332\n",
      "[Epoch 45/51] [Batch 300/476] [D loss: 0.046501] [G loss: 1.086280, adv: 0.690669, cycle: 0.026492, identity: 0.026139] ETA: 0:13:37.175145\n",
      "[Epoch 45/51] [Batch 400/476] [D loss: 0.095238] [G loss: 0.967060, adv: 0.538424, cycle: 0.030010, identity: 0.025707] ETA: 0:13:06.702724\n",
      "[Epoch 46/51] [Batch 0/476] [D loss: 0.087766] [G loss: 1.154223, adv: 0.635429, cycle: 0.034191, identity: 0.035377] ETA: 0:24:11.378713\n",
      "[Epoch 46/51] [Batch 100/476] [D loss: 0.032972] [G loss: 1.154589, adv: 0.664394, cycle: 0.028771, identity: 0.040496] ETA: 0:12:07.233038\n",
      "[Epoch 46/51] [Batch 200/476] [D loss: 0.066172] [G loss: 1.360444, adv: 0.826275, cycle: 0.036795, identity: 0.033243] ETA: 0:11:46.059866\n",
      "[Epoch 46/51] [Batch 300/476] [D loss: 0.091316] [G loss: 1.065757, adv: 0.525650, cycle: 0.034258, identity: 0.039506] ETA: 0:13:32.420654\n",
      "[Epoch 46/51] [Batch 400/476] [D loss: 0.106679] [G loss: 0.965877, adv: 0.402485, cycle: 0.040602, identity: 0.031474] ETA: 0:11:49.080834\n",
      "[Epoch 47/51] [Batch 0/476] [D loss: 0.070827] [G loss: 1.282456, adv: 0.806880, cycle: 0.031288, identity: 0.032539] ETA: 0:18:02.897095\n",
      "[Epoch 47/51] [Batch 100/476] [D loss: 0.060721] [G loss: 1.183292, adv: 0.711581, cycle: 0.032837, identity: 0.028669] ETA: 0:09:38.638588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/51] [Batch 200/476] [D loss: 0.113313] [G loss: 1.216030, adv: 0.669288, cycle: 0.038195, identity: 0.032958] ETA: 0:09:17.694134\n",
      "[Epoch 47/51] [Batch 300/476] [D loss: 0.050548] [G loss: 1.238013, adv: 0.788235, cycle: 0.030470, identity: 0.029015] ETA: 0:08:36.616155\n",
      "[Epoch 47/51] [Batch 400/476] [D loss: 0.063344] [G loss: 1.574782, adv: 0.814658, cycle: 0.049558, identity: 0.052908] ETA: 0:08:03.479797\n",
      "[Epoch 48/51] [Batch 0/476] [D loss: 0.074013] [G loss: 1.204886, adv: 0.622937, cycle: 0.038453, identity: 0.039483] ETA: 0:13:22.646021\n",
      "[Epoch 48/51] [Batch 100/476] [D loss: 0.082867] [G loss: 1.129058, adv: 0.622297, cycle: 0.033590, identity: 0.034172] ETA: 0:07:04.296593\n",
      "[Epoch 48/51] [Batch 200/476] [D loss: 0.194970] [G loss: 1.016443, adv: 0.508455, cycle: 0.033674, identity: 0.034249] ETA: 0:06:30.437635\n",
      "[Epoch 48/51] [Batch 300/476] [D loss: 0.132115] [G loss: 1.215285, adv: 0.732144, cycle: 0.030881, identity: 0.034865] ETA: 0:06:09.436523\n",
      "[Epoch 48/51] [Batch 400/476] [D loss: 0.177100] [G loss: 1.261105, adv: 0.535760, cycle: 0.049369, identity: 0.046332] ETA: 0:05:39.518594\n",
      "[Epoch 49/51] [Batch 0/476] [D loss: 0.143760] [G loss: 0.896193, adv: 0.495160, cycle: 0.025870, identity: 0.028467] ETA: 0:09:27.210833\n",
      "[Epoch 49/51] [Batch 100/476] [D loss: 0.129326] [G loss: 0.951000, adv: 0.477063, cycle: 0.031726, identity: 0.031335] ETA: 0:04:57.009765\n",
      "[Epoch 49/51] [Batch 200/476] [D loss: 0.112318] [G loss: 1.047770, adv: 0.406002, cycle: 0.043352, identity: 0.041650] ETA: 0:04:14.075283\n",
      "[Epoch 49/51] [Batch 300/476] [D loss: 0.099633] [G loss: 0.910498, adv: 0.532851, cycle: 0.026275, identity: 0.022980] ETA: 0:03:33.399180\n",
      "[Epoch 49/51] [Batch 400/476] [D loss: 0.181560] [G loss: 0.930504, adv: 0.473694, cycle: 0.029311, identity: 0.032739] ETA: 0:02:54.928019\n",
      "[Epoch 50/51] [Batch 0/476] [D loss: 0.168536] [G loss: 0.863461, adv: 0.505460, cycle: 0.023436, identity: 0.024728] ETA: 0:04:38.176867\n",
      "[Epoch 50/51] [Batch 100/476] [D loss: 0.094324] [G loss: 1.141809, adv: 0.762052, cycle: 0.026169, identity: 0.023614] ETA: 0:02:02.220278\n",
      "[Epoch 50/51] [Batch 200/476] [D loss: 0.138910] [G loss: 1.246189, adv: 0.740390, cycle: 0.034862, identity: 0.031437] ETA: 0:01:29.072840\n",
      "[Epoch 50/51] [Batch 300/476] [D loss: 0.145699] [G loss: 0.998117, adv: 0.448411, cycle: 0.036779, identity: 0.036384] ETA: 0:00:56.963261\n",
      "[Epoch 50/51] [Batch 400/476] [D loss: 0.154907] [G loss: 1.024830, adv: 0.672130, cycle: 0.025950, identity: 0.018640] ETA: 0:00:24.722998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = 0\n",
    "dataset_name = '224CycleGAN3'\n",
    "os.makedirs(\"ganimages/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "train_gan(dataloader3, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
